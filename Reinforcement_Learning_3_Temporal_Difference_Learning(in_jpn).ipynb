{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD学習 (Temporal Difference Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : Richard S. Sutton and Andrew G.Barto, 「強化学習」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化学習の枠組み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の枠組みは、学習と意思決定を行う「エージェント」と  \n",
    "それ以外のすべてから構成される「環境」の相互作用として表される。  \n",
    "  \n",
    "離散的な時間ステップ　$t=0,1,...$のあるステップtにおいて、  \n",
    "エージェントは環境から状態$s_{t}$を受け取り、行動$a_{t}$を選択する。  \n",
    "  \n",
    "このとき、状態$s_{t}$から行動$a_{t}$への写像はエージェントの方策（policy）と呼ばれ、$\\pi(s,a)$で表される。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、エージェントは、最終的に受け取る報酬を最大化することを目標として学習する。  \n",
    "一般的には期待収益（expected return）を最大化するように設定される。  \n",
    "  \n",
    "各時間ステップtに受け取る報酬を$r_{t}$とするとき、  \n",
    "最も単純な場合には、収益$R_{t}$は  \n",
    "　$R(t) = r_{t+1} + r_{t+2} + ... + r_{T}  $  \n",
    "として表される。ここでTは最終時間ステップである。（相互作用が離散的な場合）  \n",
    "  \n",
    "連続タスクにおいてはT=∞となりR(t)が発散しうるため、割引収益  \n",
    "$$\n",
    "\\begin{align}\n",
    "R_{t} &= r_{t+1} + \\gamma r_{t+2} + \\gamma^{2} r_{t+3} + ... \\\\  \n",
    "　　　&= \\sum_{k=0}^{\\infty}\\gamma^{k} r_{t+k+1} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "を最大化するようにa(t)を選択する。  \n",
    "ただし、γは割引率と呼ばれるパラメータで(0<=γ<=1)である。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的な強化学習アルゴリズムは価値関数に基づく評価を行っている。  \n",
    "方策πのもとでの状態sの価値Vπ(s)は、MDP（マルコフ決定過程）では  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) = E_{\\pi}\\{R_{t} | s_{t}=s\\} = E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "と表される。関数 $ V^{\\pi} $を方策πに対する状態価値観数と呼ぶ。\n",
    "  \n",
    "同様に、方策πのもとで状態sにおいて行動aを取ることの価値を $Q^{\\pi}(s,a)$で表し、  \n",
    "状態sで行動aを取り、その後に方策πに従った期待報酬として次のように定義する。  \n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a)=E_{\\pi}\\{R_{t}|s_{t}=s, a_{t}=a\\} = E_{\\pi}\\{\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}|s_{t}=s, a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のs,aが与えられたときの次に可能な各状態s'の確率を遷移確率と呼ぶ。  \n",
    "$$\n",
    "\\begin{align}\n",
    "P^{a}_{ss'} = Pr\\{s_{t+1}=s'| s_{t}=s, a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "同様にして、次の報酬の期待値を次のように表す。\n",
    "$$\n",
    "\\begin{align}\n",
    "R^{a}_{ss'}=E\\{r_{t+1}| s_{t}=s, a_{t}=a, s_{t+1}=s'\\}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "    \n",
    "強化学習と動的計画法で使われている価値観数は、  \n",
    "任意の方策πと状態sに対して、sの価値と可能な後続状態群の価値との間に  \n",
    "以下の整合性条件（consistency condition）がなりたち、これを$V^{\\pi}$に対するBellman方程式という。  \n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'} [ R^{a}_{ss'} + \\gamma E_{\\pi} \\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t+1}=s'\\} ] \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{\\pi}(s')]\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いま、すべての状態に対して、方策πの期待収益がπ'よりも良いか同じであるなら、  \n",
    "πはπ'よりも良いか、同じであると定義する。  \n",
    "つまり、すべての $ s \\in S $ に対して、$ V^{\\pi}(s) \\leqq V^{\\pi'}(s) $ であるなら、その時に限り $\\pi \\leqq \\pi'$である。\n",
    "  \n",
    "これが１つの最適方策であり、すべての最適方策を$\\pi^{*}$と記す。  \n",
    "最適方策群は最適状態価値関数 $V^{*}(s) = \\max_{\\pi}V^{\\pi}(s)$ を共有する。    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に、最適方策群は最適行動価値関数 $Q^{*}(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)$ を共有する。  \n",
    "$V^{*}$を用いて$Q^{*}$を次のように書くことができる。  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{*}(s,a)=E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{*}$に対するBellman方程式を、Bellman最適方程式という。  \n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "V^{*}(s) &= \\max_{a \\in A(s)}Q^{\\pi^{*}}(s,a) \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\max_{a}\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{*}(s')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$Q^{*}$に対するBellman最適方程式は次の通り。\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{*}(s) &= E\\{r_{t+1}+\\gamma \\max_{a'}Q^{*}(s_{t+1},a')|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma \\max_{a'} Q^{*}(s',a')]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD法は経験を用いて予測問題を解決し、方策πに従って経験をいくつか得ることで  \n",
    "$V^{\\pi}$ の推定値$V$を更新する手法の一つである。  \n",
    "  \n",
    "最も単純なTD法はTD(0)と呼ばれ、以下のようになる。\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha [ r_{t+1}+\\gamma V(s_{t+1}) - V(s_{t})]\n",
    "\\end{align}\n",
    "$$\n",
    "ここで$\\alpha$はステップサイズパラメータである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{\\pi}$に対するBellman方程式より\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "\\therefore V^{\\pi}(s) &= E_{\\pi}\\{r_{t+1}+ \\gamma V^{\\pi}(s_{t+1})|s_{t}=s \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "としたとき、モンテカルロ法は前者の推定値を、動的計画法は後者の推定値を目標とする。  \n",
    "TD法は両者を融合させたものである。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブル型 TD(0) アルゴリズム\n",
    "Algorithm\n",
    ">$V(s)$を任意に初期化し、$\\pi$を評価対象の方策に初期化する  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$ a \\leftarrow s $に対して$\\pi$で与えられる行動  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$V(s) \\leftarrow V(s) + \\alpha[r+\\gamma V(s')-V(s)]$  \n",
    ">　　　　$s \\leftarrow s'$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このTD予測法を制御問題に適用する方法について考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa : 方策オン型TD制御\n",
    "行動価値関数を学習するためにTD法を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q$から導かれる方策を用いて、$s'$での行動$a'$を選択する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s'; a \\leftarrow a';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＊エージェントと環境との相互作用は離散的であり、エピソード的タスク群に分解されること、  \n",
    "　および行動の集合Aと状態の集合Sは有限の要素しか持たず、  \n",
    "　その数は学習開始時に既知であることを仮定する。  \n",
    "＊また、テーブル型TD(0)アルゴリズムとして実装しており、  \n",
    "　state(i) | i=0~nが一次元的に並べられることを前提としている。  \n",
    "　policyとしてはe-greedyを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_table_function(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 learning_rate=0.01, discount_rate=0.95, initial_value=1,random_initial_value=True,\n",
    "                 decay_learning_rate=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.initial_value = initial_value\n",
    "        self.random_initial_value = random_initial_value\n",
    "        self.decay_learning_rate = decay_learning_rate\n",
    "\n",
    "        if self.random_initial_value:\n",
    "            self.q_table = np.random.rand(self.state_space_size, self.action_space_size) * self.initial_value\n",
    "        else:\n",
    "            self.q_table = np.ones((self.state_space_size, self.action_space_size), dtype=float64) * self.initial_value\n",
    "\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "    \n",
    "    def estimate_q_value(self, state, action=None):\n",
    "        self.last_state, self.last_action = state, action\n",
    "        if action is None:\n",
    "            return self.q_table[state]\n",
    "        else:\n",
    "            return self.q_table[state][action]\n",
    "    \n",
    "    def update_q_table(self, reward, next_state, next_action,\n",
    "                       last_state=None, last_action=None):\n",
    "        last_state = self.last_state if last_state is None else last_state\n",
    "        last_action = self.last_action if last_action is None else last_action\n",
    "\n",
    "        delta = reward + self.discount_rate * self.estimate_q_value(next_state, next_action) \\\n",
    "                - self.estimate_q_value(last_state, last_action)\n",
    "        self.q_table[state][action] = self.q_table[state][action] + self.learning_rate * delta\n",
    "\n",
    "    def decay_learning_rate_value(self, decay_rate=None):\n",
    "        decay_rate = self.decay_learning_rate if decay_rate is None else decay_rate\n",
    "        if 0<=decay_rate<=1:\n",
    "            self.learning_rate = self.learning_rate * decay_rate\n",
    "\n",
    "    def save_q_table(self):\n",
    "        return self.q_table\n",
    "\n",
    "    def load_q_table(self, q_table=None):\n",
    "        if q_table is not None:\n",
    "            self.q_table = q_table\n",
    "\n",
    "    def reset(self, reset_q_table=True, learning_rate=None, discount_rate=None, decay_learning_rate=None):\n",
    "        if reset_q_table:\n",
    "            if self.random_initial_value:\n",
    "                self.q_table = np.random.rand(self.state_space_size, self.action_space_size) * initial_value\n",
    "            else:\n",
    "                self.q_table = np.ones((self.state_space_size, self.action_space_size), dtype=float64) * initial_value\n",
    "        self.learning_rate = learning_rate if learning_rate is not None else self.learning_rate\n",
    "        self.discount_rate = discount_rate if discount_rate is not None else self.discount_rate\n",
    "        self.decay_learning_rate = decay_learning_rate if decay_learning_rate is not None else self.decay_learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Policy_e_greedy(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 action_count_list=None,\n",
    "                 initial_play_count=None, epsilon=0.1, min_choose=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.total_play_count = 0\n",
    "        self.min_choose = min_choose\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        # self.action_count_list[action] = number of [action] is choosed\n",
    "        if action_count_list is None:\n",
    "            self.action_count_list = np.zeros(self.action_space_size, dtype=int)\n",
    "        else:\n",
    "            self.action_count_list = action_count_list\n",
    "        if initial_play_count is not None:\n",
    "            self.total_play_count = initial_play_count\n",
    "\n",
    "    def choose_act_greedy(self, state, value_table):\n",
    "        index_of_less_selected = np.where(self.action_count_list)\n",
    "        if index_of_less_selected[0].size == 0:\n",
    "            max_index = np.where(value_table == value_table.max())\n",
    "            action = np.random.choice(max_index[0], 1)\n",
    "        else:\n",
    "            action = int(np.random.choice(index_of_less_selected, 1))\n",
    "\n",
    "    def choose_act(self, state, update_flag=True, epsilon=None):\n",
    "        epsilon = self.epsilon if epsilon is None else epsilon\n",
    "        if np.random.choice([1, 0], p=[epsilon, 1-epsilon]):            \n",
    "            action = int(np.random.choice(range(action_space_size)))\n",
    "        else:\n",
    "            action = self.choose_act_greedy(state)\n",
    "\n",
    "        self.last_state, self.last_action = state, action\n",
    "        if update_flag:\n",
    "            self.total_play_count += 1\n",
    "            self.action_count_list[action] += 1\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def save_record(self):\n",
    "        return self.action_count_list, self.total_play_count\n",
    "    \n",
    "    def load_record(self, action_count_list=None, total_play_count=None):\n",
    "        self.action_count_list = action_count_list if action_count_list is not None else self.action_count_list\n",
    "        self.total_play_count = total_play_count if total_play_count is not None else self.total_play_count\n",
    "    \n",
    "    def reset_record(self, reset_count=True, reset_last=True):\n",
    "        if reset_count:\n",
    "            self.total_play_count = 0\n",
    "            self.action_count_list = np.zeros(self.action_space_size, dtype=int)\n",
    "        if reset_last:\n",
    "            self.last_state = None\n",
    "            self.last_action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent_SARSA(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 state_function, q_function=None, policy_function=None,\n",
    "                 learning_rate=0.01, discount_rate=0.95, \n",
    "                 initial_value=1, random_initial_value=True, decay_learning_rate=1,\n",
    "                 action_count_list=None, initial_play_count=None, epsilon=0.1, min_choose=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.initial_value = initial_value\n",
    "        self.random_initial_value = random_initial_value\n",
    "        self.decay_learning_rate = decay_learning_rate\n",
    "        self.action_count_list = action_count_list\n",
    "        self.initial_play_count = initial_play_count\n",
    "        self.epsilon = epsilon\n",
    "        self.min_choose = min_choose\n",
    "\n",
    "        self.total_play_count = 0\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        self.state_function = self.state_function\n",
    "\n",
    "        if q_function is None:\n",
    "            self.q_function = Q_table_function(self.state_space_size, self.action_space_size,\n",
    "                                               self.learning_rate, self.discount_rate, self.initial_value,\n",
    "                                               self.random_initial_value, self.decay_learning_rate)\n",
    "        else:\n",
    "            self.q_function = q_function\n",
    "\n",
    "        if policy_function is None:\n",
    "            self.policy_function = Policy_e_greedy(self.state_space_size, self.action_space_size,\n",
    "                                                   self.action_count_list, self.initial_play_count,\n",
    "                                                   self.epsilon, self.min_choose)\n",
    "        else:\n",
    "            self.policy_function = policy_function\n",
    "\n",
    "\n",
    "    def act(self, state, update_flag=True):\n",
    "        action = self.policy_function.choose_act(state, update_flag=update_flag)\n",
    "        self.last_state, self.last_action = state, action\n",
    "        return action\n",
    "\n",
    "    def observe_state(action=None):\n",
    "        # state_function must return (reward, next_state, episode_end_flag)\n",
    "        return self.state_function.return_next(action)\n",
    "\n",
    "\n",
    "    def learning_step(self, action, update_flag=True):\n",
    "        reward, next_state, episode_end_flag = self.observe_state(action)\n",
    "        if episode_end_flag:\n",
    "            next_action = -1\n",
    "        else:\n",
    "            next_action = self.act(next_state, update_flag)\n",
    "        if update_flag:\n",
    "            self.q_function.update_q_table(reward, next_state, next_action,\n",
    "                                           self.last_state, self.last_action)\n",
    "        return reward, next_state, next_action, episode_end_flag\n",
    "\n",
    "    def learn(self, episode_num, maximum_trial_per_episode=1000, \n",
    "              save_flag=True, update_flag=True, reset_when_finished=False):\n",
    "        reward = 0\n",
    "        episode_end_flag = False\n",
    "        trial_count = 0\n",
    "\n",
    "        # episode loop\n",
    "        for episode in range(episode_num):\n",
    "            # choose initial action\n",
    "            self.state_function.reset_state()\n",
    "            state = self.observe_state()\n",
    "            action = self.act(state, update_flag)\n",
    "            \n",
    "            while episode_end_flag==False and (trial_count < maximum_trial_per_episode):\n",
    "                reward, state, action, episode_end_flag = self.learning_step(action)\n",
    "                trial_count += 1\n",
    "            else:\n",
    "                reward, episode_end_flag, trial_count = 0, False, 0\n",
    "\n",
    "        save_data = self.save() if save_flag else None\n",
    "        if reset_when_finished:\n",
    "            self.reset()\n",
    "\n",
    "        return save_data\n",
    "\n",
    "    def demo_play(self, episode_num=1, maximum_trial_per_episode=1000):\n",
    "        self.learn(episode_num, maximum_trial_per_episode, safe_flag=False, update_flag=False)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        # returns (q_table, (action_count_list, total_play_count))\n",
    "        q_save_data = self.q_function.save_q_table()\n",
    "        policy_save_data = self.policy_function.save_record()\n",
    "        return q_save_data, policy_save_data\n",
    "\n",
    "    def load(self, q_save_data=None, policy_save_data=None):\n",
    "        if q_save_data is not None:\n",
    "            self.q_function.load_q_table(q_table)\n",
    "        if policy_save_data is not None:\n",
    "            self.policy_function.load_record(self, action_count_list=policy_save_data[0], \n",
    "                                             total_play_count=policy_save_data[1])\n",
    "\n",
    "    def reset(self, reset_q_table=True, reset_count=True, reset_last=True,\n",
    "              learning_rate=None, discount_rate=None,decay_learning_rate=None):\n",
    "        self.q_function.reset(reset_q_table, learning_rate, discount_rate, decay_learning_rate)\n",
    "        self.policy_function.reset_record(reset_count, reset_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q学習 : 方策オフ型TD制御\n",
    "SARSAでは次状態$s'$を観測した後に$Q$から導かれる方策により行動$a'$を選択したが、  \n",
    "Q学習では$\\max_{a'}Q(s',a')$を与える$a'$を用いる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化    \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma \\max_{a'} Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from SARSA import Agent_SARSA, Q_table_function, Policy_e_greedy\n",
    "\n",
    "class Agent_Q_learning(Agent_SARSA):\n",
    "    def learning_step(self, action, update_flag=True):\n",
    "        reward, next_state, episode_end_flag = self.observe_state(action)\n",
    "        if episode_end_flag:\n",
    "            next_action = -1\n",
    "        else:\n",
    "            # choose a' which gives max Q(s',a')\n",
    "            next_action = self.act(next_state, update_flag, epsilon=0)\n",
    "        if update_flag:\n",
    "            self.q_function.update_q_table(reward, next_state, next_action,\n",
    "                                           self.last_state, self.last_action)\n",
    "        return reward, next_state, next_action, episode_end_flag\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
