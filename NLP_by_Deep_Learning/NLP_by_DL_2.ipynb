{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Ch.2　ニューラルネットの基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1　教師あり学習\n",
    "入力変数ベクトル $\\mathbf{x} \\in \\mathcal{X}$、出力変数 $y \\in \\mathcal{Y}$ について  \n",
    "ある変数 $\\mathbf{x}$ が与えられた時に $y$ を予測するモデルを学習することを目的とする。\n",
    "\n",
    "教師あり学習の定義\n",
    "$|\\mathcal{D}|$ 個の訓練事例を $\\mathcal{D} = \\{(\\mathbf{x}^{(n)}, y^{(n)})\\}_{n=1}^{|\\mathcal{D}|}$ としたとき、  \n",
    "訓練データでの誤差（以下の目的関数）を最小化するモデルパラメータ $\\mathbf{\\theta}$ の値を求める手続きを教師あり学習と呼ぶ。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{\\theta}) = \\frac{1}{|\\mathcal{D}|} \\sum_{n=1}^{|\\mathcal{D}|} l_{\\mathbf{\\theta}} (\\mathbf{x}^{(n)}, y^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "自然言語処理では出力は単語のように離散集合 $\\mathcal{Y} = \\{1, \\ldots, |\\mathcal{Y}| \\}$ であることが多く、分類問題として定式化できる。  \n",
    "分類問題では 0-1損失関数の代理損失（surrogate loss）として、  \n",
    "次の交差エントロピー損失関数と（多クラス）ヒンジ損失関数がよく用いられる。\n",
    "\n",
    "・cross-entropy loss\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\mathbf{\\theta}}^{\\text{cross-entropy}} \\left( \\mathbf{x}^{(n)}, y^{(n)} \\right) = - \\log \\frac{\\exp ( f_{\\mathbf{\\theta}}( \\mathbf{x}^{(n)}, y^{(n)} ))}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp ( f_{\\mathbf{\\theta}}(\\mathbf{x}^{(n)}, \\tilde{y}))}\n",
    "\\end{align*}\n",
    "\n",
    "・multiclass hinge loss\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\theta}^{\\text{hinge}}\\left( \\mathbf{x}^{(n)}, y^{(n)} \\right) = \\max \\left( 0, \\, 1 - f_{\\theta}\\left( \\mathbf{x}^{(n)}, y^{(n)} \\right) + \\max_{\\tilde{y} \\in {\\mathcal{Y} \\\\ \\{y^{(n)}\\}} } f_{\\theta}( \\mathbf{x}^{(n)}, \\tilde{y}) \\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "$f_{\\theta}(\\mathbf{x},y)$ は $\\theta$ をパラメータとするスコア関数で、$\\hat{y} = \\operatorname{argmax}_{y} f_{\\theta}(\\mathbf{x}^{(n)}, y)$ を予測として使用することを想定している。  \n",
    "注記なき限りスコア関数はニューラルネットによる実装とする。\n",
    "\n",
    "誤差関数として交差エントロピー、スコア関数 $f_{\\theta}(\\mathbf{x},y)$ を線形モデルとするとき多項ロジスティック回帰という。  \n",
    "交差エントロピー損失関数は以下の条件付き確率モデルを想定し、  \n",
    "その負の対数尤度を想定していると考えることもできる。  \n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y|\\mathbf{x}) = \\frac{\\exp ( f_{\\theta} (\\mathbf{x},y))}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp ( f_{\\theta} ( \\mathbf{x}, \\tilde{y}))}\n",
    "\\end{align*}\n",
    "\n",
    "交差エントロピー損失関数は、真の分布 $P^{*}_{\\theta}(y|\\mathbf{x})$ とモデル $P_{\\theta}(y|\\mathbf{x})$ との距離を表す交差エントロピーを  \n",
    "訓練データで経験近似していることに相当することからそう呼ばれる。\n",
    "\n",
    "ソフトマックス関数\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{softmax}(\\mathbf{o})_{y} = \\frac{\\exp (o_{y})}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp (o_{y})}\n",
    "\\end{align*}\n",
    "\n",
    "を用いると、上の式は\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(y|\\mathbf{x}) = \\operatorname{softmax}(\\mathbf{o})_{y}\n",
    "\\end{align*}\n",
    "\n",
    "と書くことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.2　順伝播型ニューラルネット\n",
    "非線形変換を $L$ 回繰り返すFFNN (feedforward neural network) を想定する。  \n",
    "$l$ 回目の変換結果の出力ベクトル $\\mathbf{h}^{(l)} \\in \\mathcal{R}^{N^{(1)}}$ を隠れ状態ベクトルと呼ぶ。  \n",
    "\n",
    "出力層からの出力候補 $y$ に対するスコアの計算は $|\\mathcal{Y}| \\times N^{(L)}$ のパラメータ行列 $\\mathbf{W}^{(o)}$ と  \n",
    "バイアスパラメータベクトル $\\mathbf{b}^{(o)}$ を使って以下のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{o} = \\mathbf{W}^{(o)}\\mathbf{h}^{(L)} + \\mathbf{b}^{(o)}\n",
    "\\end{align*}\n",
    "\n",
    "次に、各層の隠れ状態ベクトル $\\mathbf{h}^{(l)} \\, (l=1,\\ldots,L)$ は活性化関数を $a$ として\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}^{(l)} = a^{(l)} \\left( \\mathbf{W}^{(l)}\\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "と表される。ここで $l=0$ のとき $\\mathbf{h}^{0} = \\mathbf{x}$ （$x$ は入力）とする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.3　活性化関数\n",
    "シグモイド関数、tanh、ReLUなどがよく用いられる。  \n",
    "活性化関数は非線形でなければならない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.4　勾配法\n",
    "学習率を $\\eta$ とするとき、次の更新式によって学習する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta^{(k+1)} = \\theta^{(k)} - \\eta \\partial L(\\theta^{(k)})\n",
    "\\end{align*}\n",
    "\n",
    "勾配法は局所的な最急降下方向に $\\eta$ に比例してパラメータを変化させる。\n",
    "\n",
    "バッチ法による勾配降下法を行うとき、  \n",
    "損失関数の偏微分は個々のデータにおける偏微分の和に分解できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\partial L(\\theta)^{batch} \\frac{1}{|\\mathcal{D}|} \\sum_{n=1}^{|\\mathcal{D}|} \\partial l_{\\theta}(\\mathbf{x}^{(n)}, y^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "バッチ法では訓練データ全てについての計算が必要なことから、  \n",
    "ランダムに選んだ事例で近似する確率的勾配法がよく用いられる。  \n",
    "１事例のみを用いるオンライン確率的勾配法では次のように近似する。$i$ はランダムに選んだ事例の番号。  \n",
    "　（メモ：原文中の式に $i$ 見当たらず）\n",
    "\n",
    "\\begin{align*}\n",
    "\\partial L(\\theta)^{\\text{online}} = \\partial l_{\\theta} (\\mathbf{x}^{(n)}, y^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "ミニバッチ確率的勾配法では次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\partial L(\\theta)^{\\text{mini-batch}} = \\frac{1}{|\\mathcal{B}|} \\sum_{m=1}^{|\\mathcal{B}|} \\partial l_{\\theta} (\\mathbf{x}^{(\\mathcal{B}[m])}, y^{(\\mathcal{B}[m])})\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.5　誤差逆伝播法\n",
    "(他のnotebookで取り扱ったため省略）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.6　再帰ニューラルネット\n",
    "Recurrent Neural Networksでは、長さ $T$ の入力ベクトル列 $\\mathbf{X} = (\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\ldots, \\mathbf{x}_{T_{x}})$ が与えられたとき、  \n",
    "$l$ 層目の隠れ状態ベクトルを次のとおりに再帰的に更新する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t}^{(l)} = a^{(l)} \\left( \\mathbf{W}^{(l)} \\left[ \\begin{array} \\mathbf{h}_{t}^{(l-1)} \\\\\\mathbf{h}_{t-1}^{(l)} \\end{array} \\right] + \\mathbf{b}^{(l)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "RNNでは時刻 $t$ の $l-1$ 層目の隠れ状態ベクトルと時刻 $t-1$ の $l$ 層目の隠れ状態ベクトルを線形変換することから  \n",
    "パラメータ行列 $\\mathbf{W}^{(l)}$ の要素数は $(N^{(l)} + N^{(l-1)}) \\times N^{(l)}$ となる。  \n",
    "また、パラメータの初期値 $\\mathbf{h}_{0}^{(l)}$ としては零ベクトルなど定数値が使われることが多い。\n",
    "\n",
    "スコア関数は $\\mathbf{h}_{t}^{(L)}$ を用いて次のように計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{x}, y) &= o_{t,y} \\\\\n",
    "\\mathbf{o}_{t} &= \\mathbf{W}^{(o)} \\mathbf{h}_{t}^{(L)} + \\mathbf{b}^{(o)}\n",
    "\\end{align*}\n",
    "\n",
    "RNNは時間方向に深いモデルとなっていること、時刻 $t$ によらない共通のパラメータを用いることが特徴である。  \n",
    "誤差逆伝播法は時間鳳凰に遡ることからBPTTと呼ばれることがあるが、計算グラフ上は大きな違いがない。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 双方向再帰ニューラルネット（bi-directional RNN）\n",
    "時間の前向き方向と後ろ向き方向の走査を組み合わせた双方向RNNはよく用いられる。  \n",
    "前向き走査の隠れ状態ベクトル $\\overrightarrow{\\mathbf{h}}$ と後ろ向き走査の隠れ状態ベクトル $\\overleftarrow{\\mathbf{h}}$ を次の式に従って計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\overrightarrow{\\mathbf{h}}_{t}^{(l)} &= a^{(l)} \\left( \\overrightarrow{W}^{(l)} \\left[ \\begin{array} \\overrightarrow{\\mathbf{h}}_{t}^{(l-1)} \\\\ \\overleftarrow{\\mathbf{h}}_{t}^{(l-1)} \\\\ \\overrightarrow{\\mathbf{h}}_{t-1}^{(l)} \\end{array} \\right] + \\overrightarrow{\\mathbf{b}}^{(l)} \\right) \\\\\n",
    "\\overleftarrow{\\mathbf{h}}_{t}^{(l)} &= a^{(l)} \\left( \\overleftarrow{W}^{(l)} \\left[ \\begin{array} \\overrightarrow{\\mathbf{h}}_{t}^{(l-1)} \\\\ \\overleftarrow{\\mathbf{h}}_{t}^{(l-1)} \\\\ \\overrightarrow{\\mathbf{h}}_{t+1}^{(l)} \\end{array} \\right] + \\overleftarrow{\\mathbf{b}}^{(l)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "出力 $\\mathbf{o}_{t}$ については次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{o}_{t} = \\mathbf{W}^{(o)} \\left[ \\begin{array} \\overrightarrow{\\mathbf{h}}_{t}^{(L)} \\\\ \\overleftarrow{\\mathbf{h}}_{t}^{(L)} \\end{array} \\right] + \\mathbf{b}^{(o)}\n",
    "\\end{align*}\n",
    "\n",
    "上の式では各層で双方向の隠れ状態ベクトルを計算に用いるが  \n",
    "この部分は手法により様々で、例えば逆向きの隠れ状態ベクトルを用いない計算式もある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### パラメータ行列の固有値による勾配不安定性の議論\n",
    "パラメータ行列の固有値分解を次のように書く。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "このときパラメータ行列の $t$ 乗は、$\\mathbf{Q}^{-1}\\mathbf{Q}$ が単位行列となることから次のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{W}^{t} = \\mathbf{Q} \\mathbf{\\Lambda}^{t} \\mathbf{Q}^{-1}\n",
    "\\end{align*}\n",
    "\n",
    "よって、RNNの隠れ状態ベクトルについて $\\mathbf{h}_{t} = \\mathbf{W}\\mathbf{h}_{t-1}$ が成り立つとき、\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t} &= \\mathbf{W}\\mathbf{h}_{t-1} = \\mathbf{W}^{2}\\mathbf{h}_{t-2} = \\cdots = \\mathbf{W}^{t}\\mathbf{h}_{0} \\\\\n",
    "&= \\mathbf{Q} \\mathbf{\\Lambda}^{t} \\mathbf{Q}^{-1} \\mathbf{h}_{0}\n",
    "\\end{align*}\n",
    "\n",
    "となることから、$t$ が大のとき固有値の値により状態変数の絶対値が強く影響されることとなる。  \n",
    "ここから、パラメータ行列の固有値により勾配不安定性について議論することができる。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.7　ゲート付き再帰ニューラルネット\n",
    "RNNは時間方向に深いため、長期の依存関係を学習するのが難しいという問題がある。  \n",
    "ゲートはこれを解決する手法の一つである。  \n",
    "\n",
    "ゲート $\\mathbf{g}^{(f)}, \\mathbf{g}^{(h)}$ を用いてショートカットを次の重み付き和へ拡張する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}^{(l)} = \\mathbf{g}^{(f)} \\odot f^{(l)}(\\mathbf{h}^{(l-1)}) + \\mathbf{g}^{(h)} \\odot \\mathbf{h}^{(l-1)}\n",
    "\\end{align*}\n",
    "\n",
    "また、ゲート自体も学習可能な関数とする。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{g}(\\mathbf{h}) = a^{(g)}(\\mathbf{W}^{(g)} \\mathbf{h} + \\mathbf{b}^{(g)})\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ゲート付RNNは時間方向の隠れ状態の計算にゲート付きショートカットを用いたRNNである。\n",
    "\n",
    "まず、１つ前の時刻の隠れ状態ベクトル $\\mathbf{h}_{t-1}$ へのショートカットを持ったRNNを考える。\n",
    "再帰的な評価を行うと\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t} &= f(\\mathbf{h}_{t-1}) + \\mathbf{h}_{t-1} = f(\\mathbf{h}_{t-1}) + f(\\mathbf{h}_{t-2}) + \\mathbf{h}_{t-2} = \\cdots \\\\\n",
    "&= \\sum_{i=1}^{t} f(\\mathbf{h}_{i-1})\n",
    "\\end{align*}\n",
    "\n",
    "これをゲート付きに拡張する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{t} &= f(\\mathbf{h}_{t-1}) + \\mathbf{g}_{t} \\odot \\mathbf{h}_{t-1} \\\\\n",
    "&= f(\\mathbf{h}_{t-1}) + \\mathbf{g}_{t} \\odot f(\\mathbf{h}_{t-2}) + \\mathbf{g}_{t} \\odot \\mathbf{g}_{t-1} \\odot \\mathbf{h}_{t-2} = \\cdots \\\\\n",
    "&= f(\\mathbf{h}_{t-1}) + \\sum_{i=1}^{t-1} ( \\odot_{j=i}^{t} \\mathbf{g}_{j}) f(\\mathbf{h}_{i-1})\n",
    "\\end{align*}\n",
    "\n",
    "ゲート自体を学習することで、忘却の度合いを調節することができるほか  \n",
    "誤差が過去に直接伝播することで勾配消失が避けられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 長短期記憶（LSTM）\n",
    "LSTM (long short-term memory) は最も代表的なゲート付RNNであり、  \n",
    "入力ゲート $\\mathbf{i}$、忘却ゲート $\\mathbf{f}$、出力ゲート $\\mathbf{o}$ を使用するほか  \n",
    "セルと呼ばれる隠れ状態ベクトル $\\mathbf{c}$ を追加する。\n",
    "\n",
    "LSTMの一つは次のようなモデルとなる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\left[ \\begin{array} \\bar{\\mathbf{h}}_{t}^{(l)} \\\\ \\mathbf{i}_{t}^{(l)} \\\\ \\mathbf{f}_{t}^{(l)} \\\\ \\mathbf{o}_{t}^{(l)} \\end{array} \\right] &= \\left[ \\begin{array} \\text{tanh} \\\\ \\text{sigmoid} \\\\ \\text{sigmoid} \\\\ \\text{sigmoid} \\end{array} \\right] \\left( \n",
    "\\left[ \\begin{array} \\mathbf{w}_{\\bar{h}}^{(l)} \\\\ \\mathbf{w}_{i}^{(l)} \\\\ \\mathbf{w}_{f}^{(l)} \\\\ \\mathbf{w}_{o}^{(l)} \\end{array} \\right]\n",
    "\\left[ \\begin{array}\\mathbf{h}_{t}^{(l-1)} \\\\ \\mathbf{h}_{t-1}^{(l)} \\end{array} \\right]\n",
    " + \\left[ \\begin{array} \\mathbf{b}_{\\bar{h}}^{(l)} \\\\ \\mathbf{b}_{i}^{(l)} \\\\ \\mathbf{b}_{f}^{(l)} \\\\ \\mathbf{b}_{o}^{(l)} \\end{array} \\right]\n",
    "\\right) \\\\\n",
    "\\mathbf{c}_{t}^{(l)} &= \\mathbf{i}_{t}^{(l)} \\odot \\bar{h}_{t}^{(l)} + \\mathbf{f}_{t}^{(l)} \\odot \\mathbf{c}_{t-1}^{(l)} \\\\\n",
    "\\mathbf{h}_{t}^{(l)} &= \\mathbf{o}_{t}^{(l)} \\odot \\operatorname{tanh} ( \\mathbf{c}_{t}^{(l)})\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ゲート付き回帰ユニット（GRU）\n",
    "GRU (gated recurrent unit) はセルを使わないゲート付RNNで、  \n",
    "再設定ゲート $\\mathbf{r}$ と更新ゲート \\mathbf{z}$ を用いて忘却と状態の更新を操作する。  \n",
    "\n",
    "\\begin{align*}\n",
    "\\left[ \\begin{array} \\mathbf{r}_{t}^{(l)} \\\\ \\mathbf{z}_{t}^{(l)} \\end{array} \\right] &= \\operatorname{sigmoid} \\left( \n",
    "\\left[ \\begin{array} \\mathbf{W}_{r}^{(l)} \\\\ \\mathbf{W}_{z}^{(l)} \\end{array} \\right]\n",
    "\\left[ \\begin{array}\\mathbf{h}_{t}^{(l-1)} \\\\ \\mathbf{h}_{t-1}^{(l)} \\end{array} \\right]\n",
    " + \\left[ \\begin{array} \\mathbf{b}_{r}^{(l)} \\\\ \\mathbf{b}_{z}^{(l)} \\end{array} \\right]\n",
    "\\right) \\\\\n",
    "\\tilde{\\mathbf{h}}_{t}^{(l)} &= \\operatorname{tanh} \\left( \\left[ \\begin{array} \\mathbf{h}_{t}^{(l-1)} \\\\ \\mathbf{r}_{t}^{(l)} \\odot \\mathbf{h}_{t-1}^{(l)} \\end{array} \\right] \\right) \\\\\n",
    "\\mathbf{h}_{t}^{(l)} &= (1-\\mathbf{z}_{t}^{(l)}) \\odot \\tilde{\\mathbf{h}}_{t}^{(l)} + \\mathbf{z}_{t}^{(l)} \\odot \\mathbf{h}_{t-1}^{(l)}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.8　木構造再帰ニューラルネット\n",
    "木構造再帰ニューラルネット（Recursive neural networks, Tree-RNN）はRNNを木構造に拡張したものである。\n",
    "\n",
    "Tree-RNNでは木構造を事前に定義することが必要である。  \n",
    "仮に二分木を使うと仮定し、隠れ状態ベクトル $\\mathbf{h}_{P}$ の左の子と右の子を $\\mathbf{h}_{L}, \\mathbf{h}_{R}$ とすると\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{P} = a \\left( \\mathbf{W} \\left[ \\begin{array} \\mathbf{h}_{L} \\\\ \\mathbf{h}_{R} \\end{array} \\right] + \\mathbf{b} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "として $\\mathbf{h}_{P}$ が計算される。\n",
    "\n",
    "この演算は木構造の葉ノードからルートノードに向かって順次計算される。  \n",
    "また、垂直方向に層を重ねるようにすることもできる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{P}^{(l)} = a \\left( \\mathbf{W}^{(l)} \\left[ \\begin{array} \\mathbf{h}_{L}^{(l)} \\\\ \\mathbf{h}_{R}^{(l)} \\\\ \\mathbf{h}_{P}^{(l-1)} \\end{array} \\right] + \\mathbf{b}^{(l)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "Tree-RNNは要素数 $T$ の計算グラフであっても分岐により深さ $\\log_{2}(T)$ で済む。\n",
    "\n",
    "自然言語処理では既存の解析器を用いて構文木を予測し、これをTree-RNNへ与えるものの  \n",
    "構文解析は自明なタスクでなく、構文木が最適かどうかも不明である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
