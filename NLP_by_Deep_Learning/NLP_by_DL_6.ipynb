{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6　汎化性能を向上させる技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.1　汎化誤差の分解\n",
    "汎化誤差は以下の３つに分解して考えることができる。\n",
    "\n",
    " - 近似誤差：モデルの表現力不足により、パラメータを増やすなどして自由度を上げると改善\n",
    " - 推定誤差：訓練データの偏りにより、訓練データを増やすか自由度を下げると改善\n",
    " - 最適化誤差：最適化アルゴリズムにより、別のアルゴリズムを試すと改善することがある\n",
    "\n",
    "訓練データでの誤差が大ならば近似誤差や最適化誤差を考え、  \n",
    "評価データや開発データでの誤差が大ならば推定誤差が大きいと判断できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.2　推定誤差低減に効く手法\n",
    "推定誤差を防ぐ手法は正則化（regularization）と呼ばれる。  \n",
    "これらの手法は一般に\n",
    " - 訓練データの差が予測結果に大きく影響することを防ぐため、ばらつきを抑える\n",
    " - 事前知識を用いて特定の仮定を置く\n",
    "\n",
    "という点が共通する。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### L2正則化 （L2 regularization）\n",
    "パラメータの値の大きさに罰則を与える正則化項を導入する。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta) + \\frac{\\lambda}{2}||\\theta||^{2}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\lambda$ は正則化の強さを調整するパラメータである。\n",
    "\n",
    "正則化項の偏微分は\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta} \\frac{\\lambda}{2}||\\theta||^{2} = \\lambda \\theta\n",
    "\\end{align*}\n",
    "\n",
    "であるから、勾配法での更新式は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta^{(k+1)} &= \\theta^{(k)} - \\eta \\partial L(\\theta^{(k)}) - \\eta \\lambda \\theta^{(k)} \\\\\n",
    "&= (1-\\eta \\lambda) \\theta^{(k)} - \\eta \\partial L(\\theta^{(k)})\n",
    "\\end{align*}\n",
    "\n",
    "式より、パラメータの値は $(1-\\eta \\lambda) \\leq 1$ 倍されることがわかる。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### early stopping\n",
    "学習中に性能評価を実施して、改善が見られない場合に学習を早期終了することで過学習を防ぐ手法である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 学習率減衰 （learning rate decay）\n",
    "学習中に性能評価を実施して、改善が見られない場合に更新率を下げる手法である。  \n",
    "　（注：この定義は著者により差があり、改善の有無を見ずに更新率を下げる手法について言われることのほうが多い）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### パラメータ共有 （parameter sharing）\n",
    "ニューラルネット内の複数の要素あるいは複数のネット間で同じパラメータを使用する手法の総称である。  \n",
    "これは学習対象について何らかの共通点があることを仮定しており、  \n",
    "手法の使用にあたっては事前知識を必要とすることに注意が必要である。\n",
    "\n",
    "RNNにおいて入力単語と出力単語のパラメータ行列を共有するのが有名な適用例であり、  \n",
    "性能が向上した例が報告されている。\n",
    "\n",
    "マルチタスク学習でもしばしば適用があり、  \n",
    "複数言語間翻訳でモデルの一部を共有することで性能が改善する例が報告されている。\n",
    "\n",
    "パラメータ共有に類似した手法としてパラメータ結束（parameter tying）があり  \n",
    "２つのパラメータの差に対して罰則を与えることで近いパラメータを学習させる。  \n",
    "罰則としてはフロベニウスノルム（Frobenius norm）の２乗\n",
    "\n",
    "\\begin{align*}\n",
    "||\\mathbf{W}^{(a)} - \\mathbf{W}^{(b)}||^{2}\n",
    "\\end{align*}\n",
    "\n",
    "などを用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 事前学習 （pre-training）\n",
    "別の補助タスクで学習したパラメータを主タスクのニューラルネットの一部として使う手法である。  \n",
    "このうち、学習するときの初期値として使い、主タスクの損失を減らすよう学習させるものを fine tuning という。\n",
    "\n",
    "NLP分野では単語埋め込みベクトルが最も広く使われている例で、  \n",
    "事前学習したパラメータで固定または初期化していることが多い。\n",
    "\n",
    "例えば補助タスクとして一般的な大規模コーパスにより埋め込みベクトルを学習し  \n",
    "主タスクとして目的の特定分野に特化したコーパスにより学習することで  \n",
    "性能向上がみられることがしばしばある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### アンサンブル法 （ense,ble method）\n",
    "複数モデルを組み合わせて予測する手法で、  \n",
    "コミッティマシン（committee machines）やモデル平均化法（model averaging）とも呼ばれる。\n",
    "\n",
    "アンサンブル法はいわばモデルの多数決による予測で、  \n",
    "単独モデルでの予測より正解する確率が高いことが期待できる。\n",
    "\n",
    "類似のものとして、バギング（bootstrap aggregating, bagging）がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ドロップアウト （dropout）\n",
    "訓練時に状態変数の一部をランダムに０に設定する手法である。\n",
    "\n",
    "実装としては、ドロップアウトマスク $\\zeta^{(l)} \\in \\{ 0,1 \\}^{N^{(l)}}$ を用いて\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}^{(l)} = \\zeta^{(l)} \\odot \\mathbf{h}^{(l-1)}\n",
    "\\end{align*}\n",
    "\n",
    "の計算により出力を加工する。\n",
    "\n",
    "Y.Gal and Z.Ghahramani, [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287) In proc. of NIPS, 2016.  \n",
    "ではベイズ的解釈に基づき、ドロップアウトをパラメータ分布からサンプリング方法ととらえ、  \n",
    "事後分布を変分近似しているとの解釈が与えられている。  \n",
    "\n",
    "この解釈を元に提案された Monte-Carlo Dropout は  \n",
    "予測時に複数の Dropout パターンを試して平均を取る手法で、  \n",
    "単一モデルによる既存手法よりも良い精度を出した。\n",
    "\n",
    "これらの Dropout 手法は、暗にアンサンブル法を適用しているものと解釈される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6.3　最適化誤差低減に効く手法\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
