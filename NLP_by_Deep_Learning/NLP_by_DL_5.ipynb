{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5　応用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1　機械翻訳\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計翻訳とニューラル翻訳\n",
    "ある言語の文章を多言語の文章に翻訳する、人手を介さない方法論を機械翻訳（machine translation）と総称する。  \n",
    "ルールベース翻訳、用例ベース翻訳、統計翻訳（statistical machine translation）などの方法論があり、  \n",
    "系列変換モデルを利用した機械翻訳はニューラル翻訳（neural machine translation）と呼ばれる場合が多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### NMTの典型的なモデル構成\n",
    "[OpenNMT](http://opennmt.net/)（旧seq2seq-attn）が現在ベースラインと認識されるモデルの代表である。  \n",
    "この中のモデルの１つについて取り上げる。\n",
    "\n",
    "符号化器再帰層に２層bi-directional LSTMを使うとする。  \n",
    "$\\Psi(\\cdot)$ をLSTMの処理を表す関数とすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} X = (x_{i})_{i=1}^{I} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt}  h_{0}^{fw1} \\leftarrow 0, h_{0}^{fw2} \\leftarrow 0, h_{I+1}^{bw1} \\leftarrow 0, h_{I+1}^{bw2} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt}  \\bar{X} \\leftarrow E^{s}X \\\\\n",
    "&\\hspace{20pt}  \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw1} \\leftarrow \\Psi^{fw1}(\\bar{x}_{i},h_{i-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw2} \\leftarrow \\Psi^{fw2}(h_{i}^{fw1},h_{i-1}^{fw2}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} i' \\leftarrow I+1-i \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw1} \\leftarrow \\Psi^{bw1}(\\bar{x}_{i'},h_{i'+1}^{bw1}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw2} \\leftarrow \\Psi^{bw2}(h_{i'}^{bw1},h_{i'+1}^{bw2}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{s} = h_{i'}^{fw2} + h_{i'}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} H^{s}=(h_{i}^{s})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、復号化再帰層に２層LSTMを使うものとすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2}, Y=(y_{j})_{j=1}^{J} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} y_{0} \\leftarrow y^{(BOS)}, y_{J+1} \\leftarrow y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}y_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow \\operatorname{softmax}(W^{O}c_{j}) \\\\\n",
    "&\\hspace{40pt} \\text{loss}_{j} \\leftarrow \\operatorname{nll}(o_{j}, y_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{loss} \\, \\leftarrow \\sum_{j=1}^{J+1} \\text{loss}_{j} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\text{loss} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価時の復号化処理は一般にビーム探索を用いて処理を行うことが多い。  \n",
    "学習後に未知データを評価する際の復号化器における復号化処理を擬似コードで表したものは次の通り。  \n",
    "（これらのアルゴリズム一式はK=1のビーム探索と等価なアルゴリズムである。）\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\hat{y}_{0} \\leftarrow y^{(BOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} j=0 \\\\\n",
    "&\\hspace{20pt} \\text{repeat} \\\\\n",
    "&\\hspace{40pt} j \\leftarrow j+1 \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}\\hat{y}_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow W^{O}c_{j} \\\\\n",
    "&\\hspace{40pt} \\hat{y}_{i} \\leftarrow \\sigma'_{m}(o_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{until} \\hspace{5pt} \\hat{y}_{j}=y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} J \\leftarrow j-1 \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\hat{Y} = (\\hat{y}_{j})_{j=1}^{J} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 入出力の処理単位、未知語に対する改良\n",
    "語彙数を増やすと計算量が大きくなりすぎ、また語彙選択が難しくなる。  \n",
    "一方で未知語を減らすには語彙を増やすしかないが、零にはできない。\n",
    "\n",
    "SMTの頃より未知語を後処理で何らかの語に置き換えるという手法が一般的である。\n",
    "\n",
    "また、入出力単位を単語から文字に変更するという手法もあり、  \n",
    "語彙数の問題を改善できるが、系列長が極めて長くなるという問題もある。\n",
    "\n",
    "単語と文字の中間的なバイト対符号化（byte pair encoding; BPE）という手法も使われている。  \n",
    "これは出現頻度順に文字ペアを１つの文字としてまとめ、指定の語彙数まで符号化を進めるというものである。\n",
    "\n",
    "現時点では、これらの手法の優劣は不明である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 被覆に関する改良\n",
    "NMTの問題として\n",
    " - 過剰生成問題（over-generation ploblem） ：同じ単語やフレーズが繰り返し生成される\n",
    " - 不足生成問題（under-generation ploblem） ：元の文の必要な語やフレーズが無視される\n",
    "\n",
    "が知られている。  \n",
    "これらの問題を解決する手法として、被覆（coverage）という概念の導入が提案されている。\n",
    "\n",
    "注意機構で計算される注意確率は、復号化器の時刻 $j$ で得られた固定長ベクトルが  \n",
    "入力分のどの部分と適合するかの確率を計算したものになる。  \n",
    "この確率の合計が最終的に何らかの値（要素がすべて１のベクトルなど）になるように学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2　文書要約\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
