{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5　応用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1　機械翻訳\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計翻訳とニューラル翻訳\n",
    "ある言語の文章を多言語の文章に翻訳する、人手を介さない方法論を機械翻訳（machine translation）と総称する。  \n",
    "ルールベース翻訳、用例ベース翻訳、統計翻訳（statistical machine translation）などの方法論があり、  \n",
    "系列変換モデルを利用した機械翻訳はニューラル翻訳（neural machine translation）と呼ばれる場合が多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### NMTの典型的なモデル構成\n",
    "[OpenNMT](http://opennmt.net/)（旧seq2seq-attn）が現在ベースラインと認識されるモデルの代表である。  \n",
    "この中のモデルの１つについて取り上げる。\n",
    "\n",
    "符号化器再帰層に２層bi-directional LSTMを使うとする。  \n",
    "$\\Psi(\\cdot)$ をLSTMの処理を表す関数とすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} X = (x_{i})_{i=1}^{I} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt}  h_{0}^{fw1} \\leftarrow 0, h_{0}^{fw2} \\leftarrow 0, h_{I+1}^{bw1} \\leftarrow 0, h_{I+1}^{bw2} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt}  \\bar{X} \\leftarrow E^{s}X \\\\\n",
    "&\\hspace{20pt}  \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw1} \\leftarrow \\Psi^{fw1}(\\bar{x}_{i},h_{i-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw2} \\leftarrow \\Psi^{fw2}(h_{i}^{fw1},h_{i-1}^{fw2}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} i' \\leftarrow I+1-i \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw1} \\leftarrow \\Psi^{bw1}(\\bar{x}_{i'},h_{i'+1}^{bw1}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw2} \\leftarrow \\Psi^{bw2}(h_{i'}^{bw1},h_{i'+1}^{bw2}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{s} = h_{i'}^{fw2} + h_{i'}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} H^{s}=(h_{i}^{s})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、復号化再帰層に２層LSTMを使うものとすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2}, Y=(y_{j})_{j=1}^{J} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} y_{0} \\leftarrow y^{(BOS)}, y_{J+1} \\leftarrow y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}y_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow \\operatorname{softmax}(W^{O}c_{j}) \\\\\n",
    "&\\hspace{40pt} \\text{loss}_{j} \\leftarrow \\operatorname{nll}(o_{j}, y_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{loss} \\, \\leftarrow \\sum_{j=1}^{J+1} \\text{loss}_{j} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\text{loss} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価時の復号化処理は一般にビーム探索を用いて処理を行うことが多い。  \n",
    "学習後に未知データを評価する際の復号化器における復号化処理を擬似コードで表したものは次の通り。  \n",
    "（これらのアルゴリズム一式はK=1のビーム探索と等価なアルゴリズムである。）\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\hat{y}_{0} \\leftarrow y^{(BOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} j=0 \\\\\n",
    "&\\hspace{20pt} \\text{repeat} \\\\\n",
    "&\\hspace{40pt} j \\leftarrow j+1 \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}\\hat{y}_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow W^{O}c_{j} \\\\\n",
    "&\\hspace{40pt} \\hat{y}_{i} \\leftarrow \\sigma'_{m}(o_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{until} \\hspace{5pt} \\hat{y}_{j}=y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} J \\leftarrow j-1 \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\hat{Y} = (\\hat{y}_{j})_{j=1}^{J} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 入出力の処理単位、未知語に対する改良\n",
    "語彙数を増やすと計算量が大きくなりすぎ、また語彙選択が難しくなる。  \n",
    "一方で未知語を減らすには語彙を増やすしかないが、零にはできない。\n",
    "\n",
    "SMTの頃より未知語を後処理で何らかの語に置き換えるという手法が一般的である。\n",
    "\n",
    "また、入出力単位を単語から文字に変更するという手法もあり、  \n",
    "語彙数の問題を改善できるが、系列長が極めて長くなるという問題もある。\n",
    "\n",
    "単語と文字の中間的なバイト対符号化（byte pair encoding; BPE）という手法も使われている。  \n",
    "これは出現頻度順に文字ペアを１つの文字としてまとめ、指定の語彙数まで符号化を進めるというものである。\n",
    "\n",
    "現時点では、これらの手法の優劣は不明である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 被覆に関する改良\n",
    "NMTの問題として\n",
    " - 過剰生成問題（over-generation ploblem） ：同じ単語やフレーズが繰り返し生成される\n",
    " - 不足生成問題（under-generation ploblem） ：元の文の必要な語やフレーズが無視される\n",
    "\n",
    "が知られている。  \n",
    "これらの問題を解決する手法として、被覆（coverage）という概念の導入が提案されている。\n",
    "\n",
    "注意機構で計算される注意確率は、復号化器の時刻 $j$ で得られた固定長ベクトルが  \n",
    "入力分のどの部分と適合するかの確率を計算したものになる。  \n",
    "この確率の合計が最終的に何らかの値（要素がすべて１のベクトルなど）になるように学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2　文書要約\n",
    "文書要約問題は、教師あり学習のための正解データが整備されていないという大きな問題がある。\n",
    "\n",
    " - 文書要約課題では文書（複数の文）を要約文（複数文）に変換する課題であり、データ整備の負担が大きい。\n",
    " - 正解を定義するのが困難で、要約率すらシステムが利用される場面に依存する。\n",
    " \n",
    "といった理由から、整備が十分に行われてこなかったと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 短文生成タスク／見出し生成タスク\n",
    "文書要約問題の比較的簡易なものとして、  \n",
    "ニュース記事の１文目を入力として見出しを生成する見出し生成タスク（headline generation task）について  \n",
    "ニューラルネットによる符号化復号化モデルを用いた文書要約法が2015年に提案されている。\n",
    "\n",
    "　（A.M.Rush et al, [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/abs/1509.00685). Proceedings of EMNLP 2015）\n",
    "\n",
    "見出し生成タスクのための正解入出力ペアは比較的簡単かつ大量に集めることが可能で、  \n",
    "１文を１文に変換する課題であることからデータ量も対訳と同程度で済む。\n",
    "\n",
    "上記論文での実装、学習／評価データ作成手順は公開されており、  \n",
    "生成型文書要約のベンチマークデータとして広く利用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 文書要約タスクでの符号化復号化方式の発展\n",
    "(1)注意機構を用いるモデル\n",
    "\n",
    "A.M.Rush et al(2015)ではFFNNとattentionを組み合わせた符号化器を用いた見出し生成モデルが提案されており、  \n",
    "Attention Based Summarization と名付けられている。\n",
    "\n",
    "入力文長 $I$ と出力（要約）文長 $J$ について $I>J$ を仮定する。  \n",
    "ABSでは文脈の長さを $C$ として、入力文 $\\mathbf{X}$ が与えられた時の要約文 $\\mathbf{Y}$ の条件付き確率を次のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{abs}(\\mathbf{Y}|\\mathbf{X}) = \\prod_{j=1}^{J+1} P_{abs}(\\mathbf{y}_{j} | \\mathbf{X}, \\mathbf{Y}_{[j-C, j-1]})\n",
    "\\end{align*}\n",
    "\n",
    "また、$j$ 番目の出力単語に関する条件付き確率を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{abs}(\\mathbf{y}_{j}|\\mathbf{X},\\mathbf{Y}_{[j-C, j-1]}) &= \\operatorname{softmax}(\\tilde{\\mathbf{o}}_{j}) \\cdot \\mathbf{y}_{j} \\\\\n",
    "\\tilde{\\mathbf{o}}_{j} &= \\operatorname{nnlm}(\\mathbf{Y}_{[j-C,j-1]}) + \\operatorname{enc}(\\mathbf{X}, \\mathbf{Y}_{[j-C,j-1]})\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\operatorname{nnlm}(\\cdot)$ はFFNNと同等のモデルで、出力層のソフトマックス適用前の値であり  \n",
    "$\\operatorname{enc}(\\cdot)$ は注意機構を用いて入力文を符号化したベクトルである。\n",
    "\n",
    "$\\operatorname{nnlm}(\\cdot)$ はパラメータとして $\\mathbf{E}, \\mathbf{U}, \\mathbf{O}$ をもち、  \n",
    "出力側の語彙数を $|\\mathcal{V}^{t}|$、単語埋め込みベクトルの次元数を $D$ 、隠れ状態ベクトルの次元数を $H$ とすると  \n",
    "$\\mathbf{E} \\in \\mathbb{R}^{D \\times |\\mathcal{V}^{t}|}, \\mathbf{U} \\in \\mathbb{R}^{H \\times (CD)}, \\mathbf{O} \\in \\mathbb{R}^{|\\mathcal{V}^{t}| \\times H}$ となる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ここで、 $\\operatorname{nnlm}(\\cdot)$ の計算は次のような一連の処理である。  \n",
    "これは $C$ 単語前までの情報である埋め込みベクトルを使い次の単語を予測するニューラル言語モデルに相当する。\n",
    "\n",
    "入力層の計算  \n",
    "　出力側の単語に対応する埋め込みベクトル $\\mathbf{e}_{k}$ を埋め込み行列 $\\mathbf{E}$ から取得する  \n",
    "　$j-1$ 番目の単語から前 $C$ 個分の埋め込みベクトルを取得する  \n",
    "   \n",
    "　隠れ層の計算の入力ベクトルを得るために入力層で得られた埋め込みベクトルを連結して $\\tilde{\\mathbf{y}}_{j}$ を得る  \n",
    "　$\\tilde{\\mathbf{y}}_{j}$ は $j-C$ から $j-1$ までの $C$ 個の $D$ 次元ベクトルを列方向に連結した $CD$ 次元列ベクトル  \n",
    " \n",
    "隠れ層の計算  \n",
    "　連結したベクトル $\\tilde{\\mathbf{y}}_{j}$ に対して変換行列 $\\mathbf{U}$ をかけ、  \n",
    "　関数 $\\operatorname{tanh}(\\cdot)$ により非線形変換を行い $\\mathbf{h}_{j}$ を出力する。  \n",
    " \n",
    "出力層の変換  \n",
    "　$\\mathbf{h}_{j}$ を変換行列 $\\mathbf{O}$ を用いて線形変換することで、各出力単語に対するスコアを求める  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "また、$\\operatorname{enc}(\\cdot)$ の計算式は次の通り。  \n",
    "ここで、式中の $\\mathbf{F} \\in \\mathbb{R}^{D \\times | \\mathcal{V}^{s}|}, \\mathbf{E}' \\in \\mathbb{R}^{D \\times | \\mathcal{V}^{t}|}$ はそれぞれ入力側／出力側に対応する埋め込み行列、  \n",
    "$\\mathbf{O}' \\in \\mathbb{R}^{|\\mathcal{V}^{t}| \\times D}$ は出力層の変換行列、  \n",
    "$\\mathbf{P} \\in \\mathbb{R}^{D \\times (CD)}$ は注意確率を計算する際に利用される入力と出力の関連度を表現した変換行列である。  \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{enc}(\\mathbf{X}, \\mathbf{Y}_{[j-C,j-1]}) &= \\mathbf{O}'\\bar{\\mathbf{X}}\\mathbf{p}_{j} \\\\\n",
    "\\mathbf{p}_{j} &= \\operatorname{softmax}(\\tilde{\\mathbf{X}}^{\\mathrm{T}}\\mathbf{P}\\tilde{\\mathbf{y}}'_{j}) \\\\\n",
    "\\bar{\\mathbf{X}} &= [ \\bar{\\mathbf{x}}_{1}, \\ldots, \\bar{\\mathbf{x}}_{I}] \\\\\n",
    "\\bar{\\mathbf{x}}_{i} &= \\sum_{q=i-Q}^{i+Q} \\frac{1}{Q} \\tilde{\\mathbf{x}}_{q} \\\\\n",
    "\\tilde{\\mathbf{y}}'_{j} &= \\operatorname{concat} ( \\mathbf{e}'_{j-C}, \\ldots, \\mathbf{e}'_{j-1}) \\\\\n",
    "\\tilde{\\mathbf{X}} &= \\mathbf{FX} \\\\\n",
    "\\mathbf{e}'_{k} &= \\mathbf{E}'\\mathbf{y}_{k} \\hspace{10pt} \\forall k \\in \\{ j-C, \\ldots, j-1 \\}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "この処理は入出力間の注意機構の計算に相当する。\n",
    "\n",
    "はじめに、入出力それぞれを埋め込みベクトルに変換して、  \n",
    "入力側の埋め込みベクトルのリスト $\\tilde{\\mathbf{X}}$ と、出力側の $k=j-C$ から $k=j-1$ までの埋め込みベクトル $\\mathbf{e}'_{k}$ を得る。\n",
    "\n",
    "出力 $\\mathbf{e}_{k}$ 側は $j-1$ 番目から前 $C$ 個分の埋め込みベクトルを連結して $\\tilde{\\mathbf{y}}'_{j}$ を得る。\n",
    "\n",
    "次に $\\tilde{\\mathbf{X}}$ と $\\tilde{\\mathbf{y}}'_{j}$ を使って、双線形モデルにより注意確率を計算する。\n",
    "\n",
    "最後に、入力側の行列 $\\bar{\\mathbf{X}}$ を得られた注意確率 $\\mathbf{p}_{j}$ の確率分布に従って線型結合し、  \n",
    "変換行列 $\\mathbf{O}'$ によって線形変換した値が各単語を選択する $\\operatorname{enc}(\\cdot)$ のスコアとなる。  \n",
    "ただし $\\bar{\\mathbf{X}}$ は窓幅 $Q$ にある入力層の単語埋め込みベクトルの平均値ベクトルのリストを行列形式にしたものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(2)再帰ニューラルネットに基づく拡張  \n",
    "翻訳タスクと同様に、見出し生成タスクにおいても符号化器／復号化器ともRNNによるモデル化がよいという発表があり、  \n",
    "見出し生成タスクでも系列変換モデルが用いられるようになった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 今後の発展\n",
    "要約率  \n",
    "　要約文長の制御は未解決の課題である。  \n",
    "　事前に与えられた文字数で要約を生成する方法などが提案されている。  \n",
    "\n",
    "意味表現の利用  \n",
    "　意味情報を利用する方法も提案されている。  \n",
    "　Abstract Meaning Representation と呼ばれる符号化器により意味的な情報をグラフ構造で表現し、  \n",
    "　このグラフ構造を木構造 LSTM によって符号化して利用する手法が提案されている。\n",
    "\n",
    "コピー機構  \n",
    "　文書要約タスクでは入力側に出現した単語そのものを出力として利用することが多く、  \n",
    "　これを「コピー」処理として導入する方法論が提案されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.3　対話\n",
    "対話システム（dialog system）は人とコンピュータが会話することを目的としたシステムで、  \n",
    "主にユーザインターフェースとして開発が進められている。\n",
    "\n",
    "旅行案内などのタスク志向対話システム個別のサブシステムの組み合わせで構成されてきたが  \n",
    "ニューラルネットに基づく手法で、人の発話などを入力、応答を出力とするシステムの直接学習がはじまっている。  \n",
    "2017年初頭ではテキスト化された会話が主な研究対象であるが、  \n",
    "直接音声翻訳を生成するなどテキスト翻訳を生成しない研究例もある。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 対話モデル\n",
    "主なタスクは対話文脈を参照して適切な応答を予測するタスクである。  \n",
    "話者が変わることを話者交代（turn-taking）、  \n",
    "複数の話者交代で区切られた発話の系列を対話文脈（context）と呼ぶ。  \n",
    "\n",
    "最後の話者交代の後の次の話者がコンピュータ（対話システム）という想定で、  \n",
    "返すべき発話（応答、response）を予測する。  \n",
    "これにもう一人の話者が応答し、対話システムが次の応答を予測することを繰り返して対話が行われ、  \n",
    "目的を達成するか達成できないことが分かるまで対話が続く。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "系列変換モデルを適用するときは、入力系列：対話文脈、出力系列：応答　とする。  \n",
    "また話者交代を示すために、発話の最後を示す仮単語を導入したり  \n",
    "発話の最初に話者役割を表す仮単語を挿入するなどの方法で話者交代を表現する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "別の手法として、話者交代および発話者を積極的にモデル化した手法も提案されている。  \n",
    "\n",
    "I.V.Serban et al, [Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models](https://arxiv.org/abs/1507.04808) Proceedings of AAAI, p 3776-3783, 2016  \n",
    "では文脈状態を表現する新たな隠れ状態ベクトルを用意し、  \n",
    "これを発話単位で更新することで階層的な文脈符号化を行う手法が提案されている。   \n",
    "\n",
    "J. Li et al, [A Persona-Based Neural Conversation Model](https://arxiv.org/abs/1603.06155) Proceedings of ACL, p 994-1003, 2016  \n",
    "では訓練データ内の話者をモデル化しており、  \n",
    "訓練データ中の話者 ID を使って個性（話者の発話特徴）を反映した応答生成を提案している。\n",
    "\n",
    "H.Ouchi and Y.tsuboi. [Addressee and Response Selection System](https://github.com/hiroki13/response-ranking) Proceedings of EMNLP, p 2133-2143, 2016.  \n",
    "では対話内の話者をモデル化しており、各話者ごとの隠れ状態ベクトルで発話者の状態を表すことで  \n",
    "個々の発話者が１つの対話内で一貫した発言をするように工夫している。  \n",
    "実装上は、階層モデルの文脈表現状態を発話者ごとに分割し  \n",
    "隠れ状態ベクトルがその発話者による発話だけで動的に更新されるように拡張している。  \n",
    "本手法では訓練データにない話者も表現することが可能である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 対話モデルの発展\n",
    "系列変換モデルでは多くの文脈で許容されるような短い応答ばかり出してしまうという問題がある。\n",
    "\n",
    "現在のモデルでは長い文脈を適切に符号化し応答を生成する能力に不足があることも考えられ、  \n",
    "I.V.Servn et al (2016) では、スコアが最大の応答を返す代わりに  \n",
    "条件付き確率 $P(R|C)$ に従って復号化器で応答単語をサンプリングした方が多様な応答が得られること、  \n",
    "ビーム探索や応答単語列全体の大域的なスコアの最大化では応答の多様性が得られないことを報告している。\n",
    "\n",
    "J.Li et al. [A Diversity-Promoting Objective Function for Neural Conversation Models](https://arxiv.org/abs/1510.03055) In. Proc of NAACL 2016  \n",
    "では次のような文脈 $C$ と応答 $R$ の相互情報量を最大化する応答を生成することで  \n",
    "さまざまな応答が生成されることを示した。\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \\frac{P(R,C)}{P(R)P(C)}\n",
    "\\end{align*}\n",
    "\n",
    "ただし相互情報量の最大化と系列変換モデルの結合は難しいため、  \n",
    "この論文では応答予測モデル $P(R|C)$ で生成した候補 $k$ 個を  \n",
    "応答から文脈の予測を行う系列変換モデル $P(C|R)$ で並び替えるなどの手法により近似している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 対話システムの自動評価\n",
    "人手評価を代替する自動評価手法は重要であるが、2017年では明らかな解決策はない。\n",
    "\n",
    "対話タスクは評価という点では翻訳や要約と類似しており、  \n",
    "どれだけ人の応答に似た発話が返せたかを評価することとなる。  \n",
    "しかし、文脈依存であるために対話ログを参照しての自動評価は難しい。\n",
    "\n",
    "機械翻訳で使われる評価指標のように１つの参照応答との単語の重なりを重視する評価の結果は    \n",
    "人手によるシステム応答の評価結果とほとんど相関がない。  \n",
    "（注：翻訳でも性能が一定の水準に達したことにより、BLEEスコアによるモデルの相対評価が不可能となりつつある）\n",
    "\n",
    "生成された応答の自動評価の難しさを避けるために  \n",
    "生成を直接行わず複数の応答候補群から正しい参照応答を選択する応答選択タスクによる評価方法も提案されており、  \n",
    "システムが出力した候補の $k$ 番目以内に正解が含まれる割合（再現率@k）で自動評価することが一般的である。\n",
    "\n",
    "応答選択タスクは人とシステムとの比較も容易であり、人の再現率は偶然よりかなり高く、  \n",
    "対話システムを評価するのに有効と考えられている。  \n",
    "ただし、不正解応答の作り方に課題があり、妥当な応答が不正解となる可能性があるほか、  \n",
    "文脈を読み取る能力などを評価できるかどうかは明らかでない。\n",
    "\n",
    "実環境での話者交代は話者同士の相互作用により、対話システムはこのタイミングも予測する必要がある。  \n",
    "また複数人会話では話者交代はより複雑となり、対話相手を何らかの方法で明示する必要が出てくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.4　質問応答 （question answering; QA）\n",
    "QAは自然言語で与えられた質問に対して自然言語で回答を行うタスクである。\n",
    "\n",
    "最も典型的な質問は事実型質問応答（factoid QA）といい、  \n",
    "誰、どこ、いつといった質問のように回答が具体的な名刺となるものである。  \n",
    "それに対し、非事実型質問応答（non-factoid QA）は、なぜ、どのようにといった質問に代表される。\n",
    "\n",
    "対象とする分野の違いによる区分では、  \n",
    "特定の分野に限った場合をクローズドドメイン質問応答（closed-domain QA）、  \n",
    "分野に限定を置かない場合をオープンドメイン質問応答（open-domain QA）と呼ぶ。\n",
    "\n",
    "また、画像に対して質問を行う画像質問応答（visual QA）や文書読解（reading comprehension）などのタスクもある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 回答選択タスク\n",
    "質問応答はいくつかの処理に分解されて実行されることが一般的であり、例えば次のように分けられる。\n",
    "\n",
    " - 質問解析（question analysis）\n",
    " - 文書検索：関連する文書を検索する\n",
    " - 回答抽出：検索結果から回答候補の抽出を行う　\n",
    " - 回答選択（answer selection）\n",
    "\n",
    "検索部分に情報検索の技術を応用し、回答選択部分に深層学習を利用することが多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "以下は質問応答の回答選択タスクについて考える。\n",
    "\n",
    "回答選択問題を次のように定式化する。  \n",
    "質問文 $q$ と $N$ 個の回答候補 $\\{a_{1}, \\ldots, a_{N}\\}$ が与えられ、  \n",
    "システムは $q$ の適切な回答を回答候補の中から選択する。  \n",
    "\n",
    "・実装上は、質問文や回答候補をベクトル化するものとする。  \n",
    "　ベクトル化の手法としては bi-directional LSTM, Tree-RNN, CNN, CNN + LSTM, attention など様々である。\n",
    "\n",
    "質問文ベクトル $\\mathbf{v}^{(q)}$ と回答文ベクトル $\\mathbf{v}^{(a)}$ に対して、類似度関数 $f$ によって類似度 $f(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a)})$ を計算する。  \n",
    "類似度関数はcos類似度などが用いられ、この類似度を元にして損失を定義する。\n",
    "\n",
    "よく用いられる損失関数は次のようなものである。  \n",
    "正解事例を $a^{+}$、事前に定めたマージンを $m \\in \\mathbb{R}$ として\n",
    "\n",
    "\\begin{align*}\n",
    "f(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a^{+})}) < m + f(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a^{-})})\n",
    "\\end{align*}\n",
    "\n",
    "を満たす負例 $a^{-}$ を全回答候補の中から無作為に抽出する。  \n",
    "この負例によって損失関数を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "l(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a^{+})}, \\mathbf{v}^{(a^{-})}, m) = \\max \\{ 0, m - f(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a^{+})}) + f(\\mathbf{v}^{(q)}, \\mathbf{v}^{(a^{-})})\\}\n",
    "\\end{align*}\n",
    "\n",
    "この損失関数は重み付き近似順位ペアワイズ損失（WARP loss）から重みの部分を省略した形となっている。\n",
    "\n",
    "符号化（ベクトル化）部分と損失計算部分は別々に設計でき、  \n",
    "組み合わせて単一のニューラルネットにすることで end-to-end に学習させることができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "質問応答のタスクは Text REtrieval Conference (TREC) の  \n",
    "[Question Answering Track](http://trec.nist.gov/data/qamain.html) タスクとして長く研究されている。\n",
    "\n",
    "M.Wang et al. [What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA](http://www.aclweb.org/anthology/D/D07/D07-1003.pdf) In Proc. of EMNLP-CoNLL 2007  \n",
    "では過去のTRECのタスクから回答文候補と正解の回答文を与えたデータセットが公開されており、比較用に広く使われている。\n",
    "\n",
    "M.Iyyer et al. [A Neural Network for Factoid Question Answering over Paragraphs.](https://cs.umd.edu/~miyyer/pubs/2014_qb_rnn.pdf) In Proc. of EMNLP, p 633-644, 2014.  \n",
    "ではクイズボウルという回答が固有名詞となるタスクを解いており、こちらの[データセット](https://cs.umd.edu/~miyyer/qblearn/index.html)も公開されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 回答選択問題の評価方法\n",
    "評価尺度としては次のようなものがよく使われる。\n",
    "\n",
    "平均適合率の平均（mean average precision; MAP）  \n",
    "　全体で $N$ 件の正解候補があり、$M$ 件が正解とする。  \n",
    "　システムが $S$ 個の候補を正解として選択し、うち $T$ 件が実際に正解であったとき、  \n",
    "　$T/S$ を適合率、$T/M$ を再現率と呼ぶ。\n",
    "\n",
    "　さて、システムが各回答候補に得点をつけて確信度の高い順に並べるとき、  \n",
    "　上位 $k$ 件の中での適合率を適合率@k（P@k）、  \n",
    "　正解回答候補の順位の適合率@kの平均を平均適合率と呼ぶ。  \n",
    " \n",
    "　データセットは複数の質問文を含むので、これらよりMAPを計算する。\n",
    " \n",
    "平均逆順位（mean reciprocal rank; MRR）  \n",
    "　正解の回答候補の中で一番よい順位を $r$ とするとき、$1/r$ を逆順位と呼ぶ。  \n",
    "　（正解が複数あるとして、システムが並べたリストのうち最高順位の正解のみに着目している）  \n",
    "　複数の質問文について逆順位の平均がMRRである。\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### end-to-endの質問応答\n",
    "質問応答全体を単一のニューラルネットで解決するには、知識源の情報もニューラルネットで扱う必要があり、  \n",
    "記憶ネットワークをQAタスクに適用する研究が進められている。\n",
    "\n",
    "J.Weston et al., [Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks](https://arxiv.org/abs/1502.05698) In Proc. of ICLR, 2015.  \n",
    "で提案された **bAbI タスク**は20種の問題形式で構成された toy-task setであり、  \n",
    "訓練データには文の羅列、質問、回答、サポート事実のセットが含まれる。\n",
    "\n",
    "どのタスクもノイズを含まず、意味論や知識表現を必要とせず、  \n",
    "人間が解けば精度100%になるよう単純かつ自然な問題で構成されている。  \n",
    "　（参考：[論文解説 Memory Networks (MemNN)](http://deeplearning.hatenablog.com/entry/memory_networks) by ryobot）  \n",
    "\n",
    "しかしLSTMなどの手法では十分ない精度で回答することができず、  \n",
    "記憶ネットワークなどによりはじめて高精度に回答できるようになることが報告されている。\n",
    "\n",
    "bAbIタスクのデータセットなどはプロジェクトのページから利用可能。  \n",
    "-> [The bAbI project](https://research.fb.com/downloads/babi/) | facebook research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "また、open-domain QA に記憶ネットワークを適用する研究もあり、  \n",
    "知識源としてWeb文書や、より構造化された知識ベース（knowledge base; KB）が用いられる。\n",
    "\n",
    "知識ベースとしては [Freebase](https://developers.google.com/freebase/) や [DBpedia](http://wiki.dbpedia.org/) が知られており、  \n",
    "いずれも主語・述語・目的語の３つ組の集合となっている。\n",
    "\n",
    "一例として、  \n",
    "A.Bordes et al. [Large-scale Simple Question Answering with Memory Networks](https://arxiv.org/abs/1506.02075) \tarXiv:1506.02075 [cs.LG] (2015)   \n",
    "では単一の３つ組から回答できる事実型質問応答タスクを記憶ネットワークで回答する方法が提案されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "次のnotebookへ続く。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
