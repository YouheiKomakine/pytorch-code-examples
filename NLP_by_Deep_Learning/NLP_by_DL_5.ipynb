{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5　応用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.1　機械翻訳\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計翻訳とニューラル翻訳\n",
    "ある言語の文章を多言語の文章に翻訳する、人手を介さない方法論を機械翻訳（machine translation）と総称する。  \n",
    "ルールベース翻訳、用例ベース翻訳、統計翻訳（statistical machine translation）などの方法論があり、  \n",
    "系列変換モデルを利用した機械翻訳はニューラル翻訳（neural machine translation）と呼ばれる場合が多い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### NMTの典型的なモデル構成\n",
    "[OpenNMT](http://opennmt.net/)（旧seq2seq-attn）が現在ベースラインと認識されるモデルの代表である。  \n",
    "この中のモデルの１つについて取り上げる。\n",
    "\n",
    "符号化器再帰層に２層bi-directional LSTMを使うとする。  \n",
    "$\\Psi(\\cdot)$ をLSTMの処理を表す関数とすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} X = (x_{i})_{i=1}^{I} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt}  h_{0}^{fw1} \\leftarrow 0, h_{0}^{fw2} \\leftarrow 0, h_{I+1}^{bw1} \\leftarrow 0, h_{I+1}^{bw2} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt}  \\bar{X} \\leftarrow E^{s}X \\\\\n",
    "&\\hspace{20pt}  \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw1} \\leftarrow \\Psi^{fw1}(\\bar{x}_{i},h_{i-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} h_{i}^{fw2} \\leftarrow \\Psi^{fw2}(h_{i}^{fw1},h_{i-1}^{fw2}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} i \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} I \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} i' \\leftarrow I+1-i \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw1} \\leftarrow \\Psi^{bw1}(\\bar{x}_{i'},h_{i'+1}^{bw1}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{bw2} \\leftarrow \\Psi^{bw2}(h_{i'}^{bw1},h_{i'+1}^{bw2}) \\\\\n",
    "&\\hspace{40pt} h_{i'}^{s} = h_{i'}^{fw2} + h_{i'}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} H^{s}=(h_{i}^{s})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、復号化再帰層に２層LSTMを使うものとすると、擬似コードは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2}, Y=(y_{j})_{j=1}^{J} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} y_{0} \\leftarrow y^{(BOS)}, y_{J+1} \\leftarrow y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}y_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{for} \\hspace{5pt} j \\leftarrow 1 \\hspace{5pt} \\text{to} \\hspace{5pt} J+1 \\hspace{5pt} \\text{do} \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow \\operatorname{softmax}(W^{O}c_{j}) \\\\\n",
    "&\\hspace{40pt} \\text{loss}_{j} \\leftarrow \\operatorname{nll}(o_{j}, y_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{end} \\\\\n",
    "&\\hspace{20pt} \\text{loss} \\, \\leftarrow \\sum_{j=1}^{J+1} \\text{loss}_{j} \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\text{loss} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価時の復号化処理は一般にビーム探索を用いて処理を行うことが多い。  \n",
    "学習後に未知データを評価する際の復号化器における復号化処理を擬似コードで表したものは次の通り。  \n",
    "（これらのアルゴリズム一式はK=1のビーム探索と等価なアルゴリズムである。）\n",
    "\n",
    "\\begin{align*}\n",
    "\\hline{}\n",
    "&\\text{Input:} \\hspace{5pt} H^{s}=(h_{i})_{i=1}^{I}, h_{I}^{fw1}, h_{I}^{fw2}, h_{1}^{bw1}, h_{1}^{bw2} \\\\\n",
    "&\\text{begin} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw1} \\leftarrow h_{I}^{fw1} + h_{1}^{bw1} \\\\\n",
    "&\\hspace{20pt} z_{0}^{fw2} \\leftarrow h_{I}^{fw2} + h_{1}^{bw2} \\\\\n",
    "&\\hspace{20pt} \\hat{y}_{0} \\leftarrow y^{(BOS)} \\\\\n",
    "&\\hspace{20pt} c_{0} \\leftarrow 0 \\\\\n",
    "&\\hspace{20pt} j=0 \\\\\n",
    "&\\hspace{20pt} \\text{repeat} \\\\\n",
    "&\\hspace{40pt} j \\leftarrow j+1 \\\\\n",
    "&\\hspace{40pt} \\bar{y}_{j} \\leftarrow E^{t}\\hat{y}_{j-1} \\\\\n",
    "&\\hspace{40pt} z_{j}^{0} \\leftarrow \\operatorname{concat}(c_{j-1},\\bar{y}_{j}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw1} \\leftarrow \\Psi^{fw1}(z_{j}^{0},z_{j-1}^{fw1}) \\\\\n",
    "&\\hspace{40pt} z_{j}^{fw2} \\leftarrow \\Psi^{fw2}(z_{j}^{fw1},z_{j-1}^{fw2}) \\\\\n",
    "&\\hspace{40pt} h_{j}^{t} \\leftarrow z_{j}^{fw2} \\\\\n",
    "&\\hspace{40pt} a_{j} \\leftarrow \\operatorname{softmax}((H^{s})^{\\mathrm{T}}W_{2}^{a}h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j}' \\leftarrow H^{s}a_{j} \\\\\n",
    "&\\hspace{40pt} \\tilde{c}_{j} \\leftarrow \\operatorname{concat}(c'_{j}, h_{j}^{t}) \\\\\n",
    "&\\hspace{40pt} c_{j} \\leftarrow \\operatorname{tanh}(W_{1}^{a} \\tilde{c}_{j}) \\\\\n",
    "&\\hspace{40pt} o_{j} \\leftarrow W^{O}c_{j} \\\\\n",
    "&\\hspace{40pt} \\hat{y}_{i} \\leftarrow \\sigma'_{m}(o_{j}) \\\\\n",
    "&\\hspace{20pt} \\text{until} \\hspace{5pt} \\hat{y}_{j}=y^{(EOS)} \\\\\n",
    "&\\hspace{20pt} J \\leftarrow j-1 \\\\\n",
    "&\\text{end} \\\\\n",
    "&\\text{Output:} \\hspace{5pt} \\hat{Y} = (\\hat{y}_{j})_{j=1}^{J} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 入出力の処理単位、未知語に対する改良\n",
    "語彙数を増やすと計算量が大きくなりすぎ、また語彙選択が難しくなる。  \n",
    "一方で未知語を減らすには語彙を増やすしかないが、零にはできない。\n",
    "\n",
    "SMTの頃より未知語を後処理で何らかの語に置き換えるという手法が一般的である。\n",
    "\n",
    "また、入出力単位を単語から文字に変更するという手法もあり、  \n",
    "語彙数の問題を改善できるが、系列長が極めて長くなるという問題もある。\n",
    "\n",
    "単語と文字の中間的なバイト対符号化（byte pair encoding; BPE）という手法も使われている。  \n",
    "これは出現頻度順に文字ペアを１つの文字としてまとめ、指定の語彙数まで符号化を進めるというものである。\n",
    "\n",
    "現時点では、これらの手法の優劣は不明である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 被覆に関する改良\n",
    "NMTの問題として\n",
    " - 過剰生成問題（over-generation ploblem） ：同じ単語やフレーズが繰り返し生成される\n",
    " - 不足生成問題（under-generation ploblem） ：元の文の必要な語やフレーズが無視される\n",
    "\n",
    "が知られている。  \n",
    "これらの問題を解決する手法として、被覆（coverage）という概念の導入が提案されている。\n",
    "\n",
    "注意機構で計算される注意確率は、復号化器の時刻 $j$ で得られた固定長ベクトルが  \n",
    "入力分のどの部分と適合するかの確率を計算したものになる。  \n",
    "この確率の合計が最終的に何らかの値（要素がすべて１のベクトルなど）になるように学習する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2　文書要約\n",
    "文書要約問題は、教師あり学習のための正解データが整備されていないという大きな問題がある。\n",
    "\n",
    " - 文書要約課題では文書（複数の文）を要約文（複数文）に変換する課題であり、データ整備の負担が大きい。\n",
    " - 正解を定義するのが困難で、要約率すらシステムが利用される場面に依存する。\n",
    " \n",
    "といった理由から、整備が十分に行われてこなかったと考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 短文生成タスク／見出し生成タスク\n",
    "文書要約問題の比較的簡易なものとして、  \n",
    "ニュース記事の１文目を入力として見出しを生成する見出し生成タスク（headline generation task）について  \n",
    "ニューラルネットによる符号化復号化モデルを用いた文書要約法が2015年に提案されている。\n",
    "\n",
    "　（A.M.Rush et al, [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/abs/1509.00685). Proceedings of EMNLP 2015）\n",
    "\n",
    "見出し生成タスクのための正解入出力ペアは比較的簡単かつ大量に集めることが可能で、  \n",
    "１文を１文に変換する課題であることからデータ量も対訳と同程度で済む。\n",
    "\n",
    "上記論文での実装、学習／評価データ作成手順は公開されており、  \n",
    "生成型文書要約のベンチマークデータとして広く利用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 文書要約タスクでの符号化復号化方式の発展\n",
    "(1)注意機構を用いるモデル\n",
    "\n",
    "A.M.Rush et al(2015)ではFFNNとattentionを組み合わせた符号化器を用いた見出し生成モデルが提案されており、  \n",
    "Attention Based Summarization と名付けられている。\n",
    "\n",
    "入力文長 $I$ と出力（要約）文長 $J$ について $I>J$ を仮定する。  \n",
    "ABSでは文脈の長さを $C$ として、入力文 $\\mathbf{X}$ が与えられた時の要約文 $\\mathbf{Y}$ の条件付き確率を次のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{abs}(\\mathbf{Y}|\\mathbf{X}) = \\prod_{j=1}^{J+1} P_{abs}(\\mathbf{y}_{j} | \\mathbf{X}, \\mathbf{Y}_{[j-C, j-1]})\n",
    "\\end{align*}\n",
    "\n",
    "また、$j$ 番目の出力単語に関する条件付き確率を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{abs}(\\mathbf{y}_{j}|\\mathbf{X},\\mathbf{Y}_{[j-C, j-1]}) &= \\operatorname{softmax}(\\tilde{\\mathbf{o}}_{j}) \\cdot \\mathbf{y}_{j} \\\\\n",
    "\\tilde{\\mathbf{o}}_{j} &= \\operatorname{nnlm}(\\mathbf{Y}_{[j-C,j-1]}) + \\operatorname{enc}(\\mathbf{X}, \\mathbf{Y}_{[j-C,j-1]})\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\operatorname{nnlm}(\\cdot)$ はFFNNと同等のモデルで、出力層のソフトマックス適用前の値であり  \n",
    "$\\operatorname{enc}(\\cdot)$ は注意機構を用いて入力文を符号化したベクトルである。\n",
    "\n",
    "$\\operatorname{nnlm}(\\cdot)$ はパラメータとして $\\mathbf{E}, \\mathbf{U}, \\mathbf{O}$ をもち、  \n",
    "出力側の語彙数を $|\\mathcal{V}^{t}|$、単語埋め込みベクトルの次元数を $D$ 、隠れ状態ベクトルの次元数を $H$ とすると  \n",
    "$\\mathbf{E} \\in \\mathbb{R}^{D \\times |\\mathcal{V}^{t}|}, \\mathbf{U} \\in \\mathbb{R}^{H \\times (CD)}, \\mathbf{O} \\in \\mathbb{R}^{|\\mathcal{V}^{t}| \\times H}$ となる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ここで、 $\\operatorname{nnlm}(\\cdot)$ の計算は次のような一連の処理である。  \n",
    "これは $C$ 単語前までの情報である埋め込みベクトルを使い次の単語を予測するニューラル言語モデルに相当する。\n",
    "\n",
    "入力層の計算  \n",
    "　出力側の単語に対応する埋め込みベクトル $\\mathbf{e}_{k}$ を埋め込み行列 $\\mathbf{E}$ から取得する  \n",
    "　$j-1$ 番目の単語から前 $C$ 個分の埋め込みベクトルを取得する  \n",
    "   \n",
    "　隠れ層の計算の入力ベクトルを得るために入力層で得られた埋め込みベクトルを連結して $\\tilde{\\mathbf{y}}_{j}$ を得る  \n",
    "　$\\tilde{\\mathbf{y}}_{j}$ は $j-C$ から $j-1$ までの $C$ 個の $D$ 次元ベクトルを列方向に連結した $CD$ 次元列ベクトル  \n",
    " \n",
    "隠れ層の計算  \n",
    "　連結したベクトル $\\tilde{\\mathbf{y}}_{j}$ に対して変換行列 $\\mathbf{U}$ をかけ、  \n",
    "　関数 $\\operatorname{tanh}(\\cdot)$ により非線形変換を行い $\\mathbf{h}_{j}$ を出力する。  \n",
    " \n",
    "出力層の変換  \n",
    "　$\\mathbf{h}_{j}$ を変換行列 $\\mathbf{O}$ を用いて線形変換することで、各出力単語に対するスコアを求める  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "また、$\\operatorname{enc}(\\cdot)$ の計算式は次の通り。  \n",
    "ここで、式中の $\\mathbf{F} \\in \\mathbb{R}^{D \\times | \\mathcal{V}^{s}|}, \\mathbf{E}' \\in \\mathbb{R}^{D \\times | \\mathcal{V}^{t}|}$ はそれぞれ入力側／出力側に対応する埋め込み行列、  \n",
    "$\\mathbf{O}' \\in \\mathbb{R}^{|\\mathcal{V}^{t}| \\times D}$ は出力層の変換行列、  \n",
    "$\\mathbf{P} \\in \\mathbb{R}^{D \\times (CD)}$ は注意確率を計算する際に利用される入力と出力の関連度を表現した変換行列である。  \n",
    "\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{enc}(\\mathbf{X}, \\mathbf{Y}_{[j-C,j-1]}) &= \\mathbf{O}'\\bar{\\mathbf{X}}\\mathbf{p}_{j} \\\\\n",
    "\\mathbf{p}_{j} &= \\operatorname{softmax}(\\tilde{\\mathbf{X}}^{\\mathrm{T}}\\mathbf{P}\\tilde{\\mathbf{y}}'_{j}) \\\\\n",
    "\\bar{\\mathbf{X}} &= [ \\bar{\\mathbf{x}}_{1}, \\ldots, \\bar{\\mathbf{x}}_{I}] \\\\\n",
    "\\bar{\\mathbf{x}}_{i} &= \\sum_{q=i-Q}^{i+Q} \\frac{1}{Q} \\tilde{\\mathbf{x}}_{q} \\\\\n",
    "\\tilde{\\mathbf{y}}'_{j} &= \\operatorname{concat} ( \\mathbf{e}'_{j-C}, \\ldots, \\mathbf{e}'_{j-1}) \\\\\n",
    "\\tilde{\\mathbf{X}} &= \\mathbf{FX} \\\\\n",
    "\\mathbf{e}'_{k} &= \\mathbf{E}'\\mathbf{y}_{k} \\hspace{10pt} \\forall k \\in \\{ j-C, \\ldots, j-1 \\}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "この処理は入出力間の注意機構の計算に相当する。\n",
    "\n",
    "はじめに、入出力それぞれを埋め込みベクトルに変換して、  \n",
    "入力側の埋め込みベクトルのリスト $\\tilde{\\mathbf{X}}$ と、出力側の $k=j-C$ から $k=j-1$ までの埋め込みベクトル $\\mathbf{e}'_{k}$ を得る。\n",
    "\n",
    "出力 $\\mathbf{e}_{k}$ 側は $j-1$ 番目から前 $C$ 個分の埋め込みベクトルを連結して $\\tilde{\\mathbf{y}}'_{j}$ を得る。\n",
    "\n",
    "次に $\\tilde{\\mathbf{X}}$ と $\\tilde{\\mathbf{y}}'_{j}$ を使って、双線形モデルにより注意確率を計算する。\n",
    "\n",
    "最後に、入力側の行列 $\\bar{\\mathbf{X}}$ を得られた注意確率 $\\mathbf{p}_{j}$ の確率分布に従って線型結合し、  \n",
    "変換行列 $\\mathbf{O}'$ によって線形変換した値が各単語を選択する $\\operatorname{enc}(\\cdot)$ のスコアとなる。  \n",
    "ただし $\\bar{\\mathbf{X}}$ は窓幅 $Q$ にある入力層の単語埋め込みベクトルの平均値ベクトルのリストを行列形式にしたものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "(2)再帰ニューラルネットに基づく拡張  \n",
    "翻訳タスクと同様に、見出し生成タスクにおいても符号化器／復号化器ともRNNによるモデル化がよいという発表があり、  \n",
    "見出し生成タスクでも系列変換モデルが用いられるようになった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 今後の発展\n",
    "要約率  \n",
    "　要約文長の制御は未解決の課題である。  \n",
    "　事前に与えられた文字数で要約を生成する方法などが提案されている。  \n",
    "\n",
    "意味表現の利用  \n",
    "　意味情報を利用する方法も提案されている。  \n",
    "　Abstract Meaning Representation と呼ばれる符号化器により意味的な情報をグラフ構造で表現し、  \n",
    "　このグラフ構造を木構造 LSTM によって符号化して利用する手法が提案されている。\n",
    "\n",
    "コピー機構  \n",
    "　文書要約タスクでは入力側に出現した単語そのものを出力として利用することが多く、  \n",
    "　これを「コピー」処理として導入する方法論が提案されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.3　対話"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
