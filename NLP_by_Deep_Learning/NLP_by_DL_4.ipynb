{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4　言語処理特有の深層学習の発展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1　注意機構\n",
    "複数のベクトルがあるとき、どのベクトルを重要視するかも含めて学習させる仕組みを  \n",
    "注意機構（attention mechanism）、あるいは注意と呼ぶ。\n",
    "\n",
    "このうち、複数ベクトルの重み付き平均を使う方法をソフト注意機構（soft attention mechanism）と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ソフト注意機構\n",
    "長さ $I$ の入力系列 $\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{I}\\}$ に対し、各時刻で符号化されたベクトルを $\\{ \\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}$ とおく。  \n",
    "この計算は、RNN の遷移関数を $\\Psi^{(s)}$ とするとき次の式による。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{i}^{(s)} = \\Psi^{(s)} ( \\mathbf{x}_{i}, \\mathbf{h}_{i-1}^{(s)})\n",
    "\\end{align*}\n",
    "\n",
    "通常の系列変換モデルでは最後の状態出力 $\\mathbf{h}_{I}$ のみを用いるが、\n",
    "ここではより直接的に情報を伝播する方法を考える。\n",
    "\n",
    "復号化器でも RNN により隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$ を計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j}^{(t)} = \\Psi^{(t)}(\\mathbf{y}_{j}, \\mathbf{h}_{j-1}^{(t)})\n",
    "\\end{align*}\n",
    "\n",
    "いま、復号化器が $j$ 番目の単語を推定するときに、  \n",
    "復号化器の $i$ 番目の隠れ状態ベクトル $\\mathbf{h}_{i}^{(s)}$ の重要度を示すスカラー値の重みを $a_{i} \\in \\mathbb{R}$ とする。  \n",
    "$a_{i}$ による $\\mathbf{h}_{i}^{(s)}$ の重み付き平均 $\\bar{\\mathbf{h}}$ を\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{h}} = \\sum_{\\tilde{i}=1}^{I} a_{\\tilde{i}}\\mathbf{h}_{\\tilde{i}^{(s)}}\n",
    "\\end{align*}\n",
    "\n",
    "とする。\n",
    "\n",
    "復号化器が $j$ 番目の単語の予測に $\\bar{\\mathbf{h}}$ を利用するため、$\\mathbf{h}_{j}^{(t)}$ の代わりに次式で定義される $\\hat{\\mathbf{h}}_{j}^{(t)}$ を利用する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{h}}_{j}^{(t)} = \\operatorname{tanh} \\left( \\mathbf{W}^{(a)} \\left[ \\begin{array} \\,\\bar{\\mathbf{h}}\\\\ \\mathbf{h}_{j}^{(t)} \\end{array} \\right] \\right)\n",
    "\\end{align*}\n",
    "\n",
    "$\\{a_{1},\\ldots,a_{I}\\}$ もモデル内で計算を行う。  \n",
    "まず関数 $\\Omega$ によって $\\mathbf{h}_{i}^{(s)}$ と $\\mathbf{h}_{j}^{(t)}$ の間の重み $e_{i}$ を計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "e_{i} = \\Omega(\\mathbf{h}_{i}^{(s)}, \\mathbf{h}_{j}^{(t)})\n",
    "\\end{align*}\n",
    "\n",
    "次に、$a_{i}$ は総和が１になるようにソフトマックス関数で正規化する。\n",
    "\n",
    "\\begin{align*}\n",
    "a_{i} = \\frac{\\exp(e_{i})}{\\sum_{\\tilde{i}=1}^{I} \\exp (e_{\\tilde{i}})}\n",
    "\\end{align*}\n",
    "\n",
    "上で使用した関数 $\\Omega$ については様々な関数が利用されており、以下は一例。\n",
    "\n",
    "\\begin{align*}\n",
    "\\Omega (\\mathbf{h}_{i}^{(s)}, \\mathbf{h}_{j}^{(t)}) = \\begin{cases}\n",
    "\\mathbf{h}_{i} \\cdot \\mathbf{h}_{j}^{(t)} \\\\\n",
    "\\mathbf{h}_{i}^{(s)} \\cdot \\mathbf{Wh}_{j}^{(t)} \\\\\n",
    "\\mathbf{v} \\cdot \\operatorname{tanh} \\left( \\mathbf{W} \\left[ \\begin{array}\\, \\mathbf{h}_{i}^{(s)}\\\\\\mathbf{h}_{j}^{(t)} \\end{array} \\right] \\right)\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "$a_{i}$ は確率として解釈することもでき、  \n",
    "このとき $\\bar{\\mathbf{h}}$ は $\\{a_{1},\\ldots,a_{I}\\}$ の分布における $\\mathbf{h}_{i}^{(s)}$ の期待値と見なせる。\n",
    "\n",
    "より一般化する。  \n",
    "$N$ 個の参照したい情報 $\\mathbf{Y} = \\{ \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\}$ （上の例では符号化器の隠れ状態）があり、  \n",
    "それぞれの重要度、あるいはいずれかが選択される確率 $\\{ a_{1},\\ldots,a_{N}\\}$ を計算する。  \n",
    "$a_{i}$ の計算については何らかの情報 $\\mathbf{c}_{i}$ を用いる（上の例では隠れ状態）。\n",
    "\n",
    "例えば end-to-end 記憶ネットワークでは記憶を参照するのにソフト注意機構を利用しており\n",
    "\n",
    "\\begin{align*}\n",
    "a_{i} = \\frac{\\exp(\\Omega(\\mathbf{c}_{i}))}{\\sum_{i=1}^{I} \\exp (\\Omega (\\mathbf{c}_{\\tilde{i}}))}\n",
    "\\end{align*}\n",
    "\n",
    "を利用して次の通り $\\hat{\\mathbf{y}}$ を得る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} = \\mathbb{E}_{a}[\\mathbf{y}] = \\sum_{i=1}^{N} a_{i}\\mathbf{y}_{i}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ハード注意機構\n",
    "ソフト注意機構では複数の情報源が選択される確率に対しその情報の期待値を確定的に計算したのに対し、  \n",
    "ハード注意機構では情報源の中から確率的に１つ選択されるものとして計算する。\n",
    "\n",
    "$N$ 個の参照したい情報 $\\mathbf{Y} = \\{ \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\}$ があり、  \n",
    "確率変数 $X$ が $\\{1,\\ldots,N\\}$ の中から確率的に１つを選択するものとして  \n",
    "この時の確率を $P(X=i) = a_{i}$ とおく。すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "P(X=i)=a_{i}\n",
    "\\end{align*}\n",
    "\n",
    "である。$\\mathbf{Y}$ の中から $X$ を使って無作為に１つ選択された結果を\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{y}X\n",
    "\\end{align*}\n",
    "\n",
    "とする。\n",
    "\n",
    "ハード注意機構を利用した学習で目的関数 $f(\\hat{\\mathbf{y}})$ が計算されたとして、これを最小化する。  \n",
    "$\\hat{\\mathbf{y}}$ および $f(\\hat{\\mathbf{y}})$ は確率的に振る舞うため、期待値 $\\mathbb{E}[f(\\hat{\\mathbf{y}})]$ を最小化の対象とする。  \n",
    "期待値の微分は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\mathbb{E}[f(\\hat{\\mathbf{y}})] &= \\nabla \\sum_{x=1}^{N} f( \\mathbf{y}_{x}) a_{x} \\\\\n",
    "&= \\sum_{x=1}^{N} \\nabla f(\\mathbf{y}_{x}) a_{x} + \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x} \\\\\n",
    "&= \\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}})] + \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x}\n",
    "\\end{align*}\n",
    "\n",
    "いずれの項もすべての $x$ について計算することから計算の負荷が高く、近似が必要となる。  \n",
    "第１項についてはモンテカルロ法を用いる、つまり有限個の標本で次のように近似する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}})] \\approx \\frac{1}{T}\\sum_{i=1}^{T} \\nabla f(\\mathbf{y}_{\\bar{x}_{i}})\n",
    "\\end{align*}\n",
    "\n",
    "第２項についても変形すると期待値の式に変形できるので、モンテカルロ法が適用可能である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{x=1}^{N} f (\\mathbf{y}_{x}) \\nabla a_{x} &= \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x} \\frac{a_{x}}{a_{x}} \\\\\n",
    "&= \\mathbb{E} \\left[ f( \\hat{\\mathbf{y}} ) \\frac{\\nabla a_{x}}{a_{x}} \\right] \\\\\n",
    "&= \\mathbb{E} [ f( \\hat{\\mathbf{y}}) \\nabla \\log a_{x}]\n",
    "\\end{align*}\n",
    "\n",
    "以上により\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\mathbb{E}[f(\\hat{\\mathbf{y}})] &= \\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}}) + f( \\hat{\\mathbf{y}}) \\nabla \\log a_{x}] \\\\\n",
    "&\\approx \\frac{1}{T}\\sum_{i=1}^{T} \\{ \\nabla f(\\mathbf{y}_{\\bar{x}_{i}}) + f(\\mathbf{y}_{\\bar{x}_{i}}) \\nabla \\log a_{\\bar{x}_{i}} \\}\n",
    "\\end{align*}\n",
    "\n",
    "となる。\n",
    "\n",
    "なお実用上、標本から計算される $f( \\mathbf{y}_{\\bar{x}_{i}}) \\nabla \\log a_{\\bar{x}_{i}}$ の分散が大きい時に学習できないことがある。  \n",
    "そこで分散を抑えることを考える。定数 $b$ を用いて\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{e}[f( \\hat{\\mathbf{y}} ) \\nabla \\log a_{x}] &= \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}] + \\mathbb{E}[ b \\nabla \\log a_{x}] \\\\\n",
    "&= \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}] + b \\mathbb{E}[\\nabla \\log a_{x}] \\\\\n",
    "\\mathbb{E}[\\nabla \\log a_{x}] &= \\sum_{x=1}^{N} \\nabla a_{x} = \\nabla \\sum_{x}^{N} a_{x}\n",
    "\\end{align*}\n",
    "\n",
    "と変形する。$a_{x}$ は確率値であるから、$ \\sum_{x=1}^{N}a_{x}=1, \\nabla \\sum_{x=1}^{N}a_{x}=0$ である。  \n",
    "よって任意の定数 $b \\in \\mathbb{R}$ について\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[f( \\hat{\\mathbf{y}} ) \\nabla \\log a_{x}] = \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}]\n",
    "\\end{align*}\n",
    "\n",
    "が成り立ち、この $b$ をベースラインと呼ぶ。  \n",
    "ベースラインとしては $b=\\mathbb{E}[f( \\hat{\\mathbf{y}})]$ がよく用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "その他の注意機構としては、局所注意機構（local attention mechanism）などがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2　記憶ネットワーク\n",
    "記憶ネットワーク（memory networks）は、記憶したい事例の書き込み・読み込み操作をモデル化したものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 記憶ネットワークのモデル\n",
    "記憶ネットワークは $N$ 個の記憶情報 $\\mathbf{M} = (\\mathbf{m}_{i=1}^{N})$ を列として持ち、  \n",
    "内部構造は $I,G,O,R$ ４つの部品に分けてモデル化されている。\n",
    " - Input feature map：入力情報変換、入力された情報を内部表現に変換する\n",
    " - Genelarization：一般化、新しい情報により内部の記憶情報を更新する\n",
    " - Output feature map：出力情報変換、外からの質問に対し記憶情報を利用して返答のための内部表現を生成する\n",
    " - Response：応答、出力情報を文など適切な返答に変換する\n",
    " \n",
    "これらは符号化復号化モデルに記憶を取り扱う部分を追加したものと見なすことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 教師あり記憶ネットワーク\n",
    "最も簡単な状況について考察するため、記憶情報・入力・返答に加えて  \n",
    "返答生成のためにどの記憶情報を利用すべきかという根拠情報（supporting fact）が揃っていることを仮定する。\n",
    "\n",
    "このようなモデルは、根拠情報を与えることを明示するために強教師あり記憶ネットワークとも呼ばれる。\n",
    "\n",
    "いま、入力が単語列、出力（返答）が単語であるシステムを考え、  \n",
    "入力は $D$ 次元ベクトルの内部情報に変換され、記憶情報の各要素は $\\mathbf{m}_{i} \\in \\mathbb{R}^{D}$ とする。\n",
    "\n",
    "入力情報変換処理は埋め込みベクトルの和の形への変換とする。\n",
    "\n",
    "\\begin{align*}\n",
    "I(\\mathbf{x}) = \\sum \\mathbf{Ex}\n",
    "\\end{align*}\n",
    "\n",
    "一般化処理は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_{N} \\leftarrow \\mathbf{x}\n",
    "\\end{align*}\n",
    "\n",
    "出力情報変換について考える。  \n",
    "返答するのに必要な情報か否かを判定するスコア関数があるとして、これを $s_{O}$ で表す。  \n",
    "入力 $\\mathbf{x} \\in \\mathbb{R}^{D}$ に対してスコア上位 $\\kappa$ を記憶 $\\mathbf{M}$ から探す。  \n",
    "ここでは $\\kappa = 2$ として\n",
    "\n",
    "\\begin{align*}\n",
    "o_{1} = O_{1}(\\mathbf{x}, \\mathbf{M}) &= \\operatorname{argmax}_{i=1, \\ldots, N} s_{O} (\\mathbf{x}, \\mathbf{m}_{i}) \\\\\n",
    "o_{2} = O_{2}(\\mathbf{x}, \\mathbf{M}) &= \\operatorname{argmax}_{i=1, \\ldots, N} s_{O} ((\\mathbf{x}, \\mathbf{m}_{o_{1}}), \\mathbf{m}_{i})\n",
    "\\end{align*}\n",
    "\n",
    "ただし $s_{O}$ は複数の文書を扱えるような第一引数をとれるものとする。  \n",
    "\n",
    "返答は $\\mathbf{x}, \\mathbf{m}_{1}, \\mathbf{m}_{2}$ の３つの情報から生成する。  \n",
    "ここでは問題設定より単語を１つ答えるので、語彙集合 $\\mathcal{V}$ の中からスコア $s_{R}$ が最大となる単語を探す。\n",
    "\n",
    "\\begin{align*}\n",
    "r = \\operatorname{argmax}_{w \\in \\mathcal{V}} s_{r} ((\\mathbf{x}, \\mathbf{m}_{o_{1}}, \\mathbf{m}_{o_{2}}),w)\n",
    "\\end{align*}\n",
    "\n",
    "２つのスコア関数 $s_{O}, s_{R}$ は、パラメータ $\\mathbf{U} \\in R_{N \\times D}$ を用いて次の形式で表せる。\n",
    "\n",
    "\\begin{align*}\n",
    "s(\\mathbf{x}, \\mathbf{y}) = \\Phi^{(x)}(\\mathbf{x})^{\\mathrm{T}}\\mathbf{U}^{\\mathrm{T}}\\mathbf{U} \\Phi^{(y)}(\\mathbf{y})\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\Phi$ は入力情報を $D$ 次元ベクトルの特徴ベクトルに変換する関数であり、  \n",
    "種々の方法があるが、最も単純なものは単語の埋め込みベクトルの和である。  \n",
    "実装時には $s_{O},s_{R}$ で異なるパラメータを用い、$\\mathbf{x},\\mathbf{m}_{o_{1}},\\mathbf{m}_{o_{2}}$ のそれぞれに対し別の埋め込みベクトルを用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大値を取るべき候補が $o_{1}^{*}, o{2}^{*},r^{*}$ として与えられたものとする。  \n",
    "学習時は、以下の式を最小化する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{o \\in \\{ 1, \\ldots, N \\} \\backslash \\{ o_{1}^{*} \\} } &\\max (0, \\gamma - s_{O}(\\mathbf{x}, \\mathbf{m}_{o_{1}^{*}}) + s_{O}(\\mathbf{x},\\mathbf{m}_{O})) \\\\\n",
    "+ \\sum_{o \\in \\{ 1, \\ldots, N \\} \\backslash \\{ o_{1}^{*} \\} } &\\max (0, \\gamma - s_{O}((\\mathbf{x}, \\mathbf{m}_{o_{1}^{*}}),\\mathbf{m}_{o_{2}^{*}}) + s_{O}((\\mathbf{x}, \\mathbf{m}_{o_{1}^{*}}),\\mathbf{m}_{o})) \\\\\n",
    "+ \\sum_{o \\in \\mathcal{V} \\backslash \\{ r^{*} \\} } &\\max (0, \\gamma - s_{O}((\\mathbf{x}, \\mathbf{m}_{o_{1}^{*}},\\mathbf{m}_{o_{2}^{*}}),r^{*}) + s_{O}((\\mathbf{x}, \\mathbf{m}_{o_{1}^{*}},\\mathbf{m}_{o_{2}^{*}}),r))\n",
    "\\end{align*}\n",
    "\n",
    "このようなネットワーク用のタスクセットとしては、例えばbAbIタスクがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### end-to-end 記憶ネットワーク\n",
    "現実的な質疑応答タスクでは、  \n",
    "先の教師あり記憶ネットワークで用いたような「どの記憶を参照するか」という中間的な部分課題の解が  \n",
    "明示的に得られず、訓練データに含めることが不可能な場合が多い。  \n",
    "\n",
    "記憶ネットワークの全ての部品を微分可能な関数として記述し  \n",
    "中間的な部分課題の解を用いずに一気に学習させる手法を end-to-end 記憶ネットワークと呼ぶ。  \n",
    "ここでは根拠情報の代わりに注意機構を利用して情報を引き出すこととする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力情報変換では、入力文 $\\mathbf{x}_{i}$ を $D \\times |\\mathcal{V}|$ の埋め込み行列 $\\mathbf{A} \\in \\mathbb{R}^{D \\times \\mathcal{V}}$ を使って、次の単一ベクトル $\\mathbf{m}_{i}$ に符号化する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{m}_{i} = \\sum_{j} \\mathbf{Ax}_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "同様に、質問文 $q$ を $D \\times |\\mathcal{V}|$ の埋め込み行列 $\\mathbf{B} \\in \\mathbb{R}^{D \\times \\mathcal{V}}$ を使って、次の単一ベクトル $\\mathbf{u}$ に符号化する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{u} = \\sum_{j} \\mathbf{Bq}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "記憶情報の参照では、  \n",
    "まず $N$ 個の記憶情報 $\\{ \\mathbf{m}_{1}, \\ldots, \\mathbf{m}_{N}\\}$ のそれぞれに対して、$\\mathbf{u}$ にとっての重要度 $\\{ p_{1}, \\ldots, p_{N}\\}$ を計算する。  \n",
    "これは内積のソフトマックスにより、次式の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "p_{i} = \\frac{\\exp ( \\mathbf{u} \\cdot \\mathbf{m}_{i})}{\\sum_{j=1}^{N} \\exp ( \\mathbf{u} \\cdot \\mathbf{m}_{j})}\n",
    "\\end{align*}\n",
    "\n",
    "回答に利用する出力の情報では、  \n",
    "次の式によって出力情報 $\\mathbf{c}_{i}$ をつくり、全体の出力情報 $\\mathbf{o} \\in \\mathbb{R}^{D}$ を求める。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{c}_{i} &= \\sum_{j} \\mathbf{C} \\mathbf{x}_{ij} \\\\\n",
    "\\mathbf{o} &= \\sum_{i=1}^{N} p_{i} \\mathbf{c}_{i}\n",
    "\\end{align*}\n",
    "\n",
    "最後に $\\mathbf{u}$ と $\\mathbf{o}$、$|\\mathcal{V}| \\times D$ 行列 $\\mathbf{W} \\in \\mathbb{R}^{|\\mathcal{V} \\times D}$ を用いて各返答単語に対するスコアを計算し、全体で１になるよう正規化する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{a}} = \\operatorname{softmax} ( \\mathbf{W}(\\mathbf{o} + \\mathbf{u}))\n",
    "\\end{align*}\n",
    "\n",
    "以上の処理は微分可能な関数のみで構成されているので、  \n",
    "訓練データが入力情報 $\\{ \\mathbf{x}_{i} \\} _{i=1}^{N}$、質問文 $\\mathbf{q}$、正しい返答 $a$ を含んでいれば  \n",
    "$a$ と $\\hat{\\mathbf{a}}$ の交差エントロピーを損失として end-to-end の最適化が可能である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 動的記憶ネットワーク （dynamic memory networks; DMN）\n",
    "DMNは記憶ネットワークの一種で、必要な知識を繰り返し問い合わせるエピソード記憶モジュールが特徴的であり、  \n",
    "これは複数の知識を組み合わせる、知識を用いた推論機構のモデル化である。\n",
    "\n",
    "動的記憶ネットワークは次の５つのモジュールからなる。\n",
    " - input module：入力、外部からの情報を内部表現に変換する\n",
    " - semantic memory module：意味記憶、一般的な知識や常識を格納する\n",
    " - question module：質問を内部表現に変換する\n",
    " - episodic memory module：エピソード記憶、入力された知識を記憶して質問に応じて情報を引き出す\n",
    " - answer module：回答、エピソード記憶による回答の内部表現から返答のための表現を生成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.3　出力層の高速化\n",
    "自然言語処理における多くの手法では交差エントロピー損失関数を利用する。  \n",
    "この損失関数はソフトマックス関数の計算を含むが、全語彙集合に対する計算であることから計算負荷が非常に高い。  \n",
    "そこで、ソフトマックス関数の計算を効率化することを考える。\n",
    "\n",
    "入力情報 $\\mathbf{x}$ と予測対象 $y \\in \\mathcal{Y}$ に対して\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) = - \\log \\frac{\\exp(f_{\\theta}(\\mathbf{x},y))}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp (f_{\\theta}(\\mathbf{x},\\tilde{y}))}\n",
    "\\end{align*}\n",
    "\n",
    "で定義される交差エントロピー損失関数は、  \n",
    "ソフトマックス関数により定義される確率分布の最尤推定を行っているとも解釈できる。  \n",
    "すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "P(y|\\mathbf{x}) &= \\frac{\\exp(f_{\\theta}(\\mathbf{x},y))}{\\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp (f_{\\theta}(\\mathbf{x},\\tilde{y}))} \\\\\n",
    "l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) &= - \\log P(y|\\mathbf{x})\n",
    "\\end{align*}\n",
    "\n",
    "とおくと、損失関数の最小化は対数尤度を最大化していると解釈できる。\n",
    "\n",
    "$\\mathbf{x}, \\theta$ を省略して\n",
    "\n",
    "\\begin{align*}\n",
    "s(y) &= f_{\\theta}(\\mathbf{x},y) \\\\\n",
    "Z(\\mathcal{Y}) &= \\sum_{\\tilde{y} \\in \\mathcal{Y}} \\exp (s(\\tilde{y}))\n",
    "\\end{align*}\n",
    "\n",
    "とおくと\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) = - s(y) + \\log Z(\\mathcal{Y})\n",
    "\\end{align*}\n",
    "\n",
    "と表すことができ、この $Z$ を分配関数（partition function）と呼ぶ。  \n",
    "\n",
    "最適化のため勾配を求める。  \n",
    "$\\nabla s(\\cdot)$ を $s'(\\cdot)$ と書き、$Y$ を $p(\\cdot)$ を確率密度関数とする分布に従う確率変数とするとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) &= - \\nabla s(y) + \\nabla \\log Z(\\mathcal{Y}) \\\\\n",
    "\\nabla \\log Z(\\mathcal{Y}) &= \\sum_{\\tilde{y} \\in \\mathcal{Y}} \\frac{\\exp (s(\\tilde{y}))}{Z(\\mathcal{Y})} s'(\\tilde{y}) \\\\\n",
    "&= \\sum_{\\tilde{y} \\in \\mathcal{Y}} p(\\tilde{y}) s'(\\tilde{y}) \\\\\n",
    "&= \\mathbb{E}_{Y \\sim p} [ s'(Y)] \\\\\n",
    "\\therefore \\, \\nabla l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) &= - \\nabla s(y) + \\mathbb{E}_{p} [ s'(Y)]\n",
    "\\end{align*}\n",
    "\n",
    "となる。  \n",
    "ここで第２項の $\\mathbb{E}_{p} [ s'(Y)]$ が最も計算量を要する項である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 重点サンプリング （importance sampling）\n",
    "　　（参考：[重点サンプリング(1) | 人工知能に関する断創録](http://aidiary.hatenablog.com/entry/20140920/1411207305) ）\n",
    "\n",
    "少数の標本を利用して $\\mathbb{E}_{p} [ s'(Y)]$ の近似計算を行うことを考える。\n",
    "\n",
    "$Y$ の分布 $p$ に従って独立に $T$ 個の標本 $\\{ \\bar{y}_{1}, \\ldots, \\bar{y}_{T}\\}$ を無作為抽出するとき、次のように近似できる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[s'(Y)] \\approx \\frac{1}{T} \\sum_{i=1}^{T} s' (\\bar{y}_{i})\n",
    "\\end{align*}\n",
    "\n",
    "上のようにモンテカルロ法を利用するには $p$ の分布から無作為抽出が効率よく行える必要があるが、  \n",
    "単語の出現確率を求めるには分配関数 $Z$ を計算する必要があり、  \n",
    "近似に必要な計算量が直接計算するのと同等になってしまう場合がある。\n",
    "\n",
    "このとき使われる手法の一つが重点サンプリングである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "無作為抽出が容易な分布の確率密度関数 $q(\\cdot)$ があるとき、  \n",
    "$q$ の分布に従う確率変数 $Y'$ の元での $s'(Y')p(Y')/q(Y')$ の期待値は  \n",
    "$p$ の分布に従う $Y$ の元での $s'(Y)$ の期待値と一致する。  \n",
    "\n",
    "すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{Y \\sim p} [ s'(Y)] = \\mathbb{E}_{Y' \\sim q} \\left[ s'(Y') \\frac{p(Y')}{q(Y')} \\right]\n",
    "\\end{align*}\n",
    "\n",
    "が成り立つ。\n",
    "\n",
    "これは\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{Y' \\sim q} \\left[ s'(Y') \\frac{p(Y')}{q(Y')} \\right] &= \\sum_{\\tilde{y} \\in \\mathcal{Y}}\n",
    "s'(\\tilde{y}) \\frac{p(\\tilde{y})}{q(\\tilde{y})} q(\\tilde{y}) \\\\\n",
    "&= \\sum_{\\tilde{y} \\in \\mathcal{Y}} s'(\\tilde{y}) p(\\tilde{y}) \\\\\n",
    "&= \\mathbb{E}_{Y' \\sim q} [s'(Y)]\n",
    "\\end{align*}\n",
    "\n",
    "として示される。この $q$ の分布を提案分布（proposal distribution）と呼ぶ。  \n",
    "このとき、次のような近似がなされている。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{p}[s'(Y)] \\approx \\frac{1}{T} \\sum_{i=1}^{T} s'(\\bar{y}_{i}) \\frac{p(\\bar{y}_{i})}{q(\\bar{y}_{i})}\n",
    "\\end{align*}\n",
    "\n",
    "このように、元の確率分布とは異なる分布での標本を利用して期待値を近似計算する手法を重点サンプリングと呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "さて、$p(\\cdot)$ の計算には分配関数 $Z(\\mathcal{Y})$ の計算が必要で、これは語彙数に比例した計算量となる。  \n",
    "よって $q$ からの標本に対する総和 $\\hat{Z}$ を利用して $Z$ を近似する。  \n",
    "\n",
    "語彙数を $|\\mathcal{Y}|$ としたときに、$u(x)=1/|\\mathcal{Y}|$ を確率密度関数とする一様分布を考える。  \n",
    "この分布に従う確率変数 $X$ に対して\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{X \\sim u} [ \\exp(s(X))] = \\sum_{\\tilde{x} \\in \\mathcal{V}} \\exp (s(\\tilde{x}))/|\\mathcal{V}|\n",
    "\\end{align*}\n",
    "\n",
    "より\n",
    "\n",
    "\\begin{align*}\n",
    "Z(\\mathcal{Y}) = |\\mathcal{V}| \\mathbb{E}_{u} [ \\exp(s(X))]\n",
    "\\end{align*}\n",
    "\n",
    "と見なすことができる。\n",
    "\n",
    "ここで $Z(\\mathcal{Y})$ の近似値 $\\hat{Z}$ を $q$ の分布のもとで重点サンプリングして\n",
    "\n",
    "\\begin{align*}\n",
    "Z(\\mathcal{Y}) &= |\\mathcal{Y}| \\mathbb{E}_{X \\sim u} [ \\exp(s(X))] \\\\\n",
    "&= |\\mathcal{Y}| \\mathbb{E}_{X \\sim u} [ \\exp(s(Y')) \\frac{u(Y')}{q(Y')}] \\\\\n",
    "&= \\mathbb{E}_{X \\sim u} [ \\frac{\\exp(s(Y'))}{q(Y')}] \\\\\n",
    "&\\approx \\frac{1}{T} \\sum_{i=1}^{T} \\frac{\\exp (s(\\bar{y}_{i}))}{q(\\bar{y}_{i})} \\\\\n",
    "&= \\hat{Z}\n",
    "\\end{align*}\n",
    "\n",
    "とする。また\n",
    "\n",
    "\\begin{align*}\n",
    "p(y) &= \\frac{\\exp (s(y))}{Z(\\mathcal{Y})} \\\\\n",
    "&\\approx \\frac{\\exp (s(y))}{\\hat{Z}}\n",
    "\\end{align*}\n",
    "\n",
    "で近似する。これらを使って\n",
    "\n",
    "\\begin{align*}\n",
    "s'(y) \\frac{p(y)}{q(y)} &\\approx s'(y) \\frac{\\exp(s(y))/\\hat{Z} }{q(y)} \\\\\n",
    "&= s'(y) \\frac{\\exp (s(y))/q(y)}{\\frac{1}{T} \\sum _{i=1}^{T} \\exp (s(\\bar{y}_{i})) / q(\\bar{y}_{i})}\n",
    "\\end{align*}\n",
    "\n",
    "と近似する。  \n",
    "この関数の分布 $q$ における期待値をモンテカルロ近似することで、元の関数の近似を得る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}_{p}[s'(Y)] &= \\mathbb{E}_{q} \\left[ s'(Y') \\frac{p(Y')}{q(Y')} \\right] \\\\\n",
    "&\\approx \\frac{\n",
    "\\sum_{i=1}^{T} s'(\\bar{y}_{i}) \\exp ( s(\\bar{y}_{i}))/q(\\bar{y}_{i})\n",
    "}{\n",
    "\\sum_{i=1}^{T} \\exp ( s(\\bar{y}_{i})) / q(\\bar{y}_{i})\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "上式の $\\bar{y}_{i}$ は $q$ からの標本であり、$\\hat{Z}$ の計算のための標本と同じものである\n",
    "\n",
    "以上から\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla l_{\\theta}^{\\text{sofmax}}(\\mathbf{x},y) &= - \\nabla s(y) + \\mathbb{E}_{p} [ s'(Y)] \\\\\n",
    "&\\approx - \\nabla s(y) + \\frac{\n",
    "\\sum_{i=1}^{T} s'(\\bar{y}_{i}) \\exp ( s(\\bar{y}_{i}))/q(\\bar{y}_{i})\n",
    "}{\n",
    "\\sum_{i=1}^{T} \\exp ( s(\\bar{y}_{i})) / q(\\bar{y}_{i})\n",
    "}\n",
    "\\end{align*}\n",
    "\n",
    "として勾配の近似を求めることができた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 雑音対照推定 （noise contrastive estimation; NCE）\n",
    "上記の重点サンプリングでは分配関数 $Z$ の近似によって計算量を削減したが、  \n",
    "ここでは $Z$ を未知のパラメータとして学習することを考える。\n",
    "\n",
    "さて、新たにパラメータ $c$ をおいて $p(y)=\\exp (s(y)+c)$ かつ $\\sum p(y)=1$ であるように推定する。  \n",
    "これは通常の最尤推定（負の対数尤度 $- \\log p(y)$ の最小化）によっては不可能であるが、  \n",
    "雑音対照推定という手法により推定できることが知られている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "雑音対照推定では、学習対照のデータとノイズ分布からの標本を識別するような分類器を考える。  \n",
    "確率密度関数 $q(\\cdot)$ に従う分布から訓練データとは異なるノイズの標本を得るものとし、  \n",
    "訓練データとノイズの標本は混ざって観測されるものとする。\n",
    "\n",
    "0または1をとる確率変数 $D$ が、訓練データのとき $D=1$ 、ノイズから抽出された標本であるとき $D=0$ とする。  \n",
    "また、ノイズは訓練データよりも $k$ 倍出現しやすいと仮定する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(D=1)=\\frac{1}{(k+1)}, \\, P(D=0)=\\frac{k}{(k+1)}\n",
    "\\end{align*}\n",
    "\n",
    "このとき $D$ と $Y$ の同時確率は\n",
    "\n",
    "\\begin{align*}\n",
    "P(D=1,Y=y) &= \\frac{1}{k+1} p(y) \\\\\n",
    "P(D=0,Y=y) &= \\frac{k}{k+1} q(y)\n",
    "\\end{align*}\n",
    "\n",
    "と書ける。  \n",
    "従って、単語 $Y$ がノイズからの標本である確率は\n",
    "\n",
    "\\begin{align*}\n",
    "P(D=0 \\, | \\, Y=y) &= \\frac{P(D=0,Y=y)}{P(Y=y)} \\\\\n",
    "&= \\frac{\\frac{k}{k+1} q(y)}{\\frac{1}{k+1} p(y) + \\frac{k}{k+1} q(y)} \\\\\n",
    "&= \\frac{kq(y)}{p(y) + kq(y)}\n",
    "\\end{align*}\n",
    "\n",
    "となる。同様にして、訓練データである確率は\n",
    "\n",
    "\\begin{align*}\n",
    "P(D=1 \\, | \\, Y=y) = \\frac{p(y)}{p(y) + kq(y)}\n",
    "\\end{align*}\n",
    "\n",
    "となる。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで１つの訓練データ $y$ に対して、ノイズ分布 $q$ から $k$ 個の標本 $\\bar{\\mathcal{D}} = \\{ \\bar{y}_{1}, \\ldots, \\bar{y}_{k} \\}$ を無作為抽出する。  \n",
    "この $k+1$ 個の事例に対する $D$ の負の対数尤度 $l_{\\theta}^{NCE}(y)$ を最小化する。\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\theta}^{\\text{NCE}}(y) &= - \\log P(D=1|y) - \\sum_{\\bar{y} \\in \\bar{\\mathcal{D}}} \\log P(D=0|\\bar{y}) \\\\\n",
    "&= - \\log \\frac{p(y)}{p(y) + kq(y)} - \\sum_{\\bar{y} \\in \\bar{\\mathcal{D}}} \\log \\frac{kq(\\bar{y})}{p(\\bar{y}) + kq(\\bar{y})}\n",
    "\\end{align*}\n",
    "\n",
    "ここで、$\\theta$ は関数 $p$ のパラメータである。\n",
    "\n",
    "さて、NCEでは正規化パラメータ $c$ を導入して\n",
    "\n",
    "\\begin{align*}\n",
    "p(y)&=\\frac{\\exp(s(y))}{Z(\\mathcal{Y})}\\\\\n",
    "&=\\exp(s(y))\\exp(c)\n",
    "\\end{align*}\n",
    "\n",
    "として学習するが、$Z(\\mathcal{Y})=1$ と仮定しても結果に大きな変化がないことが発見されている。\n",
    "\n",
    "よって $Z=1, p(y)=\\exp(s(y))$ と置き換えて\n",
    "\n",
    "\\begin{align*}\n",
    "l_{\\theta}^{\\text{NCE}}(y) \\approx - \\log \\frac{\\exp(s(y))}{\\exp (s(y)) + kq(y)} - \\sum_{\\bar{y} \\in \\bar{\\mathcal{D}}} \\log \\frac{kq(\\bar{y})}{\\exp (s(\\bar{y})) + kq(\\bar{y})}\n",
    "\\end{align*}\n",
    "\n",
    "を最小化すればよいことになる。\n",
    "\n",
    "NCE のこのような性質は自己正規化（self-normalization）と呼ばれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 負例サンプリング"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
