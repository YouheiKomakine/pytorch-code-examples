{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4　言語処理特有の深層学習の発展"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1　注意機構\n",
    "複数のベクトルがあるとき、どのベクトルを重要視するかも含めて学習させる仕組みを  \n",
    "注意機構（attention mechanism）、あるいは注意と呼ぶ。\n",
    "\n",
    "このうち、複数ベクトルの重み付き平均を使う方法をソフト注意機構（soft attention mechanism）と呼ぶ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ソフト注意機構\n",
    "長さ $I$ の入力系列 $\\{\\mathbf{x}_{1},\\ldots,\\mathbf{x}_{I}\\}$ に対し、各時刻で符号化されたベクトルを $\\{ \\mathbf{h}_{1},\\ldots,\\mathbf{h}_{I}\\}$ とおく。  \n",
    "この計算は、RNN の遷移関数を $\\Psi^{(s)}$ とするとき次の式による。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{i}^{(s)} = \\Psi^{(s)} ( \\mathbf{x}_{i}, \\mathbf{h}_{i-1}^{(s)})\n",
    "\\end{align*}\n",
    "\n",
    "通常の系列変換モデルでは最後の状態出力 $\\mathbf{h}_{I}$ のみを用いるが、\n",
    "ここではより直接的に情報を伝播する方法を考える。\n",
    "\n",
    "復号化器でも RNN により隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$ を計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j}^{(t)} = \\Psi^{(t)}(\\mathbf{y}_{j}, \\mathbf{h}_{j-1}^{(t)})\n",
    "\\end{align*}\n",
    "\n",
    "いま、復号化器が $j$ 番目の単語を推定するときに、  \n",
    "復号化器の $i$ 番目の隠れ状態ベクトル $\\mathbf{h}_{i}^{(s)}$ の重要度を示すスカラー値の重みを $a_{i} \\in \\mathbb{R}$ とする。  \n",
    "$a_{i}$ による $\\mathbf{h}_{i}^{(s)}$ の重み付き平均 $\\bar{\\mathbf{h}}$ を\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{h}} = \\sum_{\\tilde{i}=1}^{I} a_{\\tilde{i}}\\mathbf{h}_{\\tilde{i}^{(s)}}\n",
    "\\end{align*}\n",
    "\n",
    "とする。\n",
    "\n",
    "復号化器が $j$ 番目の単語の予測に $\\bar{\\mathbf{h}}$ を利用するため、$\\mathbf{h}_{j}^{(t)}$ の代わりに次式で定義される $\\hat{\\mathbf{h}}_{j}^{(t)}$ を利用する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{h}}_{j}^{(t)} = \\operatorname{tanh} \\left( \\mathbf{W}^{(a)} \\left[ \\begin{array} \\,\\bar{\\mathbf{h}}\\\\ \\mathbf{h}_{j}^{(t)} \\end{array} \\right] \\right)\n",
    "\\end{align*}\n",
    "\n",
    "$\\{a_{1},\\ldots,a_{I}\\}$ もモデル内で計算を行う。  \n",
    "まず関数 $\\Omega$ によって $\\mathbf{h}_{i}^{(s)}$ と $\\mathbf{h}_{j}^{(t)}$ の間の重み $e_{i}$ を計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "e_{i} = \\Omega(\\mathbf{h}_{i}^{(s)}, \\mathbf{h}_{j}^{(t)})\n",
    "\\end{align*}\n",
    "\n",
    "次に、$a_{i}$ は総和が１になるようにソフトマックス関数で正規化する。\n",
    "\n",
    "\\begin{align*}\n",
    "a_{i} = \\frac{\\exp(e_{i})}{\\sum_{\\tilde{i}=1}^{I} \\exp (e_{\\tilde{i}})}\n",
    "\\end{align*}\n",
    "\n",
    "上で使用した関数 $\\Omega$ については様々な関数が利用されており、以下は一例。\n",
    "\n",
    "\\begin{align*}\n",
    "\\Omega (\\mathbf{h}_{i}^{(s)}, \\mathbf{h}_{j}^{(t)}) = \\begin{cases}\n",
    "\\mathbf{h}_{i} \\cdot \\mathbf{h}_{j}^{(t)} \\\\\n",
    "\\mathbf{h}_{i}^{(s)} \\cdot \\mathbf{Wh}_{j}^{(t)} \\\\\n",
    "\\mathbf{v} \\cdot \\operatorname{tanh} \\left( \\mathbf{W} \\left[ \\begin{array}\\, \\mathbf{h}_{i}^{(s)}\\\\\\mathbf{h}_{j}^{(t)} \\end{array} \\right] \\right)\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "$a_{i}$ は確率として解釈することもでき、  \n",
    "このとき $\\bar{\\mathbf{h}}$ は $\\{a_{1},\\ldots,a_{I}\\}$ の分布における $\\mathbf{h}_{i}^{(s)}$ の期待値と見なせる。\n",
    "\n",
    "より一般化する。  \n",
    "$N$ 個の参照したい情報 $\\mathbf{Y} = \\{ \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\}$ （上の例では符号化器の隠れ状態）があり、  \n",
    "それぞれの重要度、あるいはいずれかが選択される確率 $\\{ a_{1},\\ldots,a_{N}\\}$ を計算する。  \n",
    "$a_{i}$ の計算については何らかの情報 $\\mathbf{c}_{i}$ を用いる（上の例では隠れ状態）。\n",
    "\n",
    "例えば end-to-end 記憶ネットワークでは記憶を参照するのにソフト注意機構を利用しており\n",
    "\n",
    "\\begin{align*}\n",
    "a_{i} = \\frac{\\exp(\\Omega(\\mathbf{c}_{i}))}{\\sum_{i=1}^{I} \\exp (\\Omega (\\mathbf{c}_{\\tilde{i}}))}\n",
    "\\end{align*}\n",
    "\n",
    "を利用して次の通り $\\hat{\\mathbf{y}}$ を得る。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} = \\mathbb{E}_{a}[\\mathbf{y}] = \\sum_{i=1}^{N} a_{i}\\mathbf{y}_{i}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### ハード注意機構\n",
    "ソフト注意機構では複数の情報源が選択される確率に対しその情報の期待値を確定的に計算したのに対し、  \n",
    "ハード注意機構では情報源の中から確率的に１つ選択されるものとして計算する。\n",
    "\n",
    "$N$ 個の参照したい情報 $\\mathbf{Y} = \\{ \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\}$ があり、  \n",
    "確率変数 $X$ が $\\{1,\\ldots,N\\}$ の中から確率的に１つを選択するものとして  \n",
    "この時の確率を $P(X=i) = a_{i}$ とおく。すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "P(X=i)=a_{i}\n",
    "\\end{align*}\n",
    "\n",
    "である。$\\mathbf{Y}$ の中から $X$ を使って無作為に１つ選択された結果を\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{y}X\n",
    "\\end{align*}\n",
    "\n",
    "とする。\n",
    "\n",
    "ハード注意機構を利用した学習で目的関数 $f(\\hat{\\mathbf{y}})$ が計算されたとして、これを最小化する。  \n",
    "$\\hat{\\mathbf{y}}$ および $f(\\hat{\\mathbf{y}})$ は確率的に振る舞うため、期待値 $\\mathbb{E}[f(\\hat{\\mathbf{y}})]$ を最小化の対象とする。  \n",
    "期待値の微分は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\mathbb{E}[f(\\hat{\\mathbf{y}})] &= \\nabla \\sum_{x=1}^{N} f( \\mathbf{y}_{x}) a_{x} \\\\\n",
    "&= \\sum_{x=1}^{N} \\nabla f(\\mathbf{y}_{x}) a_{x} + \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x} \\\\\n",
    "&= \\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}})] + \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x}\n",
    "\\end{align*}\n",
    "\n",
    "いずれの項もすべての $x$ について計算することから計算の負荷が高く、近似が必要となる。  \n",
    "第１項についてはモンテカルロ法を用いる、つまり有限個の標本で次のように近似する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}})] \\approx \\frac{1}{T}\\sum_{i=1}^{T} \\nabla f(\\mathbf{y}_{\\bar{x}_{i}})\n",
    "\\end{align*}\n",
    "\n",
    "第２項についても変形すると期待値の式に変形できるので、モンテカルロ法が適用可能である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{x=1}^{N} f (\\mathbf{y}_{x}) \\nabla a_{x} &= \\sum_{x=1}^{N} f(\\mathbf{y}_{x}) \\nabla a_{x} \\frac{a_{x}}{a_{x}} \\\\\n",
    "&= \\mathbb{E} \\left[ f( \\hat{\\mathbf{y}} ) \\frac{\\nabla a_{x}}{a_{x}} \\right] \\\\\n",
    "&= \\mathbb{E} [ f( \\hat{\\mathbf{y}}) \\nabla \\log a_{x}]\n",
    "\\end{align*}\n",
    "\n",
    "以上により\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla \\mathbb{E}[f(\\hat{\\mathbf{y}})] &= \\mathbb{E}[\\nabla f(\\hat{\\mathbf{y}}) + f( \\hat{\\mathbf{y}}) \\nabla \\log a_{x}] \\\\\n",
    "&\\approx \\frac{1}{T}\\sum_{i=1}^{T} \\{ \\nabla f(\\mathbf{y}_{\\bar{x}_{i}}) + f(\\mathbf{y}_{\\bar{x}_{i}}) \\nabla \\log a_{\\bar{x}_{i}} \\}\n",
    "\\end{align*}\n",
    "\n",
    "となる。\n",
    "\n",
    "なお実用上、標本から計算される $f( \\mathbf{y}_{\\bar{x}_{i}}) \\nabla \\log a_{\\bar{x}_{i}}$ の分散が大きい時に学習できないことがある。  \n",
    "そこで分散を抑えることを考える。定数 $b$ を用いて\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{e}[f( \\hat{\\mathbf{y}} ) \\nabla \\log a_{x}] &= \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}] + \\mathbb{E}[ b \\nabla \\log a_{x}] \\\\\n",
    "&= \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}] + b \\mathbb{E}[\\nabla \\log a_{x}] \\\\\n",
    "\\mathbb{E}[\\nabla \\log a_{x}] &= \\sum_{x=1}^{N} \\nabla a_{x} = \\nabla \\sum_{x}^{N} a_{x}\n",
    "\\end{align*}\n",
    "\n",
    "と変形する。$a_{x}$ は確率値であるから、$ \\sum_{x=1}^{N}a_{x}=1, \\nabla \\sum_{x=1}^{N}a_{x}=0$ である。  \n",
    "よって任意の定数 $b \\in \\mathbb{R}$ について\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[f( \\hat{\\mathbf{y}} ) \\nabla \\log a_{x}] = \\mathbb{E}[(f( \\hat{\\mathbf{y}} ) -b ) \\nabla \\log a_{x}]\n",
    "\\end{align*}\n",
    "\n",
    "が成り立ち、この $b$ をベースラインと呼ぶ。  \n",
    "ベースラインとしては $b=\\mathbb{E}[f( \\hat{\\mathbf{y}})]$ がよく用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "その他の注意機構としては、局所注意機構（local attention mechanism）などがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.2　記憶ネットワーク"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
