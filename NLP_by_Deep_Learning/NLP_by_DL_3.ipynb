{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3　言語処理における深層学習の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1　準備：記号の世界とベクトルの世界の橋渡し\n",
    "離散的な記号である文字や単語などを連続する実数値のベクトルへ変換する方法の１つとして  \n",
    "one-hotベクトルが用いられる。\n",
    "\n",
    "語彙 $\\mathcal{V}$ があり、単語数は $|\\mathcal{V}|$ である。  \n",
    "ここに含まれる各単語に１から $|\\mathcal{V}|$ までの単語番号を割り振ったとする。  \n",
    "$i$ 番目の単語の番号が $n$ のとき、この単語を表すベクトル $\\mathbf{x}_{i}$ は  \n",
    "$n$ 番目の要素が１で残りが０であるような one-hotベクトルで表すこととする。\n",
    "\n",
    "単語などをニューラルネットで処理する場合は、例えば上記の考え方に基づいて  \n",
    "変換行列 $E$ に one-hotベクトル $\\mathbf{x}$ をかけて埋め込みベクトル $\\mathbf{Ex} = \\bar{\\mathbf{x}}$ を取得する。  \n",
    "この行列 $E$ は各列が一つの単語に対応し、これを埋め込み行列（emedding matrix）という。\n",
    "\n",
    "---\n",
    "逆にベクトルから記号へ変換する場合の典型的手法は、\n",
    "語彙数 $|\\mathcal{V}|$ に等しい次元数のベクトル $\\mathbf{o}$ を用意し、\n",
    "要素中で最大の要素番号に対応する単語番号の単語が選択されたとみなして変換することである。\n",
    "\n",
    "スケーリング係数 $a$ を導入したソフトマックス関数\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{softmax_{a}}(\\mathbf{o}) = \\frac{1}{\\exp (a \\mathbf{o}) \\cdot 1} \\exp(a \\mathbf{o})\n",
    "\\end{align*}\n",
    "\n",
    "を考える。\n",
    "式中の $\\mathbf{o}$ は強化学習でいうところの逆温度パラメータである。\n",
    "\n",
    "$\\mathbf{o}$ が $|\\mathcal{V}|$ 次元のベクトルであるとき、$\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ の $j$ 番目の要素 $x_{j}$ は\n",
    "\n",
    "\\begin{align*}\n",
    "s_{j} = \\frac{\\exp(ao_{j})}{\\sum_{j^{\\prime}=1}^{|\\mathcal{V}|} \\exp (ao_{j^{\\prime}})} \\, \\forall j\n",
    "\\end{align*}\n",
    "\n",
    "として計算される。  \n",
    "このときソフトマックス関数のパラメータ $a$ を十分大きい値に設定した場合、  \n",
    "得られるベクトル $\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ は  \n",
    "$\\mathbf{o}$ のすべての要素のうち最大値をとる要素番号の値が１であるようなone-hotベクトルを近似する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2　言語モデル\n",
    "言語モデルあるいは確率的言語モデル（probabilistic language model）は  \n",
    "人間が扱う自然言語で書かれた文や文書が生成される確率をモデル化したものであり、  \n",
    "自然言語らしさを推定する道具となるほか、その確率分布からサンプリングすることで文や文書を生成できる。  \n",
    "\n",
    "以降は言語モデルの基本処理単位を単語と呼称し、入出力は one-hotベクトルとする。\n",
    "\n",
    "長さ $T$ の単語列を $\\mathbf{Y}=(\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\ldots, \\mathbf{y}_{T})$ とする。  \n",
    "このとき単語列 $\\mathbf{Y}$ を生成する確率は $P(\\mathbf{Y})$である。  \n",
    "ある単語列が文を構成していることを明示する仮想単語として文頭を表す BOS、文末を表す EOSを導入し、  \n",
    "文を $\\mathbf{Y}^{\\prime} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ とする。  \n",
    "このとき一般に $P(\\mathbf{Y}) \\neq P(\\mathbf{Y}^{\\prime})$ である。\n",
    "\n",
    "言語モデルとして古典的にはNグラムモデルが最も多く使われた。  \n",
    "これと区別する意味で、ニューラルネットを用いた言語モデルをニューラル言語モデルという。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "BOS,EOSを導入した文を $\\mathbf{Y} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ と表し、文の生成確率 $P(\\mathbf{Y})$ をモデル化する。  \n",
    "直接モデル化は難しいため、文中の各単語の生成確率を、  \n",
    "その単語の前に出現した単語(列)が与えられた条件下で予測するモデルの組み合わせとして定義されるのが一般的である。 \n",
    "\n",
    "ここで文脈（context）を、  \n",
    "　言語モデルにおいてある単語の出現確率を計算する際に用いる周囲の単語  \n",
    "と定義する。\n",
    "\n",
    "単語の位置 $t$ より前に出現した $t-a$ 単語を $\\mathbf{Y}_{[a,t-1]} = (\\mathbf{y}_{a}, \\mathbf{y}_{a+1}, \\ldots, \\mathbf{y}_{t-1})$ と書き、これを文脈として扱う。  \n",
    "このとき文 $\\mathbf{Y}$ の出現確率 $o(\\mathbf{Y})$ を各単語が生成される条件付き確率の積で次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) = P(\\mathbf{y}_{0}) \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "例えば $\\mathbf{Y}=(\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\mathbf{y}_{2})$ について、 $P(X|Y)=P(X,Y)/P(Y)$ より\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) &= P(\\mathbf{y}_{0})P(\\mathbf{y}_{1}|\\mathbf{y}_{0})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P(\\mathbf{y}_{0},\\mathbf{y}_{1})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P (\\mathbf{y}_{0},\\mathbf{y}_{1},\\mathbf{y}_{2})\n",
    "\\end{align*}\n",
    "\n",
    "である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件付き確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{model}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "このモデルにおいて、BOSを使用するなら $P(\\mathbf{y}_{0})=1$ であるから $P_{model}(\\mathbf{y}_{0})$ は省略する。  \n",
    "また、次のように、$t-a$ 単語だけ（$\\mathbf{Y}_{[a,t-1]}$）を使って条件付き確率を表現できると仮定している。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]}) \\approx P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "たとえば $a=t-4$ とするとき、N=5のNグラム言語モデルと同様となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 順伝播型ニューラル言語モデル（FFNN言語モデル）\n",
    "前 $C$ 単語だけを入力とし、FFNNを用いて $t$ 番目の単語の出現確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{ffnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "FFNNへの入力は前 $C$ 単語分のone-hotベクトルを連結した $\\mathbf{Y}_{[t-C,t-1]}$ となる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Y}_{[t-C,t-1]} &= (\\mathbf{y}_{t-C},\\mathbf{y}_{t-C+1},\\ldots,\\mathbf{y}_{t-1}) \\\\\n",
    "&= (\\mathbf{y}_{t^{\\prime}})_{t^{\\prime}=t-C}^{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "例えば、３層FFNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\mathbf{e}_{k}=\\mathbf{Ey}_{k} \\, \\forall k \\in \\{t-C, \\ldots, t-1\\} \\\\\n",
    "\\text{(concatenate) : } \\mbox{ }& \\tilde{\\mathbf{y}}_{t}=\\operatorname{concat} ( \\mathbf{e}_{t-C}, \\ldots, \\mathbf{e}_{t-1}) \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh}(\\mathbf{W}^{(l)}\\tilde{\\mathbf{y}}_{t} + \\mathbf{b}^{(l)}) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 再帰ニューラル言語モデル（RNN言語モデル）\n",
    "入力ベクトル列 $\\mathbf{X}$ は  \n",
    "時刻 $t$ より前の単語列を one-hotベクトル表現したベクトル列 $\\mathbf{Y}_{[0,t-1]} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{t-1})$ となる。  \n",
    "これを $\\mathbf{Y}_{<t} = \\mathbf{Y}_{[0,t-1]}$ と略記する。\n",
    "\n",
    "RNN言語モデルによる文 $\\mathbf{Y}$ の生成確率 $P_{rnnlm}(\\mathbf{Y})$ は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{rnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t})\n",
    "\\end{align*}\n",
    "\n",
    "１層RNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\bar{\\mathbf{y}}_{t}=\\mathbf{Ey}_{t-1} \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh} \\left( \\mathbf{W}^{(l)} \\left[ \\begin{array}\\bar{\\mathbf{y}}_{t} \\\\ \\mathbf{h}_{t-1} \\end{array}  \\right] + \\mathbf{b}^{(l)} \\right) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### パープレキシティ（perplexity; PPL）\n",
    "評価用のデータセットを $\\mathcal{D} = \\{ \\mathbf{Y}^{(n)} \\} _{n=1}^{|\\mathcal{D}|}$、総単語数を $N$、  \n",
    "$n$ 番目の系列の長さを $T^{(n)}$ とする。  \n",
    "このときパープレキシティを次式で計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "&b^{z} &| \\, z=-\\frac{1}{N} \\sum_{n=1}^{|\\mathcal{D}|} \\sum_{t=1}^{T^{(n)}+1} \\log_{b} P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "対数の底 $b$ としては $2$ や $e$ が使われる。  \n",
    "$P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})$ の値域は $[0,1]$ であるから、その対数の値域は $[-\\infty, 0]$、よって $z$ は非負の値となる。\n",
    "\n",
    "パープレキシティは次の単語を予測する確率分布のばらつきを評価しているとみなすことができる。  \n",
    "（モデルに従って正解を選ぶための困難さを示す）  \n",
    "よい言語モデルの確率分布はデータと一致する次の単語にだけ高い確率を与え、ばらつきの小さい分布となる。\n",
    "\n",
    "また、パープレキシティの計算はモデル化の単位によらず計算可能であるので、  \n",
    "例えば文字単位の言語モデルを単語数に対する平均パープレキシティで評価することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルからの文生成\n",
    "記号 $\\sim$ を確率モデルからサンプリングする処理を示すものとするとき、  \n",
    "言語モデル $P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})$ からサンプリングされた $\\hat{\\mathbf{Y}}=(\\mathbf{y}_{0}=BOS, \\hat{\\mathbf{y}}_{1}, \\ldots, \\hat{\\mathbf{y}}_{\\hat{T}}, \\hat{\\mathbf{y}}_{\\hat{T+1}})$ を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}}_{t} \\sim P_{model}(\\mathbf{y}_{t}, \\hat{\\mathbf{Y}}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "これは文頭から順に次の単語を予測する処理ととらえることができ、  \n",
    "文の終わりを示すEOSが予測されたところでサンプリングを終了する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルの単位\n",
    "単位としては単語のほか、文字、バイト対符号化なども用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3　分散表現 (distributed representation)\n",
    "単語など自然言語処理で扱う記号は人間が恣意的に定義したものであり、  \n",
    "記号そのものの情報から記号間の類似度や関連性を直接計算することが難しい。\n",
    "\n",
    "**離散オブジェクト**を次のように定義する。  \n",
    "　人や物の名前、概念のように、物理的に計測できる量を伴わず、  \n",
    "　通常、記号を用いて離散的に表現するもの。\n",
    "\n",
    "**分散表現**はNLPの文脈では、一般的に次の意味で用いられる。  \n",
    "　任意の離散オブジェクトの集合 $\\mathcal{V}$ に対して、各離散オブジェクト $v \\in \\mathcal{V}$ に  \n",
    "　それぞれ $D$ 次元ベクトルを割り当て、離散オブジェクトを $D$ 次元ベクトルで表現したもの。\n",
    " \n",
    "分散表現への変換処理は、離散オブジェクトをベクトル空間内に埋め込む操作と捉えることもできる。   \n",
    "ベクトル空間では距離や演算が定義できることから、    \n",
    "これを離散オブジェクト間の類似度や関連性を計算する道具として用いることができる。\n",
    "\n",
    "単語に着目した分散表現を単語分散表現、あるいは単語埋め込みと呼ぶこともある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 歴史的背景\n",
    "・認知心理学や神経科学の分野では、脳のモデル化の方法論として分散表現が考えられた。  \n",
    "　分散表現の対義語として局所表現（one-hotベクトルなどが該当）が用いられる。\n",
    " \n",
    "・自然言語処理研究では、単語の持つ意味（semantics）を扱う方法の１つとして分布仮説が提案された。  \n",
    "　この仮説は、単語の意味は出現した際の周囲の単語によって決まるという考え方である。  \n",
    "　その手法としては主成分分析や潜在ディレクレ配分などがあり、  \n",
    "　分布仮説に基づいた方法論で得られた単語のベクトル表現を分散表現（distributional representation）と呼ぶ。\n",
    " \n",
    "・ニューラル言語モデルでは両分野の概念が同時に利用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 分散表現の獲得方法\n",
    "１．ニューラル言語モデルを学習することで、変換行列が分散表現に相当する値へ更新される\n",
    "\n",
    "２．対数線形性モデル（log-bilinear model; LBL model）を用いる  \n",
    "　word2vecおよび対数線形性モデルと呼ばれるほぼ同じモデルが同時期に提案された。  \n",
    "　（word2vecというツールにはskip-gramモデルおよびCBoWモデルが実装されている）\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### skip-gram および CBoWについて\n",
    "語彙を $\\mathcal{V}$、$i$ 番目の単語番号の入力単語 $\\mathbf{x}_{i}$ に割り当てる単語埋め込みベクトルを $\\mathbf{e}_{i}$、  \n",
    "$j$ 番目の単語番号の出力単語 $\\mathbf{y}_{j}$ に割り当てる単語埋め込みベクトルを $\\mathbf{o}_{j}$ で表す。  \n",
    "ここで $1 \\leq i \\leq |\\mathcal{V}|$、$1 \\leq j \\leq |\\mathcal{V}|$ である。\n",
    "\n",
    "$\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ を $1$ から $|\\mathcal{V}|$ まで並べて行列表示とみなすと、 $\\mathbf{E} = (\\mathbf{e}_{i})_{i=1}^{|\\mathcal{V}|}$、$\\mathbf{O} = (\\mathbf{o}_{j})_{j=1}^{|\\mathcal{V}|}$ と書ける。  \n",
    "ここで $\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ は $D$ 次元の列ベクトルとする。\n",
    "\n",
    "**< CBoW >**  \n",
    "入力単語のリストを $\\mathcal{H}$ とする。   \n",
    "$\\mathcal{H}$ が与えられた時に $j$ 番目の出力単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{H}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{H},\\mathbf{y}_{j}) &= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}}(\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}} \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "**< skip-gram >**  \n",
    "入力語彙 $\\mathcal{V}$ 中の $i$ 番目の単語 $\\mathbf{x}_{i}$ が与えられた時に  \n",
    "出力語彙 $\\mathcal{V}$ 中の $j$ 番目の単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{x}_{i},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{x}_{i}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{x}_{i},\\mathbf{y}_{j}) &= (\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式から、skip-gramは $\\mathcal{H} = \\{ \\mathbf{x}_{i} \\}$ であるとき（入力単語リスト長が常に１）のCBoWと見なせることが分かる。  \n",
    "また、対数双線形モデルにてCBoWやskip-gramを統一的に記述できることが分かっている。  \n",
    "\n",
    "対数双線形モデルの式はFFNNの出力層と同じ形のため、  \n",
    "これを１層NNによるニューラル言語モデルとみなして実装することができる。\n",
    "\n",
    "訓練データを $\\mathcal{D} = \\{ (\\mathcal{H}^{(n)}, \\mathbf{y}_{j}^{(n)})\\}_{n=1}^{N}$ とするとき、学習時の目的関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P(\\mathbf{y}_{j}|\\mathcal{H})) \\\\\n",
    "&= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\phi (\\mathcal{H},\\mathbf{y}_{j}) + \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log \\, \\left( \\sum_{\\mathbf{y}_{j^{\\prime}}\\in\\mathcal{V}} \\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j^{\\prime}})) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "この数式は語彙 $\\mathcal{V}$ 全てに基づいて計算されることから負荷が高く、  \n",
    "階層的ソフトマックス等を用いた対処が行われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 負例サンプリングによる獲得方法\n",
    "負例サンプリングによる学習は、与えられた入出力の組が  \n",
    "実データの確率分布から取得されたデータか、別の確率分布から生成されたデータかの  \n",
    "２クラスを判別する識別モデルの学習に相当する。\n",
    "\n",
    "$P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})$ を、学習用の１つのデータ $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "実際の訓練データ $\\mathcal{D}$ を生成する確率分布 $P^{\\mathcal{D}}$ から生成された場合を表す確率モデルとし、  \n",
    "以下のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}}) = \\frac{1}{1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j}))}\n",
    "\\end{align*}\n",
    "\n",
    "同様に、 $P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})$ を $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "ノイズデータ $\\mathcal{D}^{\\prime}$ を生成する確率分布 $P^{\\mathcal{D}^{\\prime}}$ から生成された場合を表す確率モデルとする。\n",
    "\n",
    "このとき、実データとノイズの識別問題の対数尤度関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})\n",
    "+ \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})\n",
    "\\end{align*}\n",
    "\n",
    "これら２式より\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "- \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "\\end{align*}\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.4　系列変換モデル (seq2seq model)\n",
    "自然言語処理において文から分への変換と見なせるタスクとして、機械翻訳、対話、質問応答などがある。  \n",
    "これらのタスクは系列（sequence）から系列への変換とみなすことができる。\n",
    "\n",
    "ここで、系列を別の系列へ変換する確率をモデル化したものを  \n",
    "**系列変換モデル**（sequene-to-sequence model）と呼ぶ。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力系列を $\\mathbf{X}$、出力系列を $\\mathbf{Y}$ で表す。  \n",
    "入力系列中の $i$ 番目の要素を $\\mathbf{x}_{i}$、出力系列中の $j$ 番目の要素を $\\mathbf{y}_{j}$ とし、  \n",
    "それぞれ one-hot ベクトルであり、対応する離散オブジェクトを変換したものとする。\n",
    "\n",
    "入力側の語彙を $\\mathcal{V}^{(s)}$、出力側の語彙を $\\mathcal{V}^{(t)}$ とすると、全ての $i,j$ に対して $\\mathbf{x}_{i} \\in \\mathbb{R}^{|\\mathcal{V}^{(s)}|}, \\mathbf{y}_{j} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}|}$ である。  \n",
    "また、入力分長を $I$、出力文長を $J$ とする。\n",
    "\n",
    "このとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{X} = (\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{i}, \\ldots, \\mathbf{x}_{I}) = (\\mathbf{x}_{i})_{i=1}^{I} \\\\\n",
    "\\mathbf{Y} = (\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{j}, \\ldots, \\mathbf{y}_{J}) = (\\mathbf{y}_{j})_{j=1}^{J}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{y}_{0} = \\mathbf{y}^{(BOS)}$ はBOSに対応する one-hot ベクトル、  \n",
    "$\\mathbf{y}_{J+1} = \\mathbf{y}^{(EOS)}$ はEOSに対応する one-hot ベクトルであり、  \n",
    "出力側の語彙 $\\mathcal{V}^{(t)}$ にはBOS, EOSが含まれていることを仮定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある入力系列 $\\mathbf{X}$ が与えられた時に、ある出力系列 $\\mathbf{Y}$ へ変換する条件付き確率 $P(\\mathbf{Y}|\\mathbf{X})$ を考える。  \n",
    "これをモデル化したものが系列変換モデルで、ここでは次のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}|\\mathbf{X}) = \\prod_{j=1}^{J+1} P_{\\theta}(\\mathbf{y}_{j} | \\mathbf{Y}_{<j}, \\mathbf{X})\n",
    "\\end{align*}\n",
    "\n",
    "入力 $\\mathbf{X}$ を受け取って（固定長の）符号ベクトル $\\mathbf{z}$ を生成する処理を次のように表す。  \n",
    "関数 $\\Lambda$ にはRNNを使用することが一般的である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{z} = \\Lambda (\\mathbf{X})\n",
    "\\end{align*}\n",
    "\n",
    "符号ベクトル $\\mathbf{z}$ を受け取って出力 $\\mathbf{Y}$ を生成する処理は再帰ニューラル言語モデルと同じで、  \n",
    "隠れ状態ベクトルを生成する関数を $\\Psi$、  \n",
    "単語の生成確率を返す関数を $\\Upsilon$ として次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{y}_{j}|\\mathbf{Y}_{<j}, \\mathbf{X}) &= \\Psi(\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\Upsilon ( \\mathbf{h}_{j-1}^{(t)}, \\mathbf{y}_{j-1})\n",
    "\\end{align*}\n",
    "\n",
    "ただし $j=1$ のとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j-1}^{(t)} &= \\mathbf{h}_{0}^{(t)} = \\mathbf{z} \\\\\n",
    "\\mathbf{y}_{j-1} &= \\mathbf{y}_{0} = \\mathbf{y}^{(BOS)}\n",
    "\\end{align*}\n",
    "\n",
    "である。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$j=1$ の場合（初期値の設定）を除き、系列変換モデルと再帰ニューラル言語モデルの処理は等しい。  \n",
    "ゆえに系列変換モデルを条件付き（再帰ニューラル）言語モデルとして認識することができる。  \n",
    "この視点では、系列変換モデルは次のような条件付き確率のモデル化である。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}|\\mathbf{X}) &= \\prod_{j=1}^{J+1} P_{\\theta} (\\mathbf{y}_{j}|\\mathbf{Y}_{<j},\\mathbf{X}) \\\\\n",
    "P_{\\theta} (\\mathbf{y}_{j} | \\mathbf{Y}_{<j},\\mathbf{X}) &= \\Upsilon (\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\left\\{ \n",
    "\\begin{array}\n",
    " \\,\\Psi (\\mathbf{z},\\mathbf{y}^{(BOS)}) & \\text{if} \\, j=1 \\\\\n",
    " \\,\\Psi (\\mathbf{h}_{j-1}^{(t)},\\mathbf{y}_{j-1}) & \\text{otherwise}\n",
    "\\end{array} \\right.\\\\\n",
    "\\mathbf{z} &= \\Lambda (\\mathbf{X})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再帰ニューラル言語モデルを同形式で表すと次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}) &= \\prod_{j=1}^{J+1} P_{\\theta^{\\prime}} (\\mathbf{y}_{j}|\\mathbf{Y}_{<j}) \\\\\n",
    "P_{\\theta^{\\prime}} (\\mathbf{y}_{j} | \\mathbf{Y}_{<j}) &= \\Upsilon (\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\left\\{ \n",
    "\\begin{array}\n",
    " \\,\\Psi (0,\\mathbf{y}^{(BOS)}) & \\text{if} \\, j=1 \\\\\n",
    " \\,\\Psi (\\mathbf{h}_{j-1}^{(t)},\\mathbf{y}_{j-1}) & \\text{otherwise}\n",
    "\\end{array} \\right.\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 系列変換モデルの構造\n",
    "\n",
    "１．符号化器埋め込み層（encoder embedding layer）  \n",
    "２．符号化器再帰層（encoder recurrent layer）  \n",
    "３．復号化器埋め込み層（decoder embedding layer）  \n",
    "４．復号化器再帰層（decoder recurrent layer）  \n",
    "５．復号化器出力層（decoder output layer）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (1)符号化器埋め込み層\n",
    "入力：入力文中の $i$ 番目の単語を意味する one-hotベクトル $\\mathbf{x}_{i}$  \n",
    "出力：入力文中の $i$ 番目の単語に対応する埋め込みベクトル $\\bar{\\mathbf{x}}_{i}$  \n",
    "　について、$i=1 \\sim I$ まで順に（あるいは一括で）処理する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{x}}_{i} = \\mathbf{E}^{(s)}\\mathbf{x}_{i} \\hspace{20px} \\forall i\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{E}^{(s)} \\in \\mathbb{R}^{\\mathcal{D} \\times |\\mathcal{V}^{(s)}| }$ は入力語彙に対する埋め込み行列に相当する。  \n",
    "埋め込みベクトル獲得処理は $i$ に依存しないため一括して処理が可能である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{X}} = \\mathbf{E}^{(s)}\\mathbf{X}\n",
    "\\end{align*}\n",
    "\n",
    "このとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{X}} = (\\bar{\\mathbf{x}}_{1}, \\ldots, \\bar{\\mathbf{x}}_{I}) = (\\bar{\\mathbf{x}}_{i})_{i=1}^{I}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (2)符号化器再帰層\n",
    "入力：入力文中の $i$ 番目の単語に対応する埋め込みベクトル $\\bar{\\mathbf{x}}_{i}$  \n",
    "出力：隠れ状態ベクトル $\\mathbf{h}_{i}^{(s)}$\n",
    "\n",
    "説明のため、この処理を１層単方向RNN、活性化関数tanhでモデル化したとする。  \n",
    "$\\Psi^{(s)}(\\cdot)$ を再帰ニューラルネットの処理を表す関数とし、出力される位置 $i$ の隠れ状態ベクトルを $\\mathbf{h}_{i}^{(s)}$ で表す。  \n",
    "このとき符号化器再帰層での処理は次の式に相当する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{i}^{(s)} &= \\Psi^{(s)} ( \\bar{\\mathbf{x}}_{i}, \\mathbf{h}_{i-1}^{(s)} ) \\\\\n",
    "\\Psi^{(s)} ( \\bar{\\mathbf{x}}_{i}, \\mathbf{h}_{i-1}^{(s)} ) &= \\operatorname{tanh} \\left(\n",
    "\\mathbf{W}^{(s)} \\left[ \\begin{array} \\mathbf{h}_{i-1}^{(s)} \\\\ \\bar{\\mathbf{x}}_{i} \\end{array}\n",
    "\\right] + \\mathbf{b}^{(s)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "ただし $\\mathbf{W}^{(s)} \\in \\mathbb{R}^{H \\times (H+D)}, \\mathbf{b}^{(s)} \\in \\mathbb{R}^{H}$ であり、  \n",
    "$i=0$ のとき $\\mathbf{h}_{i}^{(s)} = \\mathbf{h}_{0}^{(s)} = \\mathbf{0}$ となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (3)復号化器埋め込み層\n",
    "復号化器では位置 $j$ の処理に位置 $j-1$ の処理結果を利用することから、通常は $j$ について一括処理できない。  \n",
    "ただし学習時には $\\mathbf{Y}$ が訓練データとして事前に与えられるため一括処理として実装できる。\n",
    "\n",
    "入力：復号化器出力層で選択された出力 $\\mathbf{y}_{j-1}$  \n",
    "出力：埋め込みベクトル $\\bar{\\mathbf{y}}_{j}$  \n",
    "　について、位置 $j$ で以下の計算を行う。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{y}}_{j} = \\mathbf{E}^{(t)} \\mathbf{y}_{j-1}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{E}^{(t)} \\in \\mathbb{R}^{\\mathcal{D} \\times |\\mathcal{V}^{(t)}| }$ は復号化器埋め込み層の埋め込み行列である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (4)復号化器再帰層\n",
    "入力：復号化器埋め込み層の出力に対応する埋め込みベクトル $\\bar{\\mathbf{y}}_{j}$  \n",
    "出力：隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$  \n",
    "\n",
    "関数 $\\Psi^{(t)}(\\cdot)$ を復号化器再帰層で行う処理を表す関数とするとき、  \n",
    "各位置 $j$ での処理は次のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j}^{(t)} &= \\Psi^{(t)} (\\bar{\\mathbf{y}}_{j}, \\mathbf{h}_{j-1}^{(t)} ) \\\\\n",
    "\\Psi^{(t)}(\\bar{\\mathbf{y}}_{j},\\mathbf{h}_{j-1}^{(t)}) &= \\operatorname{tanh} \\left( \\mathbf{W}^{(t)} \\left[ \\begin{array}\\mathbf{h}_{j-1}^{(t)} \\\\ \\bar{\\mathbf{y}}_{j} \\end{array} \\right] + \\mathbf{b}^{(t)}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "ただし $\\mathbf{W}^{(t)} \\in \\mathbb{R}^{H \\times (H+D)}, \\mathbf{b}^{(t)} \\in \\mathbb{R}^{H}$ である。\n",
    "\n",
    "また、符号化器再帰層の最後の位置の隠れ状態ベクトル $\\mathbf{h}_{I}^{(s)}$ を  \n",
    "復号化器再帰層の初期状態 $j=0$ における $\\mathbf{h}_{0}^{(t)}$ として用いる。  \n",
    "すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{0}^{(t)} = \\mathbf{h}_{I}^{(s)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (5)復号化器出力層\n",
    "入力：位置 $j$ での復号化器再帰層の隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$  \n",
    "出力：$\\mathbf{y}_{j}$ が生成される確率 $p_{j}$\n",
    "\n",
    "出力層の計算は学習時と出力系列の予測時で処理が若干異なる。\n",
    "\n",
    "共通処理として、スコアベクトル $\\mathbf{o}_{j}$ を次のとおり計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{o}_{j} = \\mathbf{W}^{(o)}\\mathbf{h}_{j}^{(t)} + \\mathbf{b}^{(o)}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{W}^{(o)} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}| \\times H}$ と $\\mathbf{b}^{(o)} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}|}$ は、出力層内の変換行列とバイアス項のベクトルである。\n",
    "\n",
    "【学習時】  \n",
    "次に、学習時の場合は、訓練データとモデルの適合の度合いをみるため確率計算処理を行う。  \n",
    "$j$ 番目の単語 $\\mathbf{y}_{j}$ の生成確率を次の式で計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{y}_{j} | \\mathbf{Y}_{<j}) = \\operatorname{softmax}(\\mathbf{o}_{j}) \\cdot \\mathbf{y}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "ソフトマックス関数によりスコアのベクトル表現が各出力語彙の確率に変換されるので、  \n",
    "one-hot ベクトル $\\mathbf{y}_{j}$ との内積を取ることで $j$ 番目の単語の生成確率を得ることができる。\n",
    "\n",
    "【予測時】  \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}}_{j} = \\operatorname{softmax}_{a}(\\mathbf{o}_{j})\n",
    "\\end{align*}\n",
    "\n",
    "により、パラメータ $a$ を十分大きい値としたとき、  \n",
    "ベクトル $\\mathbf{o}_{j}$ の中で最大の要素が１でそれ以外が０であるような one-hot ベクトルの近似が得られるので、  \n",
    "これを単語を選択する処理として用いることができる。\n",
    "\n",
    "$\\mathbf{y}_{j}$ は one-hot ベクトルとなるので、  \n",
    "これを次の処理位置の入力として復号化器埋め込み層の処理に戻る。  \n",
    "このような３種からなるループ処理を、終了信号を受け取るまで繰り返し行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 系列変換モデルのパラメータ数\n",
    "仮に $D=200, H=500, |\\mathcal{V}^{(s)}|=20,000, |\\mathcal{V}^{(t)}|=10,000$ とする。  \n",
    "このとき各パラメータ数と合計は\n",
    "\n",
    "|parameter|calculation|number of parameters|\n",
    "|:--|:--|:--|\n",
    "|$\\mathbf{E}^{(s)}$|$D \\times |\\mathcal{V}^{(s)}|$|4,000,000|\n",
    "|$\\mathbf{W}^{(s)}$|$H \\times (H+D)$|350,000|\n",
    "|$\\mathbf{b}^{(s)}$|$H$|500|\n",
    "|$\\mathbf{E}^{(t)}$|$D \\times |\\mathcal{V}^{(t)}|$|2,000,000|\n",
    "|$\\mathbf{W}^{(t)}$|$H \\times (H+D)$|350,000|\n",
    "|$\\mathbf{b}^{(t)}$|$H$|500|\n",
    "|$\\mathbf{W}^{(o)}$|$H \\times |\\mathcal{V}^{(t)}|$|5,000,000|\n",
    "||||\n",
    "|Sum||11,701,000|\n",
    "\n",
    "語彙数や隠れ層の次元数は大きなものを使うことが多く、パラメータ数が億の単位に乗ることも普通である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 確率モデルの学習法\n",
    "系列変換モデル $P_{\\theta}(\\mathbf{Y}|\\mathbf{X})$ の学習のために損失関数を定義する。  \n",
    "これは条件付き再帰ニューラル言語モデルの一種とみなせば、負の対数尤度を損失関数とすることができる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{\\theta}} &= \\operatorname{argmin}_{\\theta} \\{ \\Phi (\\theta,\\mathcal{D}) \\} \\\\\n",
    "\\Phi(\\theta,\\mathcal{D}) &= - \\sum_{(\\mathbf{X},\\mathbf{Y})\\in\\mathcal{D}} \\log P_{\\theta}(\\mathbf{Y},\\mathbf{X})\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 系列生成方法：ビーム探索／貪欲法\n",
    "学習済みの系列変換モデルを用いて、ある入力文 $\\mathbf{X}$ に対して最良の出力文 $\\hat{\\mathbf{Y}}$ を生成する。  \n",
    "$\\mathcal{Y}(\\mathbf{X})$ を入力 $\\mathbf{X}$ に対して可能なすべての出力 $\\mathbf{Y}$ の集合とするとき、式は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{Y}} = \\operatorname{argmax}_{\\mathbf{Y} \\in \\mathcal{Y}(\\mathbf{X})} P_{\\theta}(\\mathbf{Y}|\\mathbf{X})\n",
    "\\end{align*}\n",
    "\n",
    "系列変換モデルでは出力分の長さに制約がないことから、$\\mathcal{Y}(\\mathbf{X})$ は無限集合となり、計算不能である。  \n",
    "上式の近似解を求める手法として、貪欲法（greedy algorithm）およびビーム探索（beam search）がよく用いられる。\n",
    "\n",
    "\n",
    "#### 貪欲法\n",
    "実際に解きたい問題を任意の複数の部分問題に分割し、その部分問題の評価値が最も高い順番に逐次決定する方法である。  \n",
    "系列変換モデルでは、復号化器において各 $j$ において単語を１つ選択する処理を部分問題として、  \n",
    "各 $j$ において最適な $\\mathbf{y}_{j}$ を逐次選択する。\n",
    "\n",
    "\n",
    "#### ビーム探索\n",
    "貪欲方法と同様にここの部分問題を順番に解いていく方法であるが、  \n",
    "各部分問題の評価値が最大のものを１つ選ぶ代わりに、事前に決めた上位 $K$ 個の候補を保持して処理を行う。  \n",
    "\n",
    "$S$を累積対数尤度、$\\hat{\\mathbf{Y}}$ を生成文、$\\mathbf{H}^{(t)}$ を復号化処理に必要な隠れ状態のリスト、  \n",
    "$\\mathcal{Q}_{W}$ を優先度付きキューとする。  \n",
    "このとき典型的な $K$ ベストビーム探索の擬似コードは次の通り。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\hline{}\n",
    "Initialize: & S \\leftarrow 0 \\\\\n",
    "& \\hat{\\mathbf{Y}} \\leftarrow \\text{'BOS'} \\\\\n",
    "& \\mathbf{H}^{(t)} \\leftarrow \\mathbf{H}^{(s)} \\\\\n",
    "1: & h \\leftarrow (S, \\hat{\\mathbf{Y}}, \\mathbf{H}^{(t)}) \\\\\n",
    "2: & \\mathcal{Q}_{W} \\leftarrow \\operatorname{push}(\\mathcal{Q}_{new}, h) \\\\\n",
    "3: & \\textbf{Repeat} \\\\\n",
    "4: &\\hspace{8pt} \\textbf{For} \\hspace{4pt} k \\in \\{ 1, \\ldots, \\operatorname{size}(\\mathcal{Q}_{W})\\} \\hspace{4pt} \\textbf{do} \\\\\n",
    "5: &\\hspace{16pt} h \\leftarrow \\mathcal{Q}_{W}[k] \\\\\n",
    "6: &\\hspace{16pt} \\textbf{If} \\hspace{4pt} \\operatorname{isCSent}(h) \\\\\n",
    "7: &\\hspace{24pt} \\mathbf{q}^{(k)} \\leftarrow \\mathbf{0} \\\\\n",
    "8: &\\hspace{16pt} \\textbf{Else} \\\\\n",
    "9: &\\hspace{24pt} \\mathbf{q}^{(k)} \\leftarrow \\operatorname{calcLL}(h) \\\\\n",
    "10: &\\hspace{16pt} \\textbf{End_if} \\\\\n",
    "11: &\\hspace{16pt} \\mathbf{s}^{(k)} \\leftarrow \\operatorname{calcScore}(\\mathbf{q}^{(k)}, h) \\\\\n",
    "12: &\\hspace{8pt} \\textbf{End_For} \\\\\n",
    "13: &\\hspace{8pt} C_{K} \\leftarrow \\operatorname{findKBest}(S_{j}, K) \\\\\n",
    "14: &\\hspace{8pt} \\mathcal{Q}_{W} \\leftarrow \\operatorname{makeCand}(C_{K}) \\\\\n",
    "15: & \\bf Until \\hspace{4pt} \\rm isALLCSent(\\mathcal{Q}_{W}) \\\\\n",
    "Output: \\hspace{4pt} \\mathcal{Q}_{W} \\\\\n",
    "\\hline{}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず時刻 $j-1$ で得られた $K$ ベスト解候補の中で、$k$ 番目の候補に対して、  \n",
    "時刻 $j$ の処理を行うのに必要な情報を $(s_{k,j-1}, \\hat{\\mathbf{Y}}_{k,j-1}, \\mathbf{H}_{k,j-1}^{(t)})$ という情報の組で表す。  \n",
    "ここで  \n",
    "　$s_{k,j-1}$：時刻 $j-1$ での $k$ 番目の出力候補の累積対数尤度  \n",
    "　$\\hat{\\mathbf{Y}}_{k,j-1}$：時刻 $j-1$ で $k$ 番目の出力候補の部分文  \n",
    "　$\\mathbf{H}_{k,j-1}^{(t)}$：$k$ 番目の出力候補を使って時刻 $j$ の処理を行うのに必要な隠れ状態ベクトルのリスト\n",
    "\n",
    "次に、時刻 $j-1$ までの予測結果 $\\hat{\\mathbf{Y}}_{k,j-1}$ と $j-1$ の隠れ状態のリスト $\\mathbf{H}_{k,j-1}^{(t)}$ を使って  \n",
    "時刻 $j$ の出力単語候補の対数尤度を計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{q}_{j}^{(k)} = \\log (\\operatorname{softmax}(\\mathbf{o}_{j}^{(k)}))\n",
    "\\end{align*}\n",
    "\n",
    "この $\\mathbf{q}_{j}^{(k)}$ に $j-1$ までのスコア $s_{k,j-1}$ を加算して各単語の累積対数尤度を求める。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{s}_{j}^{(k)} = \\mathbf{q}_{j}^{(k)} + s_{k,j-1} \\hspace{12pt} \\forall k \\in \\{ 1, \\ldots, K \\}\n",
    "\\end{align*}\n",
    "\n",
    "時刻 $j-1$ で得られた $K$ 個の候補それぞれから得られた  \n",
    "時刻 $j$ の累積対数尤度を表す行列 $\\mathbf{S}_{j} = \\{ \\mathbf{s}_{j}^{(k)} \\}_{k=1}^{K}$ の中から  \n",
    "最もスコアの大きかった上位 $K$ 個の要素を抽出する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\{ ( \\hat{k}, \\hat{m})_{k'} \\}_{k'=1}^{K} = \\operatorname{KBest}_{1 \\leq m \\leq |\\mathcal{V}^{(t)}|, 1 \\leq k \\leq K} (\\mathbf{S}_{j})\n",
    "\\end{align*}\n",
    "\n",
    "個々の出力 $( \\hat{k}, \\hat{m})$ は $|\\mathcal{V}^{(t)}| \\times K$ 行列 $\\mathbf{S}_{j}$ の要素番号を表しており、  \n",
    "単語番号 $m$ の単語が選択され、そのときの文脈が $j-1$ 番目の $k$ 番目の候補であることを表している。  \n",
    "この情報に基づき、$(s_{k,j}, \\hat{\\mathbf{Y}}_{k,j}, \\mathbf{H}_{k,j}^{(t)})$ を選択された上位 $K$ 個分生成する。\n",
    "\n",
    "ビーム探索は貪欲法に比べ、局所的には最良でなくとも系列全体でみれば尤度をより高くするような  \n",
    "出力候補を保持できるため、系列全体での最適な解を得やすくなる。  \n",
    "一方で計算コストは $K$ 倍となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 符号化と復号化の応用\n",
    "系列変換モデルを利用した機械翻訳はニューラル翻訳（NMT）と呼ばれており、  \n",
    "良好な翻訳精度が達成されている。  \n",
    "\n",
    "また、対話システムへの応用も知られている。\n",
    "\n",
    "系列として扱えるデータであれば文でなくとも扱えるため、  \n",
    "例えば木構造を $S$ 式（S-expression）で扱う構文解析などの応用が可能である。  \n",
    "入力文から $S$ 式のトークンの系列への変換として系列変換モデルを学習させると  \n",
    "既存の構文解析手法に迫る精度が出ることが報告されている。\n",
    "\n",
    "画像を入力すると文を出力するように扱うことも可能で、説明文生成（image captioning）と呼ばれる。  \n",
    "show-and-tell などの典型的なモデルでは CNN により画像を符号化し再帰ニューラルネットで復号化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encoder-decoder model\n",
    "seq2seq や符号化復号化モデル（encoder-decoder model）は\n",
    "概念的には系列変換モデルと同じである。\n",
    "\n",
    "end-to-end 学習という呼び名もこれらと同一概念を指すために使われることがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "次のnotebookへ続く。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
