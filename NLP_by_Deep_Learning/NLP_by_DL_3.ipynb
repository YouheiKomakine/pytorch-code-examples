{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3　言語処理における深層学習の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1　準備：記号の世界とベクトルの世界の橋渡し\n",
    "離散的な記号である文字や単語などを連続する実数値のベクトルへ変換する方法の１つとして  \n",
    "one-hotベクトルが用いられる。\n",
    "\n",
    "語彙 $\\mathcal{V}$ があり、単語数は $|\\mathcal{V}|$ である。  \n",
    "ここに含まれる各単語に１から $|\\mathcal{V}|$ までの単語番号を割り振ったとする。  \n",
    "$i$ 番目の単語の番号が $n$ のとき、この単語を表すベクトル $\\mathbf{x}_{i}$ は  \n",
    "$n$ 番目の要素が１で残りが０であるような one-hotベクトルで表すこととする。\n",
    "\n",
    "単語などをニューラルネットで処理する場合は、例えば上記の考え方に基づいて  \n",
    "変換行列 $E$ に one-hotベクトル $\\mathbf{x}$ をかけて埋め込みベクトル $\\mathbf{Ex} = \\bar{\\mathbf{x}}$ を取得する。  \n",
    "この行列 $E$ は各列が一つの単語に対応し、これを埋め込み行列（emedding matrix）という。\n",
    "\n",
    "---\n",
    "逆にベクトルから記号へ変換する場合の典型的手法は、\n",
    "語彙数 $|\\mathcal{V}|$ に等しい次元数のベクトル $\\mathbf{o}$ を用意し、\n",
    "要素中で最大の要素番号に対応する単語番号の単語が選択されたとみなして変換することである。\n",
    "\n",
    "スケーリング係数 $a$ を導入したソフトマックス関数\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{softmax_{a}}(\\mathbf{o}) = \\frac{1}{\\exp (a \\mathbf{o}) \\cdot 1} \\exp(a \\mathbf{o})\n",
    "\\end{align*}\n",
    "\n",
    "を考える。\n",
    "式中の $\\mathbf{o}$ は強化学習でいうところの逆温度パラメータである。\n",
    "\n",
    "$\\mathbf{o}$ が $|\\mathcal{V}|$ 次元のベクトルであるとき、$\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ の $j$ 番目の要素 $x_{j}$ は\n",
    "\n",
    "\\begin{align*}\n",
    "s_{j} = \\frac{\\exp(ao_{j})}{\\sum_{j^{\\prime}=1}^{|\\mathcal{V}|} \\exp (ao_{j^{\\prime}})} \\, \\forall j\n",
    "\\end{align*}\n",
    "\n",
    "として計算される。  \n",
    "このときソフトマックス関数のパラメータ $a$ を十分大きい値に設定した場合、  \n",
    "得られるベクトル $\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ は  \n",
    "$\\mathbf{o}$ のすべての要素のうち最大値をとる要素番号の値が１であるようなone-hotベクトルを近似する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2　言語モデル\n",
    "言語モデルあるいは確率的言語モデル（probabilistic language model）は  \n",
    "人間が扱う自然言語で書かれた文や文書が生成される確率をモデル化したものであり、  \n",
    "自然言語らしさを推定する道具となるほか、その確率分布からサンプリングすることで文や文書を生成できる。  \n",
    "\n",
    "以降は言語モデルの基本処理単位を単語と呼称し、入出力は one-hotベクトルとする。\n",
    "\n",
    "長さ $T$ の単語列を $\\mathbf{Y}=(\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\ldots, \\mathbf{y}_{T})$ とする。  \n",
    "このとき単語列 $\\mathbf{Y}$ を生成する確率は $P(\\mathbf{Y})$である。  \n",
    "ある単語列が文を構成していることを明示する仮想単語として文頭を表す BOS、文末を表す EOSを導入し、  \n",
    "文を $\\mathbf{Y}^{\\prime} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ とする。  \n",
    "このとき一般に $P(\\mathbf{Y}) \\neq P(\\mathbf{Y}^{\\prime})$ である。\n",
    "\n",
    "言語モデルとして古典的にはNグラムモデルが最も多く使われた。  \n",
    "これと区別する意味で、ニューラルネットを用いた言語モデルをニューラル言語モデルという。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "BOS,EOSを導入した文を $\\mathbf{Y} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ と表し、文の生成確率 $P(\\mathbf{Y})$ をモデル化する。  \n",
    "直接モデル化は難しいため、文中の各単語の生成確率を、  \n",
    "その単語の前に出現した単語(列)が与えられた条件下で予測するモデルの組み合わせとして定義されるのが一般的である。 \n",
    "\n",
    "ここで文脈（context）を、  \n",
    "　言語モデルにおいてある単語の出現確率を計算する際に用いる周囲の単語  \n",
    "と定義する。\n",
    "\n",
    "単語の位置 $t$ より前に出現した $t-a$ 単語を $\\mathbf{Y}_{[a,t-1]} = (\\mathbf{y}_{a}, \\mathbf{y}_{a+1}, \\ldots, \\mathbf{y}_{t-1})$ と書き、これを文脈として扱う。  \n",
    "このとき文 $\\mathbf{Y}$ の出現確率 $o(\\mathbf{Y})$ を各単語が生成される条件付き確率の積で次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) = P(\\mathbf{y}_{0}) \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "例えば $\\mathbf{Y}=(\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\mathbf{y}_{2})$ について、 $P(X|Y)=P(X,Y)/P(Y)$ より\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) &= P(\\mathbf{y}_{0})P(\\mathbf{y}_{1}|\\mathbf{y}_{0})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P(\\mathbf{y}_{0},\\mathbf{y}_{1})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P (\\mathbf{y}_{0},\\mathbf{y}_{1},\\mathbf{y}_{2})\n",
    "\\end{align*}\n",
    "\n",
    "である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件付き確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{model}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "このモデルにおいて、BOSを使用するなら $P(\\mathbf{y}_{0})=1$ であるから $P_{model}(\\mathbf{y}_{0})$ は省略する。  \n",
    "また、次のように、$t-a$ 単語だけ（$\\mathbf{Y}_{[a,t-1]}$）を使って条件付き確率を表現できると仮定している。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]}) \\approx P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "たとえば $a=t-4$ とするとき、N=5のNグラム言語モデルと同様となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 順伝播型ニューラル言語モデル（FFNN言語モデル）\n",
    "前 $C$ 単語だけを入力とし、FFNNを用いて $t$ 番目の単語の出現確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{ffnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "FFNNへの入力は前 $C$ 単語分のone-hotベクトルを連結した $\\mathbf{Y}_{[t-C,t-1]}$ となる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Y}_{[t-C,t-1]} &= (\\mathbf{y}_{t-C},\\mathbf{y}_{t-C+1},\\ldots,\\mathbf{y}_{t-1}) \\\\\n",
    "&= (\\mathbf{y}_{t^{\\prime}})_{t^{\\prime}=t-C}^{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "例えば、３層FFNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\mathbf{e}_{k}=\\mathbf{Ey}_{k} \\, \\forall k \\in \\{t-C, \\ldots, t-1\\} \\\\\n",
    "\\text{(concatenate) : } \\mbox{ }& \\tilde{\\mathbf{y}}_{t}=\\operatorname{concat} ( \\mathbf{e}_{t-C}, \\ldots, \\mathbf{e}_{t-1}) \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh}(\\mathbf{W}^{(l)}\\tilde{\\mathbf{y}}_{t} + \\mathbf{b}^{(l)}) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 再帰ニューラル言語モデル（RNN言語モデル）\n",
    "入力ベクトル列 $\\mathbf{X}$ は  \n",
    "時刻 $t$ より前の単語列を one-hotベクトル表現したベクトル列 $\\mathbf{Y}_{[0,t-1]} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{t-1})$ となる。  \n",
    "これを $\\mathbf{Y}_{<t} = \\mathbf{Y}_{[0,t-1]}$ と略記する。\n",
    "\n",
    "RNN言語モデルによる文 $\\mathbf{Y}$ の生成確率 $P_{rnnlm}(\\mathbf{Y})$ は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{rnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t})\n",
    "\\end{align*}\n",
    "\n",
    "１層RNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\bar{\\mathbf{y}}_{t}=\\mathbf{Ey}_{t-1} \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh} \\left( \\mathbf{W}^{(l)} \\left[ \\begin{array}\\bar{\\mathbf{y}}_{t} \\\\ \\mathbf{h}_{t-1} \\end{array}  \\right] + \\mathbf{b}^{(l)} \\right) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### パープレキシティ（perplexity; PPL）\n",
    "評価用のデータセットを $\\mathcal{D} = \\{ \\mathbf{Y}^{(n)} \\} _{n=1}^{|\\mathcal{D}|}$、総単語数を $N$、  \n",
    "$n$ 番目の系列の長さを $T^{(n)}$ とする。  \n",
    "このときパープレキシティを次式で計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "&b^{z} &| \\, z=-\\frac{1}{N} \\sum_{n=1}^{|\\mathcal{D}|} \\sum_{t=1}^{T^{(n)}+1} \\log_{b} P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "対数の底 $b$ としては $2$ や $e$ が使われる。  \n",
    "$P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})$ の値域は $[0,1]$ であるから、その対数の値域は $[-\\infty, 0]$、よって $z$ は非負の値となる。\n",
    "\n",
    "パープレキシティは次の単語を予測する確率分布のばらつきを評価しているとみなすことができる。  \n",
    "（モデルに従って正解を選ぶための困難さを示す）  \n",
    "よい言語モデルの確率分布はデータと一致する次の単語にだけ高い確率を与え、ばらつきの小さい分布となる。\n",
    "\n",
    "また、パープレキシティの計算はモデル化の単位によらず計算可能であるので、  \n",
    "例えば文字単位の言語モデルを単語数に対する平均パープレキシティで評価することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルからの文生成\n",
    "記号 $\\sim$ を確率モデルからサンプリングする処理を示すものとするとき、  \n",
    "言語モデル $P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})$ からサンプリングされた $\\hat{\\mathbf{Y}}=(\\mathbf{y}_{0}=BOS, \\hat{\\mathbf{y}}_{1}, \\ldots, \\hat{\\mathbf{y}}_{\\hat{T}}, \\hat{\\mathbf{y}}_{\\hat{T+1}})$ を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}}_{t} \\sim P_{model}(\\mathbf{y}_{t}, \\hat{\\mathbf{Y}}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "これは文頭から順に次の単語を予測する処理ととらえることができ、  \n",
    "文の終わりを示すEOSが予測されたところでサンプリングを終了する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルの単位\n",
    "単位としては単語のほか、文字、バイト対符号化なども用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3　分散表現 (distributed representation)\n",
    "単語など自然言語処理で扱う記号は人間が恣意的に定義したものであり、  \n",
    "記号そのものの情報から記号間の類似度や関連性を直接計算することが難しい。\n",
    "\n",
    "**離散オブジェクト**を次のように定義する。  \n",
    "　人や物の名前、概念のように、物理的に計測できる量を伴わず、  \n",
    "　通常、記号を用いて離散的に表現するもの。\n",
    "\n",
    "**分散表現**はNLPの文脈では、一般的に次の意味で用いられる。  \n",
    "　任意の離散オブジェクトの集合 $\\mathcal{V}$ に対して、各離散オブジェクト $v \\in \\mathcal{V}$ に  \n",
    "　それぞれ $D$ 次元ベクトルを割り当て、離散オブジェクトを $D$ 次元ベクトルで表現したもの。\n",
    " \n",
    "分散表現への変換処理は、離散オブジェクトをベクトル空間内に埋め込む操作と捉えることもできる。   \n",
    "ベクトル空間では距離や演算が定義できることから、    \n",
    "これを離散オブジェクト間の類似度や関連性を計算する道具として用いることができる。\n",
    "\n",
    "単語に着目した分散表現を単語分散表現、あるいは単語埋め込みと呼ぶこともある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 歴史的背景\n",
    "・認知心理学や神経科学の分野では、脳のモデル化の方法論として分散表現が考えられた。  \n",
    "　分散表現の対義語として局所表現（one-hotベクトルなどが該当）が用いられる。\n",
    " \n",
    "・自然言語処理研究では、単語の持つ意味（semantics）を扱う方法の１つとして分布仮説が提案された。  \n",
    "　この仮説は、単語の意味は出現した際の周囲の単語によって決まるという考え方である。  \n",
    "　その手法としては主成分分析や潜在ディレクレ配分などがあり、  \n",
    "　分布仮説に基づいた方法論で得られた単語のベクトル表現を分散表現（distributional representation）と呼ぶ。\n",
    " \n",
    "・ニューラル言語モデルでは両分野の概念が同時に利用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 分散表現の獲得方法\n",
    "１．ニューラル言語モデルを学習することで、変換行列が分散表現に相当する値へ更新される\n",
    "\n",
    "２．対数線形性モデル（log-bilinear model; LBL model）を用いる  \n",
    "　word2vecおよび対数線形性モデルと呼ばれるほぼ同じモデルが同時期に提案された。  \n",
    "　（word2vecというツールにはskip-gramモデルおよびCBoWモデルが実装されている）\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### skip-gram および CBoWについて\n",
    "語彙を $\\mathcal{V}$、$i$ 番目の単語番号の入力単語 $\\mathbf{x}_{i}$ に割り当てる単語埋め込みベクトルを $\\mathbf{e}_{i}$、  \n",
    "$j$ 番目の単語番号の出力単語 $\\mathbf{y}_{j}$ に割り当てる単語埋め込みベクトルを $\\mathbf{o}_{j}$ で表す。  \n",
    "ここで $1 \\leq i \\leq |\\mathcal{V}|$、$1 \\leq j \\leq |\\mathcal{V}|$ である。\n",
    "\n",
    "$\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ を $1$ から $|\\mathcal{V}|$ まで並べて行列表示とみなすと、 $\\mathbf{E} = (\\mathbf{e}_{i})_{i=1}^{|\\mathcal{V}|}$、$\\mathbf{O} = (\\mathbf{o}_{j})_{j=1}^{|\\mathcal{V}|}$ と書ける。  \n",
    "ここで $\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ は $D$ 次元の列ベクトルとする。\n",
    "\n",
    "**< CBoW >**  \n",
    "入力単語のリストを $\\mathcal{H}$ とする。   \n",
    "$\\mathcal{H}$ が与えられた時に $j$ 番目の出力単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{H}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{H},\\mathbf{y}_{j}) &= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}}(\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}} \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "**< skip-gram >**  \n",
    "入力語彙 $\\mathcal{V}$ 中の $i$ 番目の単語 $\\mathbf{x}_{i}$ が与えられた時に  \n",
    "出力語彙 $\\mathcal{V}$ 中の $j$ 番目の単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{x}_{i},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{x}_{i}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{x}_{i},\\mathbf{y}_{j}) &= (\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式から、skip-gramは $\\mathcal{H} = \\{ \\mathbf{x}_{i} \\}$ であるとき（入力単語リスト長が常に１）のCBoWと見なせることが分かる。  \n",
    "また、対数双線形モデルにてCBoWやskip-gramを統一的に記述できることが分かっている。  \n",
    "\n",
    "対数双線形モデルの式はFFNNの出力層と同じ形のため、  \n",
    "これを１層NNによるニューラル言語モデルとみなして実装することができる。\n",
    "\n",
    "訓練データを $\\mathcal{D} = \\{ (\\mathcal{H}^{(n)}, \\mathbf{y}_{j}^{(n)})\\}_{n=1}^{N}$ とするとき、学習時の目的関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P(\\mathbf{y}_{j}|\\mathcal{H})) \\\\\n",
    "&= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\phi (\\mathcal{H},\\mathbf{y}_{j}) + \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log \\, \\left( \\sum_{\\mathbf{y}_{j^{\\prime}}\\in\\mathcal{V}} \\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j^{\\prime}})) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "この数式は語彙 $\\mathcal{V}$ 全てに基づいて計算されることから負荷が高く、  \n",
    "階層的ソフトマックス等を用いた対処が行われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 負例サンプリングによる獲得方法\n",
    "負例サンプリングによる学習は、与えられた入出力の組が  \n",
    "実データの確率分布から取得されたデータか、別の確率分布から生成されたデータかの  \n",
    "２クラスを判別する識別モデルの学習に相当する。\n",
    "\n",
    "$P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})$ を、学習用の１つのデータ $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "実際の訓練データ $\\mathcal{D}$ を生成する確率分布 $P^{\\mathcal{D}}$ から生成された場合を表す確率モデルとし、  \n",
    "以下のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}}) = \\frac{1}{1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j}))}\n",
    "\\end{align*}\n",
    "\n",
    "同様に、 $P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})$ を $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "ノイズデータ $\\mathcal{D}^{\\prime}$ を生成する確率分布 $P^{\\mathcal{D}^{\\prime}}$ から生成された場合を表す確率モデルとする。\n",
    "\n",
    "このとき、実データとノイズの識別問題の対数尤度関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})\n",
    "+ \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})\n",
    "\\end{align*}\n",
    "\n",
    "これら２式より\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "- \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "\\end{align*}\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.4　系列変換モデル"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
