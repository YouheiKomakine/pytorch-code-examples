{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3　言語処理における深層学習の基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1　準備：記号の世界とベクトルの世界の橋渡し\n",
    "離散的な記号である文字や単語などを連続する実数値のベクトルへ変換する方法の１つとして  \n",
    "one-hotベクトルが用いられる。\n",
    "\n",
    "語彙 $\\mathcal{V}$ があり、単語数は $|\\mathcal{V}|$ である。  \n",
    "ここに含まれる各単語に１から $|\\mathcal{V}|$ までの単語番号を割り振ったとする。  \n",
    "$i$ 番目の単語の番号が $n$ のとき、この単語を表すベクトル $\\mathbf{x}_{i}$ は  \n",
    "$n$ 番目の要素が１で残りが０であるような one-hotベクトルで表すこととする。\n",
    "\n",
    "単語などをニューラルネットで処理する場合は、例えば上記の考え方に基づいて  \n",
    "変換行列 $E$ に one-hotベクトル $\\mathbf{x}$ をかけて埋め込みベクトル $\\mathbf{Ex} = \\bar{\\mathbf{x}}$ を取得する。  \n",
    "この行列 $E$ は各列が一つの単語に対応し、これを埋め込み行列（emedding matrix）という。\n",
    "\n",
    "---\n",
    "逆にベクトルから記号へ変換する場合の典型的手法は、\n",
    "語彙数 $|\\mathcal{V}|$ に等しい次元数のベクトル $\\mathbf{o}$ を用意し、\n",
    "要素中で最大の要素番号に対応する単語番号の単語が選択されたとみなして変換することである。\n",
    "\n",
    "スケーリング係数 $a$ を導入したソフトマックス関数\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{softmax_{a}}(\\mathbf{o}) = \\frac{1}{\\exp (a \\mathbf{o}) \\cdot 1} \\exp(a \\mathbf{o})\n",
    "\\end{align*}\n",
    "\n",
    "を考える。\n",
    "式中の $\\mathbf{o}$ は強化学習でいうところの逆温度パラメータである。\n",
    "\n",
    "$\\mathbf{o}$ が $|\\mathcal{V}|$ 次元のベクトルであるとき、$\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ の $j$ 番目の要素 $x_{j}$ は\n",
    "\n",
    "\\begin{align*}\n",
    "s_{j} = \\frac{\\exp(ao_{j})}{\\sum_{j^{\\prime}=1}^{|\\mathcal{V}|} \\exp (ao_{j^{\\prime}})} \\, \\forall j\n",
    "\\end{align*}\n",
    "\n",
    "として計算される。  \n",
    "このときソフトマックス関数のパラメータ $a$ を十分大きい値に設定した場合、  \n",
    "得られるベクトル $\\mathbf{x} = \\operatorname{softmax_{a}}(\\mathbf{o})$ は  \n",
    "$\\mathbf{o}$ のすべての要素のうち最大値をとる要素番号の値が１であるようなone-hotベクトルを近似する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.2　言語モデル\n",
    "言語モデルあるいは確率的言語モデル（probabilistic language model）は  \n",
    "人間が扱う自然言語で書かれた文や文書が生成される確率をモデル化したものであり、  \n",
    "自然言語らしさを推定する道具となるほか、その確率分布からサンプリングすることで文や文書を生成できる。  \n",
    "\n",
    "以降は言語モデルの基本処理単位を単語と呼称し、入出力は one-hotベクトルとする。\n",
    "\n",
    "長さ $T$ の単語列を $\\mathbf{Y}=(\\mathbf{y}_{1}, \\mathbf{y}_{2}, \\ldots, \\mathbf{y}_{T})$ とする。  \n",
    "このとき単語列 $\\mathbf{Y}$ を生成する確率は $P(\\mathbf{Y})$である。  \n",
    "ある単語列が文を構成していることを明示する仮想単語として文頭を表す BOS、文末を表す EOSを導入し、  \n",
    "文を $\\mathbf{Y}^{\\prime} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ とする。  \n",
    "このとき一般に $P(\\mathbf{Y}) \\neq P(\\mathbf{Y}^{\\prime})$ である。\n",
    "\n",
    "言語モデルとして古典的にはNグラムモデルが最も多く使われた。  \n",
    "これと区別する意味で、ニューラルネットを用いた言語モデルをニューラル言語モデルという。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "BOS,EOSを導入した文を $\\mathbf{Y} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{T}, \\mathbf{y}_{T+1})$ と表し、文の生成確率 $P(\\mathbf{Y})$ をモデル化する。  \n",
    "直接モデル化は難しいため、文中の各単語の生成確率を、  \n",
    "その単語の前に出現した単語(列)が与えられた条件下で予測するモデルの組み合わせとして定義されるのが一般的である。 \n",
    "\n",
    "ここで文脈（context）を、  \n",
    "　言語モデルにおいてある単語の出現確率を計算する際に用いる周囲の単語  \n",
    "と定義する。\n",
    "\n",
    "単語の位置 $t$ より前に出現した $t-a$ 単語を $\\mathbf{Y}_{[a,t-1]} = (\\mathbf{y}_{a}, \\mathbf{y}_{a+1}, \\ldots, \\mathbf{y}_{t-1})$ と書き、これを文脈として扱う。  \n",
    "このとき文 $\\mathbf{Y}$ の出現確率 $o(\\mathbf{Y})$ を各単語が生成される条件付き確率の積で次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) = P(\\mathbf{y}_{0}) \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "例えば $\\mathbf{Y}=(\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\mathbf{y}_{2})$ について、 $P(X|Y)=P(X,Y)/P(Y)$ より\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{Y}) &= P(\\mathbf{y}_{0})P(\\mathbf{y}_{1}|\\mathbf{y}_{0})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P(\\mathbf{y}_{0},\\mathbf{y}_{1})P(\\mathbf{y}_{2}|\\mathbf{y}_{1},\\mathbf{y}_{0}) \\\\\n",
    "&= P (\\mathbf{y}_{0},\\mathbf{y}_{1},\\mathbf{y}_{2})\n",
    "\\end{align*}\n",
    "\n",
    "である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件付き確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{model}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "このモデルにおいて、BOSを使用するなら $P(\\mathbf{y}_{0})=1$ であるから $P_{model}(\\mathbf{y}_{0})$ は省略する。  \n",
    "また、次のように、$t-a$ 単語だけ（$\\mathbf{Y}_{[a,t-1]}$）を使って条件付き確率を表現できると仮定している。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{t}|\\mathbf{Y}_{[0,t-1]}) \\approx P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "たとえば $a=t-4$ とするとき、N=5のNグラム言語モデルと同様となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 順伝播型ニューラル言語モデル（FFNN言語モデル）\n",
    "前 $C$ 単語だけを入力とし、FFNNを用いて $t$ 番目の単語の出現確率をモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{ffnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1}P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "FFNNへの入力は前 $C$ 単語分のone-hotベクトルを連結した $\\mathbf{Y}_{[t-C,t-1]}$ となる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{Y}_{[t-C,t-1]} &= (\\mathbf{y}_{t-C},\\mathbf{y}_{t-C+1},\\ldots,\\mathbf{y}_{t-1}) \\\\\n",
    "&= (\\mathbf{y}_{t^{\\prime}})_{t^{\\prime}=t-C}^{t-1}\n",
    "\\end{align*}\n",
    "\n",
    "例えば、３層FFNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\mathbf{e}_{k}=\\mathbf{Ey}_{k} \\, \\forall k \\in \\{t-C, \\ldots, t-1\\} \\\\\n",
    "\\text{(concatenate) : } \\mbox{ }& \\tilde{\\mathbf{y}}_{t}=\\operatorname{concat} ( \\mathbf{e}_{t-C}, \\ldots, \\mathbf{e}_{t-1}) \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh}(\\mathbf{W}^{(l)}\\tilde{\\mathbf{y}}_{t} + \\mathbf{b}^{(l)}) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{[t-C,t-1]}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 再帰ニューラル言語モデル（RNN言語モデル）\n",
    "入力ベクトル列 $\\mathbf{X}$ は  \n",
    "時刻 $t$ より前の単語列を one-hotベクトル表現したベクトル列 $\\mathbf{Y}_{[0,t-1]} = (\\mathbf{y}_{0}, \\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{t-1})$ となる。  \n",
    "これを $\\mathbf{Y}_{<t} = \\mathbf{Y}_{[0,t-1]}$ と略記する。\n",
    "\n",
    "RNN言語モデルによる文 $\\mathbf{Y}$ の生成確率 $P_{rnnlm}(\\mathbf{Y})$ は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{rnnlm}(\\mathbf{Y}) = \\prod_{t=1}^{T+1} P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t})\n",
    "\\end{align*}\n",
    "\n",
    "１層RNNでの処理の流れは次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{(embedding vector) : } \\mbox{ }& \\bar{\\mathbf{y}}_{t}=\\mathbf{Ey}_{t-1} \\\\\n",
    "\\text{(hidden layer) : } \\mbox{ }& \\mathbf{h}_{t}=\\operatorname{tanh} \\left( \\mathbf{W}^{(l)} \\left[ \\begin{array}\\bar{\\mathbf{y}}_{t} \\\\ \\mathbf{h}_{t-1} \\end{array}  \\right] + \\mathbf{b}^{(l)} \\right) \\\\\n",
    "\\text{(output layer) : } \\mbox{ }& \\mathbf{o}_{t}=\\mathbf{W}^{(o)}\\mathbf{h}_{t} + \\mathbf{b}^{(o)} \\\\\n",
    "\\text{(probability) : } \\mbox{ }& \\mathbf{p}_{t} = \\operatorname{softmax}(\\mathbf{o}_{t}) \\\\\n",
    "\\text{(extract }P(\\mathbf{y}_{t})\\text{) : }\\mbox{ }& P(\\mathbf{y}_{t}|\\mathbf{Y}_{<t}) = \\mathbf{p}_{t} \\cdot \\mathbf{y}_{t}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### パープレキシティ（perplexity; PPL）\n",
    "評価用のデータセットを $\\mathcal{D} = \\{ \\mathbf{Y}^{(n)} \\} _{n=1}^{|\\mathcal{D}|}$、総単語数を $N$、  \n",
    "$n$ 番目の系列の長さを $T^{(n)}$ とする。  \n",
    "このときパープレキシティを次式で計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "&b^{z} &| \\, z=-\\frac{1}{N} \\sum_{n=1}^{|\\mathcal{D}|} \\sum_{t=1}^{T^{(n)}+1} \\log_{b} P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})\n",
    "\\end{align*}\n",
    "\n",
    "対数の底 $b$ としては $2$ や $e$ が使われる。  \n",
    "$P_{model} ( \\mathbf{y}_{t}^{(n)}, \\mathbf{Y}_{[a,t-1]}^{(n)})$ の値域は $[0,1]$ であるから、その対数の値域は $[-\\infty, 0]$、よって $z$ は非負の値となる。\n",
    "\n",
    "パープレキシティは次の単語を予測する確率分布のばらつきを評価しているとみなすことができる。  \n",
    "（モデルに従って正解を選ぶための困難さを示す）  \n",
    "よい言語モデルの確率分布はデータと一致する次の単語にだけ高い確率を与え、ばらつきの小さい分布となる。\n",
    "\n",
    "また、パープレキシティの計算はモデル化の単位によらず計算可能であるので、  \n",
    "例えば文字単位の言語モデルを単語数に対する平均パープレキシティで評価することができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルからの文生成\n",
    "記号 $\\sim$ を確率モデルからサンプリングする処理を示すものとするとき、  \n",
    "言語モデル $P_{model}(\\mathbf{y}_{t}|\\mathbf{Y}_{[a,t-1]})$ からサンプリングされた $\\hat{\\mathbf{Y}}=(\\mathbf{y}_{0}=BOS, \\hat{\\mathbf{y}}_{1}, \\ldots, \\hat{\\mathbf{y}}_{\\hat{T}}, \\hat{\\mathbf{y}}_{\\hat{T+1}})$ を次のように定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}}_{t} \\sim P_{model}(\\mathbf{y}_{t}, \\hat{\\mathbf{Y}}_{[a,t-1]})\n",
    "\\end{align*}\n",
    "\n",
    "これは文頭から順に次の単語を予測する処理ととらえることができ、  \n",
    "文の終わりを示すEOSが予測されたところでサンプリングを終了する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 言語モデルの単位\n",
    "単位としては単語のほか、文字、バイト対符号化なども用いられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.3　分散表現 (distributed representation)\n",
    "単語など自然言語処理で扱う記号は人間が恣意的に定義したものであり、  \n",
    "記号そのものの情報から記号間の類似度や関連性を直接計算することが難しい。\n",
    "\n",
    "**離散オブジェクト**を次のように定義する。  \n",
    "　人や物の名前、概念のように、物理的に計測できる量を伴わず、  \n",
    "　通常、記号を用いて離散的に表現するもの。\n",
    "\n",
    "**分散表現**はNLPの文脈では、一般的に次の意味で用いられる。  \n",
    "　任意の離散オブジェクトの集合 $\\mathcal{V}$ に対して、各離散オブジェクト $v \\in \\mathcal{V}$ に  \n",
    "　それぞれ $D$ 次元ベクトルを割り当て、離散オブジェクトを $D$ 次元ベクトルで表現したもの。\n",
    " \n",
    "分散表現への変換処理は、離散オブジェクトをベクトル空間内に埋め込む操作と捉えることもできる。   \n",
    "ベクトル空間では距離や演算が定義できることから、    \n",
    "これを離散オブジェクト間の類似度や関連性を計算する道具として用いることができる。\n",
    "\n",
    "単語に着目した分散表現を単語分散表現、あるいは単語埋め込みと呼ぶこともある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 歴史的背景\n",
    "・認知心理学や神経科学の分野では、脳のモデル化の方法論として分散表現が考えられた。  \n",
    "　分散表現の対義語として局所表現（one-hotベクトルなどが該当）が用いられる。\n",
    " \n",
    "・自然言語処理研究では、単語の持つ意味（semantics）を扱う方法の１つとして分布仮説が提案された。  \n",
    "　この仮説は、単語の意味は出現した際の周囲の単語によって決まるという考え方である。  \n",
    "　その手法としては主成分分析や潜在ディレクレ配分などがあり、  \n",
    "　分布仮説に基づいた方法論で得られた単語のベクトル表現を分散表現（distributional representation）と呼ぶ。\n",
    " \n",
    "・ニューラル言語モデルでは両分野の概念が同時に利用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 分散表現の獲得方法\n",
    "１．ニューラル言語モデルを学習することで、変換行列が分散表現に相当する値へ更新される\n",
    "\n",
    "２．対数線形性モデル（log-bilinear model; LBL model）を用いる  \n",
    "　word2vecおよび対数線形性モデルと呼ばれるほぼ同じモデルが同時期に提案された。  \n",
    "　（word2vecというツールにはskip-gramモデルおよびCBoWモデルが実装されている）\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### skip-gram および CBoWについて\n",
    "語彙を $\\mathcal{V}$、$i$ 番目の単語番号の入力単語 $\\mathbf{x}_{i}$ に割り当てる単語埋め込みベクトルを $\\mathbf{e}_{i}$、  \n",
    "$j$ 番目の単語番号の出力単語 $\\mathbf{y}_{j}$ に割り当てる単語埋め込みベクトルを $\\mathbf{o}_{j}$ で表す。  \n",
    "ここで $1 \\leq i \\leq |\\mathcal{V}|$、$1 \\leq j \\leq |\\mathcal{V}|$ である。\n",
    "\n",
    "$\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ を $1$ から $|\\mathcal{V}|$ まで並べて行列表示とみなすと、 $\\mathbf{E} = (\\mathbf{e}_{i})_{i=1}^{|\\mathcal{V}|}$、$\\mathbf{O} = (\\mathbf{o}_{j})_{j=1}^{|\\mathcal{V}|}$ と書ける。  \n",
    "ここで $\\mathbf{e}_{i}$ と $\\mathbf{o}_{j}$ は $D$ 次元の列ベクトルとする。\n",
    "\n",
    "**< CBoW >**  \n",
    "入力単語のリストを $\\mathcal{H}$ とする。   \n",
    "$\\mathcal{H}$ が与えられた時に $j$ 番目の出力単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{H}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{H},\\mathbf{y}_{j}) &= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}}(\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\sum_{i:\\mathbf{x}_{i}\\in\\mathcal{H}} \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "**< skip-gram >**  \n",
    "入力語彙 $\\mathcal{V}$ 中の $i$ 番目の単語 $\\mathbf{x}_{i}$ が与えられた時に  \n",
    "出力語彙 $\\mathcal{V}$ 中の $j$ 番目の単語 $\\mathbf{y}_{j}$ が出力される確率を以下の式で定義する。\n",
    "\n",
    "\\begin{align*}\n",
    "P(\\mathbf{y}_{i}|\\mathcal{H}) &= \\frac{\\exp(\\phi (\\mathcal{x}_{i},\\mathbf{y}_{j}))}{\\sum_{\\mathbf{y}_{j^{\\prime}} \\in \\mathcal{V}} \\exp (\\phi (\\mathcal{x}_{i}, \\mathbf{y}_{j^{\\prime}}))} \\\\\n",
    "\\phi(\\mathcal{x}_{i},\\mathbf{y}_{j}) &= (\\mathbf{Ex}_{i}) \\cdot ( \\mathbf{Oy}_{j}) \\\\\n",
    "&= \\mathbf{e}_{i} \\cdot \\mathbf{o}_{j}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式から、skip-gramは $\\mathcal{H} = \\{ \\mathbf{x}_{i} \\}$ であるとき（入力単語リスト長が常に１）のCBoWと見なせることが分かる。  \n",
    "また、対数双線形モデルにてCBoWやskip-gramを統一的に記述できることが分かっている。  \n",
    "\n",
    "対数双線形モデルの式はFFNNの出力層と同じ形のため、  \n",
    "これを１層NNによるニューラル言語モデルとみなして実装することができる。\n",
    "\n",
    "訓練データを $\\mathcal{D} = \\{ (\\mathcal{H}^{(n)}, \\mathbf{y}_{j}^{(n)})\\}_{n=1}^{N}$ とするとき、学習時の目的関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P(\\mathbf{y}_{j}|\\mathcal{H})) \\\\\n",
    "&= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\phi (\\mathcal{H},\\mathbf{y}_{j}) + \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log \\, \\left( \\sum_{\\mathbf{y}_{j^{\\prime}}\\in\\mathcal{V}} \\exp(\\phi (\\mathcal{H},\\mathbf{y}_{j^{\\prime}})) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "この数式は語彙 $\\mathcal{V}$ 全てに基づいて計算されることから負荷が高く、  \n",
    "階層的ソフトマックス等を用いた対処が行われる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 負例サンプリングによる獲得方法\n",
    "負例サンプリングによる学習は、与えられた入出力の組が  \n",
    "実データの確率分布から取得されたデータか、別の確率分布から生成されたデータかの  \n",
    "２クラスを判別する識別モデルの学習に相当する。\n",
    "\n",
    "$P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})$ を、学習用の１つのデータ $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "実際の訓練データ $\\mathcal{D}$ を生成する確率分布 $P^{\\mathcal{D}}$ から生成された場合を表す確率モデルとし、  \n",
    "以下のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}}) = \\frac{1}{1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j}))}\n",
    "\\end{align*}\n",
    "\n",
    "同様に、 $P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})$ を $(\\mathcal{H},\\mathbf{y}_{j})$ が  \n",
    "ノイズデータ $\\mathcal{D}^{\\prime}$ を生成する確率分布 $P^{\\mathcal{D}^{\\prime}}$ から生成された場合を表す確率モデルとする。\n",
    "\n",
    "このとき、実データとノイズの識別問題の対数尤度関数は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= - \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}})\n",
    "+ \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (P((\\mathcal{H},\\mathbf{y}_{j}) \\sim P^{\\mathcal{D}^{\\prime}})\n",
    "\\end{align*}\n",
    "\n",
    "これら２式より\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\mathbf{E}, \\mathbf{O}|\\mathcal{D}) &= \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "- \\sum_{(\\mathcal{H},\\mathbf{y}_{j})\\in\\mathcal{D}^{\\prime}} \\log (1+\\exp(-\\phi (\\mathcal{H},\\mathbf{y}_{j})))\n",
    "\\end{align*}\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.4　系列変換モデル (seq2seq model)\n",
    "自然言語処理において文から分への変換と見なせるタスクとして、機械翻訳、対話、質問応答などがある。  \n",
    "これらのタスクは系列（sequence）から系列への変換とみなすことができる。\n",
    "\n",
    "ここで、系列を別の系列へ変換する確率をモデル化したものを  \n",
    "**系列変換モデル**（sequene-to-sequence model）と呼ぶ。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力系列を $\\mathbf{X}$、出力系列を $\\mathbf{Y}$ で表す。  \n",
    "入力系列中の $i$ 番目の要素を $\\mathbf{x}_{i}$、出力系列中の $j$ 番目の要素を $\\mathbf{y}_{j}$ とし、  \n",
    "それぞれ one-hot ベクトルであり、対応する離散オブジェクトを変換したものとする。\n",
    "\n",
    "入力側の語彙を $\\mathcal{V}^{(s)}$、出力側の語彙を $\\mathcal{V}^{(t)}$ とすると、全ての $i,j$ に対して $\\mathbf{x}_{i} \\in \\mathbb{R}^{|\\mathcal{V}^{(s)}|}, \\mathbf{y}_{j} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}|}$ である。  \n",
    "また、入力分長を $I$、出力文長を $J$ とする。\n",
    "\n",
    "このとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{X} = (\\mathbf{x}_{1}, \\ldots, \\mathbf{x}_{i}, \\ldots, \\mathbf{x}_{I}) = (\\mathbf{x}_{i})_{i=1}^{I} \\\\\n",
    "\\mathbf{Y} = (\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{j}, \\ldots, \\mathbf{y}_{J}) = (\\mathbf{y}_{j})_{j=1}^{J}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{y}_{0} = \\mathbf{y}^{(BOS)}$ はBOSに対応する one-hot ベクトル、  \n",
    "$\\mathbf{y}_{J+1} = \\mathbf{y}^{(EOS)}$ はEOSに対応する one-hot ベクトルであり、  \n",
    "出力側の語彙 $\\mathcal{V}^{(t)}$ にはBOS, EOSが含まれていることを仮定する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ある入力系列 $\\mathbf{X}$ が与えられた時に、ある出力系列 $\\mathbf{Y}$ へ変換する条件付き確率 $P(\\mathbf{Y}|\\mathbf{X})$ を考える。  \n",
    "これをモデル化したものが系列変換モデルで、ここでは次のようにモデル化する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}|\\mathbf{X}) = \\prod_{j=1}^{J+1} P_{\\theta}(\\mathbf{y}_{j} | \\mathbf{Y}_{<j}, \\mathbf{X})\n",
    "\\end{align*}\n",
    "\n",
    "入力 $\\mathbf{X}$ を受け取って（固定長の）符号ベクトル $\\mathbf{z}$ を生成する処理を次のように表す。  \n",
    "関数 $\\Lambda$ にはRNNを使用することが一般的である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{z} = \\Lambda (\\mathbf{X})\n",
    "\\end{align*}\n",
    "\n",
    "符号ベクトル $\\mathbf{z}$ を受け取って出力 $\\mathbf{Y}$ を生成する処理は再帰ニューラル言語モデルと同じで、  \n",
    "隠れ状態ベクトルを生成する関数を $\\Psi$、  \n",
    "単語の生成確率を返す関数を $\\Upsilon$ として次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{y}_{j}|\\mathbf{Y}_{<j}, \\mathbf{X}) &= \\Psi(\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\Upsilon ( \\mathbf{h}_{j-1}^{(t)}, \\mathbf{y}_{j-1})\n",
    "\\end{align*}\n",
    "\n",
    "ただし $j=1$ のとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j-1}^{(t)} &= \\mathbf{h}_{0}^{(t)} = \\mathbf{z} \\\\\n",
    "\\mathbf{y}_{j-1} &= \\mathbf{y}_{0} = \\mathbf{y}^{(BOS)}\n",
    "\\end{align*}\n",
    "\n",
    "である。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$j=1$ の場合（初期値の設定）を除き、系列変換モデルと再帰ニューラル言語モデルの処理は等しい。  \n",
    "ゆえに系列変換モデルを条件付き（再帰ニューラル）言語モデルとして認識することができる。  \n",
    "この視点では、系列変換モデルは次のような条件付き確率のモデル化である。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}|\\mathbf{X}) &= \\prod_{j=1}^{J+1} P_{\\theta} (\\mathbf{y}_{j}|\\mathbf{Y}_{<j},\\mathbf{X}) \\\\\n",
    "P_{\\theta} (\\mathbf{y}_{j} | \\mathbf{Y}_{<j},\\mathbf{X}) &= \\Upsilon (\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\left\\{ \n",
    "\\begin{array}\n",
    " \\,\\Psi (\\mathbf{z},\\mathbf{y}^{(BOS)}) & \\text{if} \\, j=1 \\\\\n",
    " \\,\\Psi (\\mathbf{h}_{j-1}^{(t)},\\mathbf{y}_{j-1}) & \\text{otherwise}\n",
    "\\end{array} \\right.\\\\\n",
    "\\mathbf{z} &= \\Lambda (\\mathbf{X})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再帰ニューラル言語モデルを同形式で表すと次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{Y}) &= \\prod_{j=1}^{J+1} P_{\\theta^{\\prime}} (\\mathbf{y}_{j}|\\mathbf{Y}_{<j}) \\\\\n",
    "P_{\\theta^{\\prime}} (\\mathbf{y}_{j} | \\mathbf{Y}_{<j}) &= \\Upsilon (\\mathbf{h}_{j}^{(t)}, \\mathbf{y}_{j}) \\\\\n",
    "\\mathbf{h}_{j}^{(t)} &= \\left\\{ \n",
    "\\begin{array}\n",
    " \\,\\Psi (0,\\mathbf{y}^{(BOS)}) & \\text{if} \\, j=1 \\\\\n",
    " \\,\\Psi (\\mathbf{h}_{j-1}^{(t)},\\mathbf{y}_{j-1}) & \\text{otherwise}\n",
    "\\end{array} \\right.\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 系列変換モデルの構造\n",
    "\n",
    "１．符号化器埋め込み層（encoder embedding layer）  \n",
    "２．符号化器再帰層（encoder recurrent layer）  \n",
    "３．復号化器埋め込み層（decoder embedding layer）  \n",
    "４．復号化器再帰層（decoder recurrent layer）  \n",
    "５．復号化器出力層（decoder output layer）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (1)符号化器埋め込み層\n",
    "入力：入力文中の $i$ 番目の単語を意味する one-hotベクトル $\\mathbf{x}_{i}$  \n",
    "出力：入力文中の $i$ 番目の単語に対応する埋め込みベクトル $\\bar{\\mathbf{x}}_{i}$  \n",
    "　について、$i=1 \\sim I$ まで順に（あるいは一括で）処理する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{x}}_{i} = \\mathbf{E}^{(s)}\\mathbf{x}_{i} \\hspace{20px} \\forall i\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{E}^{(s)} \\in \\mathbb{R}^{\\mathcal{D} \\times |\\mathcal{V}^{(s)}| }$ は入力語彙に対する埋め込み行列に相当する。  \n",
    "埋め込みベクトル獲得処理は $i$ に依存しないため一括して処理が可能である。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{X}} = \\mathbf{E}^{(s)}\\mathbf{X}\n",
    "\\end{align*}\n",
    "\n",
    "このとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{X}} = (\\bar{\\mathbf{x}}_{1}, \\ldots, \\bar{\\mathbf{x}}_{I}) = (\\bar{\\mathbf{x}}_{i})_{i=1}^{I}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (2)符号化器再帰層\n",
    "入力：入力文中の $i$ 番目の単語に対応する埋め込みベクトル $\\bar{\\mathbf{x}}_{i}$  \n",
    "出力：隠れ状態ベクトル $\\mathbf{h}_{i}^{(s)}$\n",
    "\n",
    "説明のため、この処理を１層単方向RNN、活性化関数tanhでモデル化したとする。  \n",
    "$\\Psi^{(s)}(\\cdot)$ を再帰ニューラルネットの処理を表す関数とし、出力される位置 $i$ の隠れ状態ベクトルを $\\mathbf{h}_{i}^{(s)}$ で表す。  \n",
    "このとき符号化器再帰層での処理は次の式に相当する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{i}^{(s)} &= \\Psi^{(s)} ( \\bar{\\mathbf{x}}_{i}, \\mathbf{h}_{i-1}^{(s)} ) \\\\\n",
    "\\Psi^{(s)} ( \\bar{\\mathbf{x}}_{i}, \\mathbf{h}_{i-1}^{(s)} ) &= \\operatorname{tanh} \\left(\n",
    "\\mathbf{W}^{(s)} \\left[ \\begin{array} \\mathbf{h}_{i-1}^{(s)} \\\\ \\bar{\\mathbf{x}}_{i} \\end{array}\n",
    "\\right] + \\mathbf{b}^{(s)} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "ただし $\\mathbf{W}^{(s)} \\in \\mathbb{R}^{H \\times (H+D)}, \\mathbf{b}^{(s)} \\in \\mathbb{R}^{H}$ であり、  \n",
    "$i=0$ のとき $\\mathbf{h}_{i}^{(s)} = \\mathbf{h}_{0}^{(s)} = \\mathbf{0}$ となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (3)復号化器埋め込み層\n",
    "復号化器では位置 $j$ の処理に位置 $j-1$ の処理結果を利用することから、通常は $j$ について一括処理できない。  \n",
    "ただし学習時には $\\mathbf{Y}$ が訓練データとして事前に与えられるため一括処理として実装できる。\n",
    "\n",
    "入力：復号化器出力層で選択された出力 $\\mathbf{y}_{j-1}$  \n",
    "出力：埋め込みベクトル $\\bar{\\mathbf{y}}_{j}$  \n",
    "　について、位置 $j$ で以下の計算を行う。\n",
    "\n",
    "\\begin{align*}\n",
    "\\bar{\\mathbf{y}}_{j} = \\mathbf{E}^{(t)} \\mathbf{y}_{j-1}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{E}^{(t)} \\in \\mathbb{R}^{\\mathcal{D} \\times |\\mathcal{V}^{(t)}| }$ は復号化器埋め込み層の埋め込み行列である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (4)復号化器再帰層\n",
    "入力：復号化器埋め込み層の出力に対応する埋め込みベクトル $\\bar{\\mathbf{y}}_{j}$  \n",
    "出力：隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$  \n",
    "\n",
    "関数 $\\Psi^{(t)}(\\cdot)$ を復号化器再帰層で行う処理を表す関数とするとき、  \n",
    "各位置 $j$ での処理は次のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{j}^{(t)} &= \\Psi^{(t)} (\\bar{\\mathbf{y}}_{j}, \\mathbf{h}_{j-1}^{(t)} ) \\\\\n",
    "\\Psi^{(t)}(\\bar{\\mathbf{y}}_{j},\\mathbf{h}_{j-1}^{(t)}) &= \\operatorname{tanh} \\left( \\mathbf{W}^{(t)} \\left[ \\begin{array}\\mathbf{h}_{j-1}^{(t)} \\\\ \\bar{\\mathbf{y}}_{j} \\end{array} \\right] + \\mathbf{b}^{(t)}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "ただし $\\mathbf{W}^{(t)} \\in \\mathbb{R}^{H \\times (H+D)}, \\mathbf{b}^{(t)} \\in \\mathbb{R}^{H}$ である。\n",
    "\n",
    "また、符号化器再帰層の最後の位置の隠れ状態ベクトル $\\mathbf{h}_{I}^{(s)}$ を  \n",
    "復号化器再帰層の初期状態 $j=0$ における $\\mathbf{h}_{0}^{(t)}$ として用いる。  \n",
    "すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{h}_{0}^{(t)} = \\mathbf{h}_{I}^{(s)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### (5)復号化器出力層\n",
    "入力：位置 $j$ での復号化器再帰層の隠れ状態ベクトル $\\mathbf{h}_{j}^{(t)}$  \n",
    "出力：$\\mathbf{y}_{j}$ が生成される確率 $p_{j}$\n",
    "\n",
    "出力層の計算は学習時と出力系列の予測時で処理が若干異なる。\n",
    "\n",
    "共通処理として、スコアベクトル $\\mathbf{o}_{j}$ を次のとおり計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{o}_{j} = \\mathbf{W}^{(o)}\\mathbf{h}_{j}^{(t)} + \\mathbf{b}^{(o)}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $\\mathbf{W}^{(o)} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}| \\times H}$ と $\\mathbf{b}^{(o)} \\in \\mathbb{R}^{|\\mathcal{V}^{(t)}|}$ は、出力層内の変換行列とバイアス項のベクトルである。\n",
    "\n",
    "【学習時】  \n",
    "次に、学習時の場合は、訓練データとモデルの適合の度合いをみるため確率計算処理を行う。  \n",
    "$j$ 番目の単語 $\\mathbf{y}_{j}$ の生成確率を次の式で計算する。\n",
    "\n",
    "\\begin{align*}\n",
    "P_{\\theta}(\\mathbf{y}_{j} | \\mathbf{Y}_{<j}) = \\operatorname{softmax}(\\mathbf{o}_{j}) \\cdot \\mathbf{y}_{j}\n",
    "\\end{align*}\n",
    "\n",
    "ソフトマックス関数によりスコアのベクトル表現が各出力語彙の確率に変換されるので、  \n",
    "one-hot ベクトル $\\mathbf{y}_{j}$ との内積を取ることで $j$ 番目の単語の生成確率を得ることができる。\n",
    "\n",
    "【予測時】  \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\mathbf{y}}_{j} = \\operatorname{softmax}_{a}(\\mathbf{o}_{j})\n",
    "\\end{align*}\n",
    "\n",
    "により、パラメータ $a$ を十分大きい値としたとき、  \n",
    "ベクトル $\\mathbf{o}_{j}$ の中で最大の要素が１でそれ以外が０であるような one-hot ベクトルの近似が得られるので、  \n",
    "これを単語を選択する処理として用いることができる。\n",
    "\n",
    "$\\mathbf{y}_{j}$ は one-hot ベクトルとなるので、  \n",
    "これを次の処理位置の入力として復号化器埋め込み層の処理に戻る。  \n",
    "このような３種からなるループ処理を、終了信号を受け取るまで繰り返し行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 系列変換モデルのパラメータ数\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
