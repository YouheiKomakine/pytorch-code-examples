{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [詳解ディープラーニング　TensorFlow・Kerasによる時系列データ処理](https://book.mynavi.jp/ec/products/detail/id=72995)\n",
    "　巣籠悠輔 著  \n",
    "　マイナビ出版  \n",
    "　ISBN : 978-4-8399-6251-7  \n",
    "　発売 : 2017/05/30  \n",
    "  \n",
    "support site : [https://book.mynavi.jp/supportsite/detail/9784839962517.html](https://book.mynavi.jp/supportsite/detail/9784839962517.html)  \n",
    "github : [yusugomori/deeplearning-tensorflow-keras](https://github.com/yusugomori/deeplearning-tensorflow-keras) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 第４章　続き"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dropout\n",
    "Overfittingなどの問題のため、深層学習では訓練データに対して予測精度を向上させたとしても  \n",
    "評価データや実運用時の予測精度が上がるとは限らない。  \n",
    "  \n",
    "訓練データに含まれない未知のデータに対して予測精度を上昇させるように学習することを汎化（generalization）、  \n",
    "また未知データに対しての予測性能を汎化性能（generalization ability）という。  \n",
    "  \n",
    "汎化のための手法はいくつも存在し、その一つがドロップアウト（dropout）である。    \n",
    "これは学習時にランダムにニューロンをdropoutさせて出力を０にする手法で、  \n",
    "擬似的なアンサンブル学習（個別に学習させた複数の識別器の出力から予測を作る）とみなすことができる。  \n",
    "  \n",
    "ドロップアウトの実装では対象の層と同じshapeを持つarrayであるマスクベクトル $m$ を用意し、  \n",
    "学習時には出力との要素積をとり、出力時にはoff（or 全て１）とする。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "式で表すと、あるネットワークの入力 $x$ に対する出力 $h_{i}$ が  \n",
    "  \n",
    "\\begin{align*}\n",
    "h_{i} = f(Wx + b)\n",
    "\\end{align*}\n",
    "  \n",
    "と表されるとき、これを  \n",
    "\n",
    "\\begin{align*}\n",
    "h_{i} = f(Wx + b) \\odot m\n",
    "\\end{align*}\n",
    "\n",
    "へと変更する。  \n",
    "  \n",
    "このとき続く層の出力 $ h_{i+1}$ を\n",
    "\n",
    "\\begin{align*}\n",
    "h_{i+1} = g(Vh_{i} + c)\n",
    "\\end{align*}\n",
    "\n",
    "また、それぞれの層で活性化関数に代入される重み付き和を\n",
    "\n",
    "\\begin{align*}\n",
    "p := Wx + b \\\\\n",
    "q := Vh_{i} + c\n",
    "\\end{align*}\n",
    "\n",
    "とするとき、誤差項 $ \\delta_{hi}, \\delta_{hi+1} $ は、\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{hi} &:= \\frac{\\partial E_{n}}{\\partial p} \\\\\n",
    "\\delta_{hi+1} &:= \\frac{\\partial E_{n}}{\\partial q}\n",
    "\\end{align*}\n",
    "\n",
    "より\n",
    "\n",
    "\\begin{align*}\n",
    "\\delta_{hi} &= \\frac{\\partial E_{n}}{\\partial q}\\frac{\\partial q}{\\partial p} \\\\\n",
    "&= \\frac{\\partial E_{n}}{\\partial q}\\frac{\\partial}{\\partial p}(Vf(p) \\odot m + c) \\\\\n",
    "&= f^{\\prime}(p) \\odot m \\odot V^{\\mathrm{T}}\\delta_{hi+1}\n",
    "\\end{align*}\n",
    "\n",
    "となる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Tensorflowでの実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class toy_test_tf_mnist(object):\n",
    "    def __init__(self, data_size=10000, n_hidden_layer = 3, n_hidden_cell=200, \n",
    "                 train_size=0.8, test_size=0.2, learning_rate=0.01, np_seed=None):\n",
    "        self.data_size = data_size\n",
    "        self.n_hidden_layer = max(n_hidden_layer, 1)\n",
    "        self.n_hidden_cell = max(n_hidden_cell, 1)\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "        self.lr = learning_rate\n",
    "        self.np_seed = np_seed\n",
    "        \n",
    "    def set_each_dataset(self):\n",
    "        mnist = fetch_mldata('MNIST original', data_home=\"../dataset\")\n",
    "        indices = np.random.permutation(len(mnist.data))[:self.data_size]\n",
    "        self.X = mnist.data[indices]\n",
    "        self.Y = mnist.target[indices]\n",
    "        self.Y_onehot = np.eye(10)[self.Y.astype(int)]\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = \\\n",
    "            train_test_split(self.X, self.Y_onehot, train_size=self.train_size, test_size=self.test_size)\n",
    "        self.n_in, self.n_out = self.X_train[0].shape[0], self.Y_train[0].shape[0]\n",
    "\n",
    "    def generate_model(self):\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, self.n_in])\n",
    "        self.t = tf.placeholder(tf.float32, shape=[None, self.n_out])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        self.h = {}\n",
    "        self.h_drop = {}\n",
    "        \n",
    "        # input layer - hidden layer 1\n",
    "        self.W[0] = tf.Variable(tf.truncated_normal([self.n_in, self.n_hidden_cell], stddev=0.01))\n",
    "        self.b[0] = tf.Variable(tf.zeros([self.n_hidden_cell]))\n",
    "        self.h[0] = tf.nn.relu(tf.matmul(self.x, self.W[0]) + self.b[0])\n",
    "        self.h_drop[0] = tf.nn.dropout(self.h[0], self.keep_prob)\n",
    "\n",
    "        for i in range(self.n_hidden_layer - 1):\n",
    "            self.W[i+1] = tf.Variable(tf.truncated_normal([self.n_hidden_cell, self.n_hidden_cell], stddev=0.01))\n",
    "            self.b[i+1] = tf.Variable(tf.zeros([self.n_hidden_cell]))\n",
    "            self.h[i+1] = tf.nn.relu(tf.matmul(self.h[i], self.W[i+1]) + self.b[i+1])\n",
    "            self.h_drop[i+1] = tf.nn.dropout(self.h[i+1], self.keep_prob)\n",
    "\n",
    "        # hidden layer - output layer\n",
    "        self.W[self.n_hidden_layer] = tf.Variable(tf.truncated_normal([self.n_hidden_cell, self.n_out], stddev=0.01))\n",
    "        self.b[self.n_hidden_layer] = tf.Variable(tf.zeros([self.n_out]))\n",
    "        self.y = tf.nn.softmax(tf.matmul(self.h_drop[self.n_hidden_layer-1], self.W[self.n_hidden_layer]) + self.b[self.n_hidden_layer])\n",
    "\n",
    "        self.cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.t * tf.log(self.y), reduction_indices=[1]))\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(self.cross_entropy)\n",
    "        \n",
    "        self.correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.t, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction , tf.float32))\n",
    "        \n",
    "    def fit(self, epochs=30, batch_size=200):\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "        \n",
    "        n_batches = int(self.data_size * self.train_size) // batch_size\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_, Y_ = shuffle(self.X_train, self.Y_train)\n",
    "            \n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "            \n",
    "                sess.run(self.train_step, feed_dict={\n",
    "                    self.x: X_[start:end],\n",
    "                    self.t: Y_[start:end],\n",
    "                    self.keep_prob:0.5\n",
    "                })\n",
    "                \n",
    "            loss = self.cross_entropy.eval(session=sess, feed_dict={\n",
    "                self.x: X_,\n",
    "                self.t: Y_,\n",
    "                self.keep_prob: 1.0\n",
    "            })\n",
    "            acc = self.accuracy.eval(session=sess, feed_dict={\n",
    "                self.x: X_,\n",
    "                self.t: Y_,\n",
    "                self.keep_prob: 1.0\n",
    "            })\n",
    "            print('epoch:', epoch, ' loss:', loss, ' accuracy:', acc)\n",
    "                \n",
    "        accuracy_rate = self.accuracy.eval(session=sess, feed_dict={\n",
    "            self.x: self.X_test,\n",
    "            self.t: self.Y_test,\n",
    "            self.keep_prob: 1.0\n",
    "        })\n",
    "        print('accuracy: ', accuracy_rate)\n",
    "\n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 2.25104  accuracy: 0.294\n",
      "epoch: 1  loss: 1.60406  accuracy: 0.498375\n",
      "epoch: 2  loss: 0.820454  accuracy: 0.739125\n",
      "epoch: 3  loss: 0.655835  accuracy: 0.796\n",
      "epoch: 4  loss: 0.496206  accuracy: 0.8485\n",
      "epoch: 5  loss: 0.388966  accuracy: 0.8815\n",
      "epoch: 6  loss: 0.352227  accuracy: 0.890375\n",
      "epoch: 7  loss: 0.409333  accuracy: 0.864625\n",
      "epoch: 8  loss: 0.281477  accuracy: 0.912375\n",
      "epoch: 9  loss: 0.261775  accuracy: 0.920875\n",
      "epoch: 10  loss: 0.223829  accuracy: 0.935375\n",
      "epoch: 11  loss: 0.204165  accuracy: 0.937875\n",
      "epoch: 12  loss: 0.189549  accuracy: 0.94575\n",
      "epoch: 13  loss: 0.170097  accuracy: 0.951\n",
      "epoch: 14  loss: 0.159841  accuracy: 0.950875\n",
      "epoch: 15  loss: 0.143205  accuracy: 0.95775\n",
      "epoch: 16  loss: 0.119476  accuracy: 0.967375\n",
      "epoch: 17  loss: 0.127007  accuracy: 0.962625\n",
      "epoch: 18  loss: 0.112417  accuracy: 0.9685\n",
      "epoch: 19  loss: 0.0926286  accuracy: 0.974625\n",
      "epoch: 20  loss: 0.0822767  accuracy: 0.977625\n",
      "epoch: 21  loss: 0.0787187  accuracy: 0.978375\n",
      "epoch: 22  loss: 0.0673099  accuracy: 0.983125\n",
      "epoch: 23  loss: 0.0625971  accuracy: 0.984375\n",
      "epoch: 24  loss: 0.0562846  accuracy: 0.9865\n",
      "epoch: 25  loss: 0.0505311  accuracy: 0.987625\n",
      "epoch: 26  loss: 0.0435244  accuracy: 0.990625\n",
      "epoch: 27  loss: 0.0463872  accuracy: 0.9885\n",
      "epoch: 28  loss: 0.0347138  accuracy: 0.992625\n",
      "epoch: 29  loss: 0.0323109  accuracy: 0.99325\n",
      "accuracy:  0.945\n"
     ]
    }
   ],
   "source": [
    "testenv = toy_test_tf_mnist(data_size=10000, n_hidden_layer=3, n_hidden_cell=200, \n",
    "                 train_size=0.8, test_size=0.2, learning_rate=0.01, np_seed=None)\n",
    "testenv.set_each_dataset()\n",
    "testenv.generate_model()\n",
    "testenv.fit(epochs=30, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  loss: 1.41529  accuracy: 0.579625\n",
      "epoch: 1  loss: 0.625784  accuracy: 0.791313\n",
      "epoch: 2  loss: 0.379587  accuracy: 0.88925\n",
      "epoch: 3  loss: 0.301959  accuracy: 0.910312\n",
      "epoch: 4  loss: 0.252873  accuracy: 0.926375\n",
      "epoch: 5  loss: 0.221352  accuracy: 0.936312\n",
      "epoch: 6  loss: 0.179019  accuracy: 0.949188\n",
      "epoch: 7  loss: 0.15513  accuracy: 0.955187\n",
      "epoch: 8  loss: 0.145781  accuracy: 0.95875\n",
      "epoch: 9  loss: 0.123266  accuracy: 0.965187\n",
      "epoch: 10  loss: 0.121881  accuracy: 0.963812\n",
      "epoch: 11  loss: 0.100241  accuracy: 0.970812\n",
      "epoch: 12  loss: 0.0888309  accuracy: 0.975375\n",
      "epoch: 13  loss: 0.0760668  accuracy: 0.9785\n",
      "epoch: 14  loss: 0.0722346  accuracy: 0.979813\n",
      "epoch: 15  loss: 0.0618083  accuracy: 0.983125\n",
      "epoch: 16  loss: 0.0496171  accuracy: 0.987188\n",
      "epoch: 17  loss: 0.0462618  accuracy: 0.988187\n",
      "epoch: 18  loss: 0.0381531  accuracy: 0.991437\n",
      "epoch: 19  loss: 0.0403054  accuracy: 0.990625\n",
      "epoch: 20  loss: 0.0305167  accuracy: 0.9935\n",
      "epoch: 21  loss: 0.0416628  accuracy: 0.987375\n",
      "epoch: 22  loss: 0.0267625  accuracy: 0.993625\n",
      "epoch: 23  loss: 0.0226289  accuracy: 0.994062\n",
      "epoch: 24  loss: 0.0200675  accuracy: 0.99525\n",
      "epoch: 25  loss: 0.0152171  accuracy: 0.996687\n",
      "epoch: 26  loss: 0.0131211  accuracy: 0.9975\n",
      "epoch: 27  loss: 0.0127535  accuracy: 0.997563\n",
      "epoch: 28  loss: 0.0125546  accuracy: 0.99775\n",
      "epoch: 29  loss: 0.0112096  accuracy: 0.997813\n",
      "accuracy:  0.9625\n"
     ]
    }
   ],
   "source": [
    "# use double size of data\n",
    "testenv_2 = toy_test_tf_mnist(data_size=20000, n_hidden_layer=3, n_hidden_cell=200, \n",
    "                 train_size=0.8, test_size=0.2, learning_rate=0.01, np_seed=None)\n",
    "testenv_2.set_each_dataset()\n",
    "testenv_2.generate_model()\n",
    "testenv_2.fit(epochs=30, batch_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Kerasでの実装例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.utils.generic_utils import CustomObjectScope\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import _to_tensor\n",
    "from keras.backend.common import epsilon\n",
    "\n",
    "def custom_categorical_crossentropy(target, output, from_logits=False, delta=1e-7):\n",
    "    if not from_logits:\n",
    "        output /= tf.reduce_sum(output,\n",
    "                                axis=len(output.get_shape()) - 1,\n",
    "                                keep_dims=True)\n",
    "        _epsilon = _to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "        output = tf.clip_by_value(output, _epsilon, 1. - _epsilon)\n",
    "        return - tf.reduce_sum(target * tf.log(output + delta),\n",
    "                               axis=len(output.get_shape()) - 1)\n",
    "    else:\n",
    "        return tf.nn.softmax_cross_entropy_with_logits(labels=target,\n",
    "                                                       logits=output)\n",
    "\n",
    "\n",
    "class toy_test_keras_mnist(object):\n",
    "    def __init__(self, data_size=10000, n_hidden_layer = 3, n_hidden_cell=200, \n",
    "                 train_size=0.8, test_size=0.2, learning_rate=0.01, keep_prob=0.5, np_seed=None):\n",
    "        self.data_size = data_size\n",
    "        self.n_hidden_layer = max(n_hidden_layer, 1)\n",
    "        self.n_hidden_cell = max(n_hidden_cell, 1)\n",
    "        self.train_size = train_size\n",
    "        self.test_size = test_size\n",
    "        self.lr = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.np_seed = np_seed\n",
    "        \n",
    "    def set_each_dataset(self):\n",
    "        mnist = fetch_mldata('MNIST original', data_home=\"../dataset\")\n",
    "        indices = np.random.permutation(len(mnist.data))[:self.data_size]\n",
    "        self.X = mnist.data[indices]\n",
    "        self.Y = mnist.target[indices]\n",
    "        self.Y_onehot = np.eye(10)[self.Y.astype(int)]\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = \\\n",
    "            train_test_split(self.X, self.Y_onehot, train_size=self.train_size, test_size=self.test_size)\n",
    "        self.n_in, self.n_out = self.X_train[0].shape[0], self.Y_train[0].shape[0]\n",
    "        print(\"n_in={}, n_out={}\".format(self.n_in, self.n_out))\n",
    "\n",
    "    def fit(self, epochs=150, batch_size=200, activation_function='tanh', loss_function='categorical_crossentropy'):\n",
    "        with CustomObjectScope({'custom_categorical_crossentropy':custom_categorical_crossentropy}):\n",
    "            self.model = Sequential()\n",
    "\n",
    "            self.model.add(Dense(self.n_hidden_cell, input_dim=self.n_in))\n",
    "            self.model.add(Activation(activation_function))\n",
    "            self.model.add(Dropout(self.keep_prob))\n",
    "\n",
    "            for i in range(self.n_hidden_layer - 1):\n",
    "                self.model.add(Dense(self.n_hidden_cell))\n",
    "                self.model.add(Activation(activation_function))\n",
    "                self.model.add(Dropout(self.keep_prob))\n",
    "\n",
    "            self.model.add(Dense(self.n_out))\n",
    "            self.model.add(Activation('softmax'))\n",
    "\n",
    "            self.model.compile(loss = loss_function,\n",
    "                               optimizer = SGD(lr=self.lr),\n",
    "                               metrics=['accuracy']\n",
    "                               )\n",
    "\n",
    "            self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size)\n",
    "            loss_and_metrics = self.model.evaluate(self.X_test, self.Y_test)\n",
    "            print(loss_and_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_in=784, n_out=10\n",
      "Epoch 1/150\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 2.7202 - acc: 0.1275\n",
      "Epoch 2/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 2.3500 - acc: 0.2032\n",
      "Epoch 3/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 2.1161 - acc: 0.2814\n",
      "Epoch 4/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 1.9114 - acc: 0.3368\n",
      "Epoch 5/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 1.7534 - acc: 0.3948\n",
      "Epoch 6/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 1.5834 - acc: 0.4584\n",
      "Epoch 7/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 1.4627 - acc: 0.4883\n",
      "Epoch 8/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 1.3699 - acc: 0.5194\n",
      "Epoch 9/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 1.2786 - acc: 0.5511\n",
      "Epoch 10/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 1.2251 - acc: 0.5741\n",
      "Epoch 11/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 1.1823 - acc: 0.5906\n",
      "Epoch 12/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 1.1423 - acc: 0.5999\n",
      "Epoch 13/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 1.0787 - acc: 0.6230\n",
      "Epoch 14/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 1.0581 - acc: 0.6335\n",
      "Epoch 15/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 1.0136 - acc: 0.6476\n",
      "Epoch 16/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.9995 - acc: 0.6520\n",
      "Epoch 17/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.9389 - acc: 0.6804\n",
      "Epoch 18/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.9172 - acc: 0.6874\n",
      "Epoch 19/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.9330 - acc: 0.6857\n",
      "Epoch 20/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.9094 - acc: 0.6946\n",
      "Epoch 21/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.8720 - acc: 0.7090\n",
      "Epoch 22/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.8407 - acc: 0.7159\n",
      "Epoch 23/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.8384 - acc: 0.7185\n",
      "Epoch 24/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.8044 - acc: 0.7353\n",
      "Epoch 25/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.8071 - acc: 0.7330\n",
      "Epoch 26/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.8410 - acc: 0.7157\n",
      "Epoch 27/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.8087 - acc: 0.7334\n",
      "Epoch 28/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.7678 - acc: 0.7519\n",
      "Epoch 29/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.7458 - acc: 0.7520\n",
      "Epoch 30/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.7411 - acc: 0.7582\n",
      "Epoch 31/150\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.7429 - acc: 0.7546\n",
      "Epoch 32/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.7256 - acc: 0.7627\n",
      "Epoch 33/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.7344 - acc: 0.7611\n",
      "Epoch 34/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.7030 - acc: 0.7691\n",
      "Epoch 35/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.6979 - acc: 0.7711\n",
      "Epoch 36/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.7105 - acc: 0.7691\n",
      "Epoch 37/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6971 - acc: 0.7756\n",
      "Epoch 38/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6764 - acc: 0.7830\n",
      "Epoch 39/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.6820 - acc: 0.7782\n",
      "Epoch 40/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.6438 - acc: 0.7915\n",
      "Epoch 41/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6662 - acc: 0.7834\n",
      "Epoch 42/150\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6525 - acc: 0.7917\n",
      "Epoch 43/150\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6570 - acc: 0.7922\n",
      "Epoch 44/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6499 - acc: 0.7904\n",
      "Epoch 45/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6328 - acc: 0.7975\n",
      "Epoch 46/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6446 - acc: 0.7899\n",
      "Epoch 47/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.6382 - acc: 0.7921\n",
      "Epoch 48/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6496 - acc: 0.7915\n",
      "Epoch 49/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.6278 - acc: 0.8005\n",
      "Epoch 50/150\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.6585 - acc: 0.7895\n",
      "Epoch 51/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6197 - acc: 0.7976\n",
      "Epoch 52/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6309 - acc: 0.8016\n",
      "Epoch 53/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.6131 - acc: 0.8067\n",
      "Epoch 54/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5909 - acc: 0.8070\n",
      "Epoch 55/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6073 - acc: 0.8036\n",
      "Epoch 56/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6197 - acc: 0.8051\n",
      "Epoch 57/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.6158 - acc: 0.8044\n",
      "Epoch 58/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5942 - acc: 0.8115\n",
      "Epoch 59/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.6101 - acc: 0.8065\n",
      "Epoch 60/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.6020 - acc: 0.8102\n",
      "Epoch 61/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5993 - acc: 0.8093\n",
      "Epoch 62/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5801 - acc: 0.8163\n",
      "Epoch 63/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5691 - acc: 0.8254\n",
      "Epoch 64/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5695 - acc: 0.8241\n",
      "Epoch 65/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5450 - acc: 0.8265\n",
      "Epoch 66/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5514 - acc: 0.8250\n",
      "Epoch 67/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5577 - acc: 0.8280\n",
      "Epoch 68/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5459 - acc: 0.8304\n",
      "Epoch 69/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5572 - acc: 0.8316\n",
      "Epoch 70/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5338 - acc: 0.8344\n",
      "Epoch 71/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5369 - acc: 0.8347\n",
      "Epoch 72/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5421 - acc: 0.8322\n",
      "Epoch 73/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5299 - acc: 0.8328\n",
      "Epoch 74/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5354 - acc: 0.8355\n",
      "Epoch 75/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5317 - acc: 0.8364\n",
      "Epoch 76/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5464 - acc: 0.8276\n",
      "Epoch 77/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5195 - acc: 0.8334\n",
      "Epoch 78/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5458 - acc: 0.8309\n",
      "Epoch 79/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5301 - acc: 0.8414\n",
      "Epoch 80/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5194 - acc: 0.8430\n",
      "Epoch 81/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5132 - acc: 0.8424\n",
      "Epoch 82/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5258 - acc: 0.8395\n",
      "Epoch 83/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5207 - acc: 0.8391\n",
      "Epoch 84/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.5233 - acc: 0.8389\n",
      "Epoch 85/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5335 - acc: 0.8371\n",
      "Epoch 86/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5139 - acc: 0.8444\n",
      "Epoch 87/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5352 - acc: 0.8356\n",
      "Epoch 88/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.5140 - acc: 0.8385\n",
      "Epoch 89/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.5200 - acc: 0.8405\n",
      "Epoch 90/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.5082 - acc: 0.8435\n",
      "Epoch 91/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5135 - acc: 0.8484\n",
      "Epoch 92/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4996 - acc: 0.8484\n",
      "Epoch 93/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5107 - acc: 0.8415\n",
      "Epoch 94/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.5190 - acc: 0.8365\n",
      "Epoch 95/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4895 - acc: 0.8500\n",
      "Epoch 96/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4883 - acc: 0.8500\n",
      "Epoch 97/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4998 - acc: 0.8465\n",
      "Epoch 98/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.5037 - acc: 0.8421\n",
      "Epoch 99/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.5056 - acc: 0.8491\n",
      "Epoch 100/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4818 - acc: 0.8554\n",
      "Epoch 101/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4900 - acc: 0.8559\n",
      "Epoch 102/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.4865 - acc: 0.8566\n",
      "Epoch 103/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4917 - acc: 0.8505\n",
      "Epoch 104/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4749 - acc: 0.8560\n",
      "Epoch 105/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4762 - acc: 0.8551\n",
      "Epoch 106/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4817 - acc: 0.8577\n",
      "Epoch 107/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4740 - acc: 0.8551\n",
      "Epoch 108/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.4635 - acc: 0.8603\n",
      "Epoch 109/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4617 - acc: 0.8560\n",
      "Epoch 110/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4440 - acc: 0.8665\n",
      "Epoch 111/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4609 - acc: 0.8573\n",
      "Epoch 112/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4601 - acc: 0.8587\n",
      "Epoch 113/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4795 - acc: 0.8550\n",
      "Epoch 114/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4622 - acc: 0.8614\n",
      "Epoch 115/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4610 - acc: 0.8638\n",
      "Epoch 116/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4558 - acc: 0.8645\n",
      "Epoch 117/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4596 - acc: 0.8625\n",
      "Epoch 118/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.4534 - acc: 0.8615\n",
      "Epoch 119/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4508 - acc: 0.8614\n",
      "Epoch 120/150\n",
      "8000/8000 [==============================] - 0s 29us/step - loss: 0.4385 - acc: 0.8679\n",
      "Epoch 121/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4411 - acc: 0.8680\n",
      "Epoch 122/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4476 - acc: 0.8686\n",
      "Epoch 123/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4505 - acc: 0.8626\n",
      "Epoch 124/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4549 - acc: 0.8645\n",
      "Epoch 125/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4600 - acc: 0.8601\n",
      "Epoch 126/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4517 - acc: 0.8624\n",
      "Epoch 127/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4471 - acc: 0.8594\n",
      "Epoch 128/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4500 - acc: 0.8675\n",
      "Epoch 129/150\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.4393 - acc: 0.8664\n",
      "Epoch 130/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4527 - acc: 0.8648\n",
      "Epoch 131/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4507 - acc: 0.8633\n",
      "Epoch 132/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4440 - acc: 0.8624\n",
      "Epoch 133/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4400 - acc: 0.8651\n",
      "Epoch 134/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4528 - acc: 0.8626\n",
      "Epoch 135/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4339 - acc: 0.8713\n",
      "Epoch 136/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4474 - acc: 0.8603\n",
      "Epoch 137/150\n",
      "8000/8000 [==============================] - 0s 30us/step - loss: 0.4431 - acc: 0.8675\n",
      "Epoch 138/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4525 - acc: 0.8680\n",
      "Epoch 139/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4420 - acc: 0.8655\n",
      "Epoch 140/150\n",
      "8000/8000 [==============================] - 0s 28us/step - loss: 0.4318 - acc: 0.8646\n",
      "Epoch 141/150\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.4384 - acc: 0.8621\n",
      "Epoch 142/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4289 - acc: 0.8690\n",
      "Epoch 143/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4244 - acc: 0.8722\n",
      "Epoch 144/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4301 - acc: 0.8713\n",
      "Epoch 145/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4127 - acc: 0.8767\n",
      "Epoch 146/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4109 - acc: 0.8753\n",
      "Epoch 147/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4142 - acc: 0.8732\n",
      "Epoch 148/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4181 - acc: 0.8712\n",
      "Epoch 149/150\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.4330 - acc: 0.8676\n",
      "Epoch 150/150\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.4307 - acc: 0.8706\n",
      "2000/2000 [==============================] - 0s 35us/step\n",
      "[0.33263760429620742, 0.90000000000000002]\n"
     ]
    }
   ],
   "source": [
    "testenv_keras = toy_test_keras_mnist(data_size=10000, n_hidden_layer = 3, n_hidden_cell=200, \n",
    "                                     train_size=0.8, test_size=0.2, \n",
    "                                     learning_rate=0.01, keep_prob=0.5, np_seed=None)\n",
    "testenv_keras.set_each_dataset()\n",
    "testenv_keras.fit(epochs=150, batch_size=200,\n",
    "                  activation_function='tanh', loss_function='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験時メモ：  \n",
    "・Kerasで活性化関数をreluにすると高率に学習失敗する問題が発生。  \n",
    "　自分のコードおよび書籍記載コードで発生を確認した。  \n",
    "  \n",
    "・本単元までで紹介された要素を対象として調査したが、この範囲ではバグの原因は不明で修正できず。  \n",
    "　追試用コードではreluを使った学習はそれなりに成功するが、今回分のコードでは各10回試験して一度も学習できず。  \n",
    "  \n",
    "追記１：  \n",
    "　Weightsの初期化部分で非ランダムな値を使うことで、問題が再現できることを確認した。  \n",
    "　ランダムな初期値を明示的に与えると正しく学習できる。  \n",
    "   \n",
    "追記２：  \n",
    "　docker上で gcr.io/tensorflow/tensorflow:latest-gpu のイメージを利用し学習したところ  \n",
    "　正常に学習できる。（上のセルはこの環境における学習結果の一例）  \n",
    "　versionが異なることもあるが、一部処理が異なるようだ。  \n",
    "  　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Kerasにおける重みの初期化方法\n",
    "　参考：[Keras Documentation | レイヤーの重み初期化方法](https://keras.io/ja/initializations/)  \n",
    "  \n",
    "・初期化用引数により、重みをランダムに初期化できる。  \n",
    "  \n",
    "~~~python\n",
    "model.add(Dense(64, init='uniform'))\n",
    "~~~\n",
    "\n",
    "初期化方法一覧\n",
    "- uniform\n",
    "- lecun_uniform (LeCun 98)\n",
    "- normal\n",
    "- identity\n",
    "- orthogonal\n",
    "- zero\n",
    "- glorot_normal (Glorot 2010)\n",
    "- glorot_uniform\n",
    "- he_normal (He et al., 2014)\n",
    "- he_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "次のnotebookへ続く。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
