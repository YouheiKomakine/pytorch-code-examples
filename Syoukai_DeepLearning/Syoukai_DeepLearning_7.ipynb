{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [詳解ディープラーニング　TensorFlow・Kerasによる時系列データ処理](https://book.mynavi.jp/ec/products/detail/id=72995)\n",
    "　巣籠悠輔 著  \n",
    "  \n",
    "support site : [https://book.mynavi.jp/supportsite/detail/9784839962517.html](https://book.mynavi.jp/supportsite/detail/9784839962517.html)  \n",
    "github : [yusugomori/deeplearning-tensorflow-keras](https://github.com/yusugomori/deeplearning-tensorflow-keras) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### (４章　続き)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Weight Initialization\n",
    "勾配降下法と誤差逆伝播法によって学習を行うとき、  \n",
    "重みの初期値が等しい各ノードは全く等しい値の誤差を受け取り、等しく更新されるため  \n",
    "結果として１つのニューロンだけがあるのと同じ振る舞いをしてしまう。  \n",
    "よって重みの初期値は同じ層のどの２つのノードも異なっていなければならず  \n",
    "これには初期値の設定をランダム化する手法が使われる。  \n",
    "  \n",
    "また、学習中に誤差が全層に正しく伝わり、また値が大きくなり過ぎない、  \n",
    "つまり不安定勾配問題を軽減するような初期値の分布であることが望ましい。  \n",
    "  \n",
    "CS231の講義での[Setting up the data and the model | Weight Initialization](http://cs231n.github.io/neural-networks-2/#init)の項では、経験則が次のようにまとめられている。\n",
    "  \n",
    " - Small random numbers for symmetry breaking\n",
    " - Calibrating the variances with 1/sqrt(n) to ensures that all neurons initially have approximately the same output distribution\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後者で言及されている出力の分布について検討する。  \n",
    "前提として、入力データは正規化され、重みの分布の平均値が零であること、すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "E(x_{i}) = E(w_{ij}) = 0\n",
    "\\end{align*}\n",
    "\n",
    "を仮定する。  \n",
    "  \n",
    "ある層への入力が $n$ 次元のベクトル $\\mathbf{x}$ 、重みが $\\mathbf{W}$ のとき、  \n",
    "活性化関数へ入力される重み付き和 $\\mathbf{p}$ の成分 $ p_{j} = \\sum_{i=1}^{n}w_{ij}x_{i}$ の分散を考えると\n",
    "\n",
    "\\begin{align*}\n",
    "Var(p_{j}) &= Var\\left( \\sum_{i=1}^{n}w_{ij}x_{i} \\right) \\\\\n",
    "&= \\sum_{i=1}^{n} Var(w_{ij}x_{n}) \\\\\n",
    "&= \\sum_{i=1}^{n} \\left\\{ \\left( E(w_{ij}) \\right)^{2} Var(x_{i}) + \\left( E(x_{i}) \\right)^{2} Var(w_{ij}) + Var(w_{ij})Var(x_{i}) \\right\\} \\\\ \n",
    "&= \\sum_{i=1}^{n} Var(x_{ij}) Var(w_{i}) & \\because E(x_{i}) = E(w_{ij}) = 0\\\\\n",
    "&= (n Var(w_{ij}))Var(x_{i})\n",
    "\\end{align*}\n",
    "\n",
    "となる。  \n",
    "  \n",
    "さて、定数 $a$ に対して $Var(ax)=a^{2} Var(x)$ であるから、層の前後で出力分布が変わらないためには  \n",
    "重みの初期値に係数 $1/ \\sqrt{n}$ が乗算されていればよい。  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "代表的な重みの初期値として、以下のようなものがよく知られている。  \n",
    "ここで fan_in, fan_out は各層（Weight tensor）における入力／出力ユニット数である。\n",
    "\n",
    "#### LuCunの初期値\n",
    "[LeCun et al., (1998)](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)で提唱された初期値で、\n",
    "正規分布または一様分布による初期化を行う。\n",
    "\n",
    "Kerasでは\n",
    "\n",
    "~~~python\n",
    "# truncated normal distribution | mean=0, stddev = sqrt(1 / fan_in)\n",
    "initializer = \"lecun_normal\"\n",
    "\n",
    "# uniform distribution | within [-sqrt(3 / fan_in), sqet(3 / fan_in)]\n",
    "initializer = \"lecun_uniform\"\n",
    "~~~\n",
    "\n",
    "により使用できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Xavierの初期値\n",
    "[Glorot and Bengio (2010)](http://proceedings.mlr.press/v9/glorot10a.html) で提唱された初期値で  \n",
    "勾配の大きさを全体として一定に保つことを目的としており、  \n",
    "前後の層のユニット数から $ Var(w) = \\frac{2}{n_{in} + n_{out}}$ の標準偏差をもつ分布を使用するというものである。  \n",
    "  \n",
    "使用の際には、一様分布ならば `var = sqrt(6. / (n_in + n_out)); [-x, x]`が、  \n",
    "正規分布ならば `var = sqrt(3. / (n_in + n_out))` が用いられる。  \n",
    "  \n",
    "[TensorFlow](https://www.tensorflow.org/versions/r0.12/api_docs/python/contrib.layers/initializers)では  \n",
    "\n",
    "~~~python\n",
    "tf.contrib.layers.xavier_initializer(uniform=True, seed=None, dtype=tf.float32)\n",
    "~~~\n",
    "  \n",
    "[Keras](https://keras.io/ja/initializers/)では initializerの指定部分で  \n",
    "\n",
    "~~~python\n",
    "# normal distribution\n",
    "initializer = 'glorot_normal'\n",
    "\n",
    "# uniform distribution\n",
    "initializer = 'glorot_uniform'\n",
    "~~~\n",
    "\n",
    "によって使用できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heの初期値\n",
    "[He et al.(2015)](https://arxiv.org/abs/1502.01852)で提唱された初期値で、活性化関数としてReLUを使う場合の初期値分布である。  \n",
    "  \n",
    "Kerasでは  \n",
    "\n",
    "~~~python\n",
    "# trancated normal distribution | mean=0, stddev = sqrt(2/fan_in)\n",
    "initializer = \"he_normal\"\n",
    "\n",
    "# uniform distribution | within [-sqrt(6 / fan_in), sqrt(6 / fan_in)]\n",
    "initializer = \"hi_uniform\"\n",
    "~~~\n",
    "\n",
    "によって使用できる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimizer\n",
    "　よく使われる最適化手法は勾配降下法とその改良手法である。  \n",
    "　そのうちいくつかを選んで記載する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （先に結論）  \n",
    "　どうも定説がなく数カ月ごとに主流の学説が入れ替わるような状況であるが、  \n",
    "　特段の理由がない限りSGDまたはMomentumで問題ないと思われる。    \n",
    "   \n",
    "#### （関連資料）  \n",
    "[On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836)  \n",
    "　minibatch(~512)がlarge batchよりも上手く行く理由の検証。  \n",
    "　小さなbatchは周辺がflatな最適解に向かう傾向が、large batchはsharpな最適解に向かう傾向がある。  \n",
    "　（より小さいbatchの使用は更新量の分散を大きくするために、局所解に留まることを難しくするようだ）  \n",
    "  \n",
    "[Train longer, generalize better: closing the generalization gap in large batch training of neural networks](https://arxiv.org/abs/1705.08741)  \n",
    "　flatな最適解を得るためには更新回数が重要で、large batchでも回数を増やすことや、  \n",
    "　学習率を高くする、Batch Normalizationの適用で汎化性能を高めることができる。  \n",
    "  \n",
    "[Understanding deep learning requires rethinking generalization](https://openreview.net/forum?id=Sy8gdB9xx&noteId=Sy8gdB9xx)   \n",
    "　DNNは全てのラベルを覚えきる能力を持ちながら正則化では説明のつかない汎化を示し、  \n",
    "　SGDがその要因のひとつである。  \n",
    "  　  \n",
    "[The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/abs/1705.08292v1)  \n",
    "　RMSprop, Adamなどの局所的な曲率情報に基づく手法は単純なSGDやmomentum SGDに比べ過学習しやすく、  \n",
    "　収束速度は改善しない。  \n",
    "  \n",
    "[Fixing Weight Decay Regularization in Adam](https://arxiv.org/abs/1711.05101)  \n",
    "　AdamがSGDに比べ汎化性能が低いのはWeight Decay regularizationが意図した値になっていないのが原因であり、  \n",
    "　学習率への依存を無くしてやると性能改善する  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### この節を記述する際の参考資料：    \n",
    "　[arXivTimes](https://github.com/arXivTimes/arXivTimes)   \n",
    "　[Optimizer : 深層学習における勾配法について](https://qiita.com/tokkuman/items/1944c00415d129ca0ee9)  \n",
    "　[OPTIMIZER 入門 ~線形回帰からAdamからEveまで](https://qiita.com/deaikei/items/29d4550fa5066184329a)  \n",
    "　[An overview of gradient descent optimization algorithms](http://ruder.io/optimizing-gradient-descent/)  \n",
    "\n",
    "・ノルム  \n",
    "　$n$次元ベクトル $\\mathbf{x} = (x_{1}, x_{2}, \\dots, x_{n})$ および $ 1 \\leq p < \\infty $ なる $p$ に対して  \n",
    "\n",
    "\\begin{align*}\n",
    "\\sqrt[p]{ |x_{1}|^{p} + |x_{2}|^{p} + \\cdots + |x_{n}|^{p}}\n",
    "\\end{align*}\n",
    "\n",
    "　を $\\mathbf{x}$ の $L^{p}$ ノルムといい、 $||\\mathbf{x}||_{p}$ と書く。 \n",
    "  \n",
    "　いわゆる「距離」は$L^{2}$ ノルムであり、ユークリッドノルムという。  \n",
    "　機械学習では $L^{1}$ ノルム、$L^{2}$ ノルム、$L^{\\infty}$ ノルムがよく用いられる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Optimization Methods\n",
    "\n",
    "・勾配降下法（再掲）  \n",
    "　現実的な最適化問題では解析的に解けることは少なく、その場合に用いられる手法の一つが勾配(降下)法である。  \n",
    "　その表現の１つは、何らかの意味で正解からの離れ具合を示す関数 $f$ および学習率 $\\eta$ を定めて、  \n",
    " \n",
    "\\begin{align*}\n",
    "x_{i+1} = x_{i} - \\eta \\frac{\\partial f}{\\partial x_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "　によって値 $x$ を更新していくものである。  \n",
    "  \n",
    "　本手法は機械学習では Batch gradient descent として実装されており、  \n",
    "　訓練データ全体に対する計算が必要であることから遅く、実用上より良いOptimizerが開発されてきた。    \n",
    "　SGDおよび他のよく使われる手法の多くは勾配降下法の改良版である。 \n",
    "  \n",
    "　以下、記号については別記なき限り、  \n",
    "　重みなどモデルのパラメータを $\\theta$ 、誤差関数を $C$ 、誤差関数の $\\theta$ に対する勾配を $\\nabla_{\\theta}$ 、学習率を $\\eta$ 、  \n",
    "　訓練データと正解（ラベル）データをそれぞれ $X = \\{ x_{i} \\}, Y = \\{ y_{i} \\}$ とする。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD (Stochastic Gradient Descent) : オンライン勾配降下法およびミニバッチ勾配降下法\n",
    "　データセットよりミニバッチ $(x^{(i)}, y^{(i)})$ をランダムに抽出し、\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta = \\theta - \\eta \\nabla_{\\theta} C(\\theta; x^{(i)}; y^{(i)})\n",
    "\\end{align*}\n",
    "\n",
    "　によってパラメータを更新する。  \n",
    "  \n",
    "### Momentum SGD\n",
    "　SGDによる学習時にパラメータが最適解の周辺で振動して学習が進まない問題を解決するため  \n",
    "　慣性の考え方を導入した Optimizer である。  \n",
    "　モメンタム項 $v$ およびその係数 $\\gamma \\, (<1) $ を導入して、  \n",
    " \n",
    "\\begin{align*}\n",
    "v_{t} &= \\gamma v_{t-1} + \\eta \\nabla_{\\theta} C(\\theta) \\\\\n",
    "\\theta &= \\theta - v_{t}\n",
    "\\end{align*}\n",
    "\n",
    "　によりパラメータを更新する。    \n",
    "　モメンタム項により、複数ステップに渡って勾配が同じ方向に向くとき  \n",
    "　学習はこの方向、つまり局所解のある方向に向けて加速する。    \n",
    "  \n",
    "### Nesterov accelerated gradient (Nesterov Momentum)\n",
    "　Momentumに先を予測する能力を持たせたもので、次のパラメータの位置を $\\theta -\\gamma v_{t-1}$ によって近似し、\n",
    " \n",
    "\\begin{align*}\n",
    "v_{t} &= \\gamma v_{t-1} + \\eta \\nabla_{\\theta} C(\\theta -\\gamma v_{t-1}) \\\\\n",
    "\\theta &= \\theta - v_{t}\n",
    "\\end{align*}\n",
    "\n",
    "　によって更新する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "　適応的な学習率を持つOptimizerで、稀なパラメータに対してより大きな更新をさせるために  \n",
    "　スパースなデータを扱うのに適する。  \n",
    "\n",
    "　Adagradでは、各パラメータ $\\theta_{t, i}$ について、対角成分 $(i,i)$ がタイムステップ $t$ までの $\\theta_{i}$ に対する勾配の二乗和であるような  \n",
    "　対角行列 $G_{t} \\in \\mathbb{R}^{d \\times d}$ および平滑化項 $\\epsilon$ （通常 1e-8 程度）を用いて  \n",
    "\n",
    "\\begin{align*}\n",
    "G_{t, ii} = \\sum_{step=0}^{t} \\left( \\Delta_{\\theta}C(\\theta_{step,i}) \\right)^{2} \\\\\n",
    "\\theta_{t+1, i} = \\theta_{t, i} - \\frac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon }} \\cdot \\nabla_{\\theta} C(\\theta_{i})\n",
    "\\end{align*}\n",
    "\n",
    "　の更新規則によって最適化を行う。これをベクトル化したものは次のようになる。\n",
    " \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t} + \\epsilon }} \\odot \\nabla_{\\theta} C(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "　Adagradは手動で学習率を調整する必要がなく扱いやすい。  \n",
    "　一方で分母の二乗勾配が蓄積するという弱点があり、学習率が単調減少して小さくなりすぎる傾向にある。  \n",
    "  \n",
    "### AdaDelta\n",
    "　Adagradの発展形で、学習率の急速な単調減少を防ぐために改良された。  \n",
    "　勾配の蓄積を減衰するため、Adagradの更新規則にタイムステップ $t$ における移動平均 $E[g^{2}]_{t}$ を導入して  \n",
    " \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{E[g^{2}]_{t} + \\epsilon }} \\odot \\nabla_{\\theta} C(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "　と変更する。  \n",
    "　この分母は勾配についての RMS (二乗平均平方根) 誤差基準であるから、これを省略して\n",
    " \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{RMS[g]_{t}} \\odot \\nabla_{\\theta} C(\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "　と書ける。  \n",
    "   \n",
    "　ただし、この更新式において単位が一致しないことから、元の論文ではパラメータと同じ単位を  \n",
    "　仮定的に持たせるためにパラメータ更新の二乗についての減衰平均を先に定義している。  \n",
    " \n",
    "\\begin{align*}\n",
    "E[\\Delta \\theta^{2}]_{t} &= \\gamma E[\\Delta \\theta^{2}]_{t-1} + (1 - \\gamma) \\Delta \\theta^{2}_{t} \\\\\n",
    "RMS[\\Delta\\theta]_{t} &= \\sqrt{E[\\Delta\\theta^{2}]_{t} + \\epsilon}\n",
    "\\end{align*}\n",
    "\n",
    "　未知である $RMS[\\Delta\\theta]_{t}$ を前のタイムステップまでのパラメータ更新のRMSで近似して、  \n",
    "　これによって学習率 $\\eta$ を置換して次の更新規則を得る。  \n",
    " \n",
    "\\begin{align*}\n",
    "\\Delta \\theta_{t} &= - \\frac{RMS[\\Delta\\theta]_{t-1}}{RMS[g]_{t}}g_{t} \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} + \\Delta \\theta_{t}\n",
    "\\end{align*}\n",
    "\n",
    "　この後者の更新規則では学習率の項を含まないため、学習率の初期値を与える必要がない。  \n",
    "   \n",
    "　TensorFlowなどの実装では、上記の更新規則を  \n",
    " \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} &= \\theta_{t} + \\alpha \\Delta \\theta_{t}\n",
    "\\end{align*}\n",
    "\n",
    "　としており、引数として与えたlearning rateは $\\alpha$ に代入される。  \n",
    "\n",
    "### RMSprop\n",
    "　Adadeltaとは独立に編み出されたAdagradの改良版で、その更新規則は\n",
    "\n",
    "\\begin{align*}\n",
    "E[g^{2}]_{t} &= 0.9 E[g^{2}]_{t-1} + 0.1 g^{2}_{t} \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} - \\frac{\\eta}{\\sqrt{E[g^{2}]_{t} + \\epsilon}} g_{t}\n",
    "\\end{align*}\n",
    "\n",
    "　となり、Adadeltaのベクトル更新の式で $\\gamma = 0.9$ とした場合と同じである。  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "　AdadeltaやRMSpropで使われた過去の勾配の２乗 $v_{t}$ の指数関数的減衰平均に加え、  \n",
    "　過去の勾配 $m_{t}$ の指数関数的減衰平均を保持しする。  \n",
    "　$m_{t}, v_{t}$ はそれぞれ勾配の一次モーメントと二次モーメントの概算値である。  \n",
    "\n",
    "\\begin{align*}\n",
    "m_{t} &= \\beta_{1}m_{t-1} + (1-\\beta_{1})g_{t} \\\\\n",
    "v_{t} &= \\beta_{2}v_{t-1} + (1-\\beta_{2})g_{t}^{2}\n",
    "\\end{align*}\n",
    "\n",
    "　これらのモーメントの偏りをバイアス補正した推定値を用いてパラメータを更新する。    \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{m_{t}} &= \\frac{m_{t}}{1-\\beta^{t}_{1}} \\\\\n",
    "\\hat{v_{t}} &= \\frac{v_{t}}{1-\\beta^{t}_{2}} \\\\\n",
    "\\theta_{t+1} &= \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v_{t}}} + \\epsilon}\\hat{m_{t}}\n",
    "\\end{align*}\n",
    "\n",
    "　元論文では初期値として、$\\beta_{1}=0.9, \\beta_{2}=0.999, \\epsilon=10^{-8}$ が提案されている。\n",
    " \n",
    "### AdaMax\n",
    "　Kingma and Ba (2015) によって提案された Optimizer である。  \n",
    "　Adamの更新式では、 $v_{t}$ の項は勾配を過去の勾配（ $v_{t-1}$ に含まれる）と  \n",
    "　現在の勾配の $l_{2}$ ノルムに反比例してscaleさせる。  \n",
    "\n",
    "\\begin{align*}\n",
    "v_{t} &= \\beta_{2}v_{t-1} + (1-\\beta_{2})|g_{t}|^{2}\n",
    "\\end{align*}\n",
    "\n",
    "　これを $l_{p}$ ノルムについて一般化すると次のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "v_{t} &= \\beta_{2}^{p}v_{t-1} + (1-\\beta_{2}^{p})|g_{t}|^{p}\n",
    "\\end{align*}\n",
    "\n",
    "　一般に大きな $p$ に対するノルムは不安定であり $l_{1}$、$l_{2}$ が実践でよく用いられるが、  \n",
    "　$l_{\\infty}$ ノルムも概して安定な振舞いを示す。  \n",
    "　著者らの報告では、$p=\\infty$ としたときの $v_{t}$ を改めて $u_{t}$ とおいた  \n",
    " \n",
    "\\begin{align*}\n",
    "u_{t} &= \\beta_{\\infty}^{p}v_{t-1} + (1-\\beta_{2}^{\\infty})|g_{t}|^{\\infty} \\\\\n",
    "&= max(\\beta_{2} \\cdot v_{t-1}, |g_{t}|)\n",
    "\\end{align*}\n",
    "\n",
    "　はより安定した振舞いを見せた。  \n",
    "　これを用いて、AdaMax の更新式は次のように書ける。  \n",
    " \n",
    "\\begin{align*}\n",
    "\\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{u_{t}}\\hat{m_{t}}\n",
    "\\end{align*}\n",
    "\n",
    "　提案されている値は $\\eta=0.002, \\beta_{1}=0.9, \\beta_{2}=0.999$ である。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Optimizer の使用法  \n",
    "  \n",
    "TensorFlowとKerasに実装されている[Optimizer](https://www.tensorflow.org/api_guides/python/train#Optimizers)は次の通り。  \n",
    "Kerasではインスタンスをmodel.compile(optimizer=[optimizer instance])で渡すほか、名前で指定することもできる。  \n",
    "\n",
    "|Methods|TensorFlow Optimizers|Keras Optimizers|\n",
    "|:--|:--|:--|\n",
    "|SGD|tf.train.GradientDescentOptimizer|keras.optimizers.SGD|\n",
    "|Momentum|tf.train.MomentumOptimizer|keras.optimizers.SGD|\n",
    "|Nesterov Momentum|tf.train.MomentumOptimizer|keras.optimizers.SGD|\n",
    "|Adagrad|tf.train.AdagradOptimizer|keras.optimizers.Adagrad|\n",
    "|Adagrad Dual Averaging|tf.train.AdagradDAOptimizer||\n",
    "|Adadelta|tf.train.AdadeltaOptimizer|keras.optimizers.Adadelta|\n",
    "|Adam|tf.train.AdamOptimizer|keras.optimizers.Adam|\n",
    "|Adamax| |keras.optimizers.Adamax|\n",
    "|Nadam| |keras.optimizers.Nadam|\n",
    "|FTRL|tf.train.FtrlOptimizer||\n",
    "|Proximal GD|tf.train.ProximalGradientDescentOptimizer||\n",
    "|Proximal Adagrad|tf.train.ProximalAdagradOptimizer||\n",
    "|RMSProp|tf.train.RMSPropOptimizer|keras.optimizers.RMSprop|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Early Stopping\n",
    "学習回数を過剰に増やした場合、訓練データでの誤差は減少するものの、  \n",
    "検証データでの誤差が増加する（Overfitting）ことが知られている。  \n",
    "  \n",
    "<**注**>：  \n",
    "　2016年末ごろまでは主流の説でしたが、2017年11月現在では意見が割れています。  \n",
    "　Early Stoppingを考えなしに本番用モデルへ導入することは推奨できません。  \n",
    "　単なる P hacking に終わる恐れがあります。  \n",
    "  \n",
    "Overfittingを防ぐため、何らかの基準を設けて学習を早期終了させるのがEarly Stoppingであり  \n",
    "予測精度の改善が一定回数連続して規定値を下回った場合に学習を終了させるという実装が多い。  \n",
    "  \n",
    "Tensorflowでは各エポックの最後などに判定処理を実装する。  \n",
    "Kerasではコールバックとして実装されており、手軽に導入できる。  \n",
    "  \n",
    "本notebookでは実装は省略する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Batch Normalization\n",
    "ミニバッチ学習では、例え訓練データを白色化している場合でも、ミニバッチ間で分布に差が生じる。  \n",
    "これをミニバッチ毎に正規化するのが Batch Normalization である。  \n",
    "  \n",
    "　参考：  \n",
    "　・[Batch Normalization の理解](https://qiita.com/t-tkd3a/items/14950dbf55f7a3095600)  \n",
    "　・[Understanding the backward pass through Batch Normalization Layer](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実装時はlayerとして用意した上で、学習可能なパラメータとして scale = $\\gamma$ , shift = $\\beta$ を与え、  \n",
    "ミニバッチ $B=\\{x_{i} \\, | i=1, \\ldots, m\\}$ について\n",
    "\n",
    "\\begin{align*}\n",
    "mean : \\, \\mu_{B} &=\\frac{1}{m} \\sum_{i=1}^{m} x_{i} \\\\\n",
    "var : \\, \\sigma^{2}_{B} &= \\frac{1}{m}\\sum_{i=1}^{m}(x_{i}-\\mu_{B})^{2}\n",
    "\\end{align*}\n",
    "\n",
    "を求め、各データ $x_{i}$ を次のように正規化して $\\{ y_{i} \\}$ を出力する。  \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{x_{i}} &= \\frac{x_{i} - \\mu_{B}}{\\sqrt{\\sigma^{2}_{B} + \\epsilon}} \\\\\n",
    "y_{i} &= \\gamma \\hat{x_{i}} + \\beta\n",
    "\\end{align*}\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization layerの勾配を求める。\n",
    "誤差関数を $C$ として\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial \\gamma} &= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial \\gamma} \\\\\n",
    "&= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial y_{i}} \\cdot \\hat{x_{i}} \\\\\n",
    "\\mbox{} \\\\\n",
    "\\frac{\\partial C}{\\partial \\beta} &= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial \\beta} \\\\\n",
    "&= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial y_{i}} \\\\\n",
    "\\mbox{} \\\\\n",
    "\\frac{\\partial C}{\\partial x_{i}} &= \\frac{\\partial C}{\\partial \\hat{x_{i}}} \\frac{\\partial \\hat{x_{i}}}{\\partial x_{i}} + \\frac{\\partial C}{\\partial \\sigma^{2}_{B}} \\frac{\\partial \\sigma^{2}_{B}}{\\partial x_{i}} + \\frac{\\partial C}{\\partial \\mu_{B}} \\frac{\\partial \\mu_{B}}{\\partial x_{i}} \\\\\n",
    "&= \\frac{\\partial C}{\\partial x_{i}} \\cdot \\frac{1}{\\sqrt{\\sigma^{2}_{B} + \\epsilon}} + \\frac{\\partial C}{\\partial \\sigma^{2}_{B}} \\cdot \\frac{2(x_{i} - \\mu_{B})}{m} + \\frac{\\partial C}{\\partial \\mu_{B}} \\cdot \\frac{1}{m}\n",
    "\\end{align*}\n",
    "\n",
    "となる。右辺の各勾配は\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial y_{i}} &= Backpropagated \\, Error \\\\\n",
    "\\mbox{} \\\\\n",
    "\\frac{\\partial C}{\\partial \\hat{x_{i}}} &= \\frac{\\partial C}{\\partial y_{i}} \\frac{\\partial y_{i}}{\\partial \\hat{x_{i}}} \\\\\n",
    "&= \\frac{\\partial C}{\\partial y_{i}} \\cdot \\gamma \\\\\n",
    "\\mbox{} \\\\\n",
    "\\frac{\\partial C}{\\partial \\sigma^{2}_{B}} &= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial x_{i}} \\frac{\\partial x_{i}}{\\sigma^{2}_{B}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial \\hat{x_{i}}} \\cdot (x_{i} - \\mu_{B}) \\cdot \\frac{-1}{2} (\\sigma^{2}_{B} + \\epsilon)^{- \\frac{3}{2}} \\\\\n",
    "\\mbox{} \\\\\n",
    "\\frac{\\partial C}{\\partial \\mu_{B}} &= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial \\hat{x_{i}}} \\frac{\\partial \\hat{x_{i}}}{\\partial \\mu_{B}} + \\frac{\\partial C}{\\partial \\sigma^{2}_{B}} \\frac{\\partial \\sigma^{2}_{B}}{\\partial \\mu_{B}} \\\\\n",
    "&= \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial \\hat{x_{i}}} \\cdot \\frac{-1}{\\sqrt{\\sigma^{2}_{B} + \\epsilon}} + \\sum_{i=1}^{m} \\frac{\\partial C}{\\partial \\sigma^{2}_{B}} \\cdot \\frac{-2 (x_{i} - \\mu_{B})}{m}\n",
    "\\end{align*}\n",
    "\n",
    "となるので、勾配を求めることができる。  \n",
    "また、バイアス項は正規化処理によって消える。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 実装\n",
    "TensorFlow, KerasともAPIで用意されているので使用は簡単である。  \n",
    "  \n",
    "TensorFlowでは [tf.contrib.layers.batch_norm](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm) や [tf.nn.batch_normalization](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)により、  \n",
    "一例としては、モデル中の適当な場所に以下のようなコードを挟めばよい。  \n",
    "\n",
    "\n",
    "```python\n",
    "h = tf.contrib.layers.batch_norm(input_h, center=True, scale=True, is_training=phase)\n",
    "\n",
    "# 中略　後半はgammaとbetaをupdateするのに必要\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss)\n",
    "```\n",
    "\n",
    "別の例では\n",
    "\n",
    "```python\n",
    "Wn = tf.Variable(Wn_initial)\n",
    "Zn = tf.matmul(h[n-1], Wn)\n",
    "batch_mean, batch_var = tf.nn.moments(input_x, axes=[0])\n",
    "\n",
    "gamma_n = tf.Variable()\n",
    "beta_n = tf.Variable()\n",
    "\n",
    "h[n] = tf.nn.batch_normalization(Zn, batch_mean, batch_var, beta_n, scale_n, epsilon)\n",
    "\n",
    "```\n",
    "\n",
    "Kerasでは\n",
    "\n",
    "```python\n",
    "model.add(BatchNormalization())\n",
    "```\n",
    "  \n",
    "となる。  \n",
    "  \n",
    "（参考）：  \n",
    "　・[Implementing Batch Normalization in Tensorflow](https://r2rt.com/implementing-batch-normalization-in-tensorflow.html)  \n",
    "　・[TENSORFLOW GUIDE: BATCH NORMALIZATION](http://ruishu.io/2016/12/27/batchnorm/)  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Batch Normalization のAPIによらない実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 脱線：生物の神経系における正規化の例\n",
    "生物の神経においても正規化に相当する情報処理が見つかっており、  \n",
    "例えば [S.Dasgupta et al.(2017) \"A neural algorithm for a fundamental computing problem\"](http://science.sciencemag.org/content/358/6364/793) は、  \n",
    "ハエの嗅覚野にてLSH（Locality-Sensitive Hashing）に相当する処理が行われていると主張している。  \n",
    " - １層目：臭いの入力を、異なる臭いの種類と濃度に対して同じ平均を持つように正規化した発火率の指数分布に変換  \n",
    " - ２層目：入力50から出力2000へ、疎かつ二値のランダム射影  \n",
    " - ３層目：APLから抑制性フィードバックを受け、Winner Takes All 則により上位5%のみを残す。これがhash値として働く  \n",
    "  \n",
    "Table 1. The fenerality of locality-sensitive hashing in the brain.:\n",
    "\n",
    "||Step 1|Random projection|Step 2 (expansion)|Step 3 (WTA)|\n",
    "|:--|-:-|-:-|-:-|\n",
    "|Fly olfaction|Antennae lobe; 50 glomeruli|Sparse, binary: samples siz glomeruli|Mushroom body; 2000 Kenyon cells|APL neuron; top 5%|\n",
    "|Mouse olfaction|Olfactory bulb; 1000 glomeruli|Dense, weak: sampes all glomeruli|Piriform cortex: 100,000 semi-lunar cells|Layer 2A: top 10%|\n",
    "|Rat cerebellum|Precerebellar nuclei|Sparse, binary: samples four precereellar nuceli|Granule cell layer: 250 million granule cells|Golgi cells: top 10 to 20%|\n",
    "|Rat hippocampus|Entorhinal cortes: 30,000 grid cells | Unknown| Dentate gyrus: 1.2 million granule cells|Hilar cells: top 2%|\n",
    "  \n",
    "Quoted from S.Dasgupta et al(2017)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
