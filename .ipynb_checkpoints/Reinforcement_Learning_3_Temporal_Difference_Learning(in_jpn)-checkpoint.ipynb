{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD学習 (Temporal Difference Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : Richard S. Sutton and Andrew G.Barto, 「強化学習」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化学習の枠組み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の枠組みは、学習と意思決定を行う「エージェント」と  \n",
    "それ以外のすべてから構成される「環境」の相互作用として表される。  \n",
    "  \n",
    "離散的な時間ステップ　$t=0,1,...$のあるステップtにおいて、  \n",
    "エージェントは環境から状態$s_{t}$を受け取り、行動$a_{t}$を選択する。  \n",
    "  \n",
    "このとき、状態$s_{t}$から行動$a_{t}$への写像はエージェントの方策（policy）と呼ばれ、$\\pi(s,a)$で表される。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、エージェントは、最終的に受け取る報酬を最大化することを目標として学習する。  \n",
    "一般的には期待収益（expected return）を最大化するように設定される。  \n",
    "  \n",
    "各時間ステップtに受け取る報酬を$r_{t}$とするとき、  \n",
    "最も単純な場合には、収益$R_{t}$は  \n",
    "　$R(t) = r_{t+1} + r_{t+2} + ... + r_{T}  $  \n",
    "として表される。ここでTは最終時間ステップである。（相互作用が離散的な場合）  \n",
    "  \n",
    "連続タスクにおいてはT=∞となりR(t)が発散しうるため、割引収益  \n",
    "$$\n",
    "\\begin{align}\n",
    "R_{t} &= r_{t+1} + \\gamma r_{t+2} + \\gamma^{2} r_{t+3} + ... \\\\  \n",
    "　　　&= \\sum_{k=0}^{\\infty}\\gamma^{k} r_{t+k+1} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "を最大化するようにa(t)を選択する。  \n",
    "ただし、γは割引率と呼ばれるパラメータで(0<=γ<=1)である。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的な強化学習アルゴリズムは価値関数に基づく評価を行っている。  \n",
    "方策πのもとでの状態sの価値Vπ(s)は、MDP（マルコフ決定過程）では  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) = E_{\\pi}\\{R_{t} | s_{t}=s\\} = E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "と表される。関数 $ V^{\\pi} $を方策πに対する状態価値観数と呼ぶ。\n",
    "  \n",
    "同様に、方策πのもとで状態sにおいて行動aを取ることの価値を $Q^{\\pi}(s,a)$で表し、  \n",
    "状態sで行動aを取り、その後に方策πに従った期待報酬として次のように定義する。  \n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{\\pi}(s,a)=E_{\\pi}\\{R_{t}|s_{t}=s, a_{t}=a\\} = E_{\\pi}\\{\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}|s_{t}=s, a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のs,aが与えられたときの次に可能な各状態s'の確率を遷移確率と呼ぶ。  \n",
    "$$\n",
    "\\begin{align}\n",
    "P^{a}_{ss'} = Pr\\{s_{t+1}=s'| s_{t}=s, a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "同様にして、次の報酬の期待値を次のように表す。\n",
    "$$\n",
    "\\begin{align}\n",
    "R^{a}_{ss'}=E\\{r_{t+1}| s_{t}=s, a_{t}=a, s_{t+1}=s'\\}\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "    \n",
    "強化学習と動的計画法で使われている価値観数は、  \n",
    "任意の方策πと状態sに対して、sの価値と可能な後続状態群の価値との間に  \n",
    "以下の整合性条件（consistency condition）がなりたち、これを$V^{\\pi}$に対するBellman方程式という。  \n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'} [ R^{a}_{ss'} + \\gamma E_{\\pi} \\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t+1}=s'\\} ] \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{\\pi}(s')]\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いま、すべての状態に対して、方策πの期待収益がπ'よりも良いか同じであるなら、  \n",
    "πはπ'よりも良いか、同じであると定義する。  \n",
    "つまり、すべての $ s \\in S $ に対して、$ V^{\\pi}(s) \\leqq V^{\\pi'}(s) $ であるなら、その時に限り $\\pi \\leqq \\pi'$である。\n",
    "  \n",
    "これが１つの最適方策であり、すべての最適方策を$\\pi^{*}$と記す。  \n",
    "最適方策群は最適状態価値関数 $V^{*}(s) = \\max_{\\pi}V^{\\pi}(s)$ を共有する。    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に、最適方策群は最適行動価値関数 $Q^{*}(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)$ を共有する。  \n",
    "$V^{*}$を用いて$Q^{*}$を次のように書くことができる。  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{*}(s,a)=E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{*}$に対するBellman方程式を、Bellman最適方程式という。  \n",
    "  \n",
    "$$\n",
    "\\begin{align}\n",
    "V^{*}(s) &= \\max_{a \\in A(s)}Q^{\\pi^{*}}(s,a) \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\max_{a}\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{*}(s')]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$Q^{*}$に対するBellman最適方程式は次の通り。\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{*}(s) &= E\\{r_{t+1}+\\gamma \\max_{a'}Q^{*}(s_{t+1},a')|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma \\max_{a'} Q^{*}(s',a')]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD法は経験を用いて予測問題を解決し、方策πに従って経験をいくつか得ることで  \n",
    "$V^{\\pi}$ の推定値$V$を更新する手法の一つである。  \n",
    "  \n",
    "最も単純なTD法はTD(0)と呼ばれ、以下のようになる。\n",
    "$$\n",
    "\\begin{align}\n",
    "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha [ r_{t+1}+\\gamma V(s_{t+1}) - V(s_{t})]\n",
    "\\end{align}\n",
    "$$\n",
    "ここで$\\alpha$はステップサイズパラメータである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{\\pi}$に対するBellman方程式より\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "\\therefore V^{\\pi}(s) &= E_{\\pi}\\{r_{t+1}+ \\gamma V^{\\pi}(s_{t+1})|s_{t}=s \\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "としたとき、モンテカルロ法は前者の推定値を、動的計画法は後者の推定値を目標とする。  \n",
    "TD法は両者を融合させたものである。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブル型 TD(0) アルゴリズム\n",
    "Algorithm\n",
    ">$V(s)$を任意に初期化し、$\\pi$を評価対象の方策に初期化する  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$ a \\leftarrow s $に対して$\\pi$で与えられる行動  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$V(s) \\leftarrow V(s) + \\alpha[r+\\gamma V(s')-V(s)]$  \n",
    ">　　　　$s \\leftarrow s'$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD予測法を制御問題に適用する方法について考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa : 方策オン型TD制御\n",
    "行動価値関数を学習するためにTD法を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化\n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q$から導かれる方策を用いて、$s'$での行動$a'$を選択する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s'; a \\leftarrow a';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WIP\n",
    "\n",
    "# エージェントと環境との相互作用は離散的であり、エピソード的タスク群に分解されること、\n",
    "# および行動の集合Aと状態の集合Sは有限の要素しか持たず、\n",
    "# その数は学習開始時に既知であることを仮定する。\n",
    "# policyとしてはe-greedyを用いる。\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Q_function(object):\n",
    "    def __init__(self, learning_rate, discount_rate, state_space_size, initial_value=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.state_space_size = state_space_size\n",
    "        self.value_table = np.ones(self.state_space_size) * initial_value\n",
    "    \n",
    "    def estimate_value(self, state):\n",
    "        return self.value_table[state]\n",
    "    \n",
    "    def update_value_table(self, state, reward, next_state):\n",
    "        self.value_table[state] = self.value_table[state] + self.learning_rate * \\\n",
    "                                    (reward + self.discount_rate * self.value_table[next_state] - self.value_table[state])\n",
    "    \n",
    "    \n",
    "class Policy_e_greed(object):\n",
    "    def __init__(self, value_function, state_space_size, action_space_size, \n",
    "                 initial_play_count=0, reward_table=None, \n",
    "                 epsilon=0.1, min_choose=1):\n",
    "        self.value_function = value_function\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.play_count = initial_play_count\n",
    "        self.min_choose = min_choose\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def act_greedy(self):\n",
    "        \n",
    "    def act(self):\n",
    "        \n",
    "    def update_reward_table(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # expected_reward[action_number] = (selected_count, sum of rewards, avarage of rewards)\n",
    "        if expected_reward is None or expected_reward=={}:\n",
    "            self.expected_reward = {}\n",
    "            for i in range(action_space_size):\n",
    "                self.expected_reward[i] = [0, 0., 0.]\n",
    "        else:\n",
    "            self.expected_reward = expected_reward\n",
    "        \n",
    "\n",
    "    def act_greedy(self):\n",
    "        keys_of_less_selected = [key for key in self.expected_reward if self.expected_reward[key][0] < self.min_choose]\n",
    "        if len(keys_of_less_selected)!=0:\n",
    "            action = int(np.random.choice(keys_of_less_selected, 1))\n",
    "            return action\n",
    "        else:\n",
    "            max_val = max(self.expected_reward[x][2] for x in self.expected_reward)\n",
    "            keys_of_max_expected = [key for key in self.expected_reward if self.expected_reward[key][2] == max_val]\n",
    "            action = int(np.random.choice(keys_of_max_expected))\n",
    "            return action    \n",
    "        \n",
    "    def act(self):\n",
    "        if np.random.choice([1, 0], p=[self.epsilon, 1-self.epsilon]):\n",
    "            action = int(np.random.choice(range(action_space_size)))\n",
    "        else:\n",
    "            action = self.act_greedy()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, action_space_size, min_choose=1, initial_play_count=0, expected_reward=None):\n",
    "        \n",
    "            \n",
    "    def act(self):\n",
    "        keys_of_less_selected = [key for key in self.expected_reward if self.expected_reward[key][0] < self.min_choose]\n",
    "        if len(keys_of_less_selected)!=0:\n",
    "            action = int(np.random.choice(keys_of_less_selected, 1))\n",
    "            return action\n",
    "        else:\n",
    "            max_val = max(self.expected_reward[x][2] for x in self.expected_reward)\n",
    "            keys_of_max_expected = [key for key in self.expected_reward if self.expected_reward[key][2] == max_val]\n",
    "            action = int(np.random.choice(keys_of_max_expected))\n",
    "            return action\n",
    "        \n",
    "    def update(self, action, reward):\n",
    "        self.play_count += 1\n",
    "        self.expected_reward[action][0] += 1\n",
    "        self.expected_reward[action][1] += reward\n",
    "        self.expected_reward[action][2] = self.expected_reward[action][1] / self.expected_reward[action][0]\n",
    "\n",
    "    def load_record(self, initial_play_count, expected_reward):\n",
    "        self.play_count = initial_play_count\n",
    "        if expected_reward is None or expected_reward=={}:\n",
    "            self.expected_reward = {}\n",
    "            for i in range(action_space_size):\n",
    "                self.expected_reward[i] = [0, 0., 0.]\n",
    "        else:\n",
    "            self.expected_reward = expected_reward\n",
    "        \n",
    "    def save_record(self):\n",
    "        return self.play_count, self.expected_reward\n",
    "\n",
    "    def reset_record(self):\n",
    "        self.play_count = 0\n",
    "        self.expected_reward = {}\n",
    "        for i in range(action_space_size):\n",
    "            self.expected_reward[i] = [0, 0., 0.]\n",
    "    \n",
    "    def sum_reward(self):\n",
    "        sum_reward = 0\n",
    "        for value in self.expected_reward.values():\n",
    "            sum_reward += value[1]\n",
    "        return sum_reward\n",
    "    \n",
    "    def debug_sum(self):\n",
    "        for action in self.expected_reward:\n",
    "            print(\"self.expected_reward[{0}]\".format(action), self.expected_reward[action])\n",
    "        print(\"sum_reward\", self.sum_reward())\n",
    "        #print(\"sum_reward_fix\", self.sum_reward_fix())\n",
    "    \n",
    "    def action_count(self):\n",
    "        action_count = []\n",
    "        for value in self.expected_reward.values():\n",
    "            action_count.append(value[0])\n",
    "        return action_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q学習 : 方策オフ型TD制御\n",
    "SARSAでは次状態$s'$を観測した後に$Q$から導かれる方策により行動$a'$を選択したが、  \n",
    "Q学習では$\\max_{a'}Q(s',a')$を与える$a'$を用いる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化\n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化    \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma \\max_{a'} Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
