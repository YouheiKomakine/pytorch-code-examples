{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network : ニューラルネットワークによるQ関数の近似+α"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original:  \n",
    "[Mnih,V. *et al.* Playing Atari with Deep Reinforcement Learning. *arXiv*:1312.5602v1 [cs.LG] (19 Dec 2013)](https://arxiv.org/abs/1312.5602)  \n",
    "[Mnih,V. *et al.* Human-level control through deep reinforcement learning. *Nature* 518, 529–533 (2015)](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html)    \n",
    "  \n",
    "A story on official blog:  \n",
    "[From Pixels to Actions: Human-level control through Deep Reinforcement Learning. Google Reseach Blog](https://research.googleblog.com/2015/02/from-pixels-to-actions-human-level.html)\n",
    "\n",
    "\n",
    "other sources:  \n",
    "[ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)  \n",
    "[いまさらだけどTensorFlowでDQN(不完全版)を実装する](https://qiita.com/yuishihara/items/0e530e9c0a17a7fa0111)  \n",
    "[いまさらだけどTensorflowでDQN（完全版）を実装する](https://qiita.com/yuishihara/items/73e8f8c4a30b8148d9fc)  \n",
    "[DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた](https://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)  \n",
    "[倒立振子で学ぶ DQN (Deep Q Network)](https://qiita.com/ashitani/items/bb393e24c20e83e54577)  \n",
    "[DQNをKerasとTensorFlowとOpenAI Gymで実装する | Elix Tech Blog](https://elix-tech.github.io/ja/2016/06/29/dqn-ja.html)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q学習（再掲）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化    \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma \\max_{a'} Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回はテーブル型TD学習の一種としてQ学習を実装したが、  \n",
    "Q関数 $Q(s,a)$ をニューラルネットワークで近似することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Mnih. et al.(2013)における手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェントが環境 $\\varepsilon$（論文中ではAtariエミュレータ）と相互作用するタスクを考える。  \n",
    "各ステップにおいてエージェントは可能な行動の集合$A$に属する行動 $a_{t}\\in A = \\{1,\\ldots,K\\}$を選択し、  \n",
    "環境 $\\varepsilon$ は確率的に内部状態を変化させるが、これはエージェントから観測できず、  \n",
    "画像 $x_{t} \\in \\mathbb{R}^{d}$ （$x_{t}$はゲーム画面のデータ）、ゲームスコアの変化を示す報酬 $r_{t}$ を受け取る。  \n",
    "また、一般に、現在の画像 $x_{t}$ のみから現在の状態の完全な情報を得ることはできない。  \n",
    "  \n",
    "ここで、行動と観測のシーケンス $s_{t}=x_{1},a_{1},x_{2},\\ldots,a_{t-1},x_{t}$ を考える。  \n",
    "環境がエミュレータであるとき、全てのシーケンスは有限の時間ステップで終了するので、  \n",
    "上記の定式化によりfinite MDP（有限マルコフ決定過程）として捉えることができる。  \n",
    "  \n",
    "エージェントの目標は将来の報酬を最大化することである。  \n",
    "報酬の割引率を $\\gamma$ 、報酬の総量を $R_{t}=\\sum^{T}_{t'=t}\\gamma^{t'-t}r_{t'}$ | (Tは最終タイムステップ)、  \n",
    "最適行動価値関数 $Q^{*}(s,a) = \\max_{\\pi}E[R_{t}\\,|\\,s_{t}=s, a_{t}=a, \\pi]$ | ($\\pi$はpolicy)　とする。  \n",
    "  \n",
    "$\\ $  \n",
    "  \n",
    "Bellman最適方程式より\n",
    "\n",
    "\\begin{align*}\n",
    "Q^{*}(s,a) = E_{s'~\\varepsilon}\\left[ r+\\gamma \\max_{a'}Q^{*}(s',a')\\,|\\,s,a\\right] \\tag{1}\n",
    "\\end{align*}\n",
    "\n",
    "古典的手法では行動価値関数は個別のシーケンスに対してそれぞれ推定されるため、一般化されず実用的でなかった。  \n",
    "これを解決するために、近似器を用いて行動価値関数を推定する手法がよく用いられている。  \n",
    "\n",
    "\\begin{align*}\n",
    "Q(s,a;\\theta) \\approx Q^{*}(s,a)\n",
    "\\end{align*}\n",
    "\n",
    "V Mnih(2013)の手法では、それまで典型的だった線形近似器ではなく、非線形近似器であるニューラルネットワークを用いる。  \n",
    "重みを $\\theta$ として、誤差関数 $L_{i}(\\theta_{i})$ は  \n",
    "\n",
    "\\begin{align*}\n",
    "L_{i}(\\theta_{i}) = E_{s,a~\\rho(\\cdot)}\\left[ (y_{i} - Q(s,a;\\theta_{i}))^{2} \\right] \\tag{2}\n",
    "\\end{align*}\n",
    "\n",
    "ここで $y_{i}$ はターゲット、$\\rho(s,a)$ は確率分布（behaviour distributionと呼ぶ）である。  \n",
    "$L_{i}(\\theta_{i})$ の更新時には重み $\\theta_{i-1}$ が用いられる。  \n",
    "誤差関数の微分は次の通り。  \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta_{i}}L_{i}(\\theta_{i}) = E_{s,a~\\rho(\\cdot);s'~\\varepsilon}\\left[ \\left(r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_{i})\\right) \\nabla_{\\theta_{i}}Q(s,a;\\theta_{i}) \\right] \\tag{3}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm** : Deep Q-learning with Experience Replay\n",
    "\n",
    ">Initialize replay memory *D* to capacity *N*  \n",
    ">Initialize action-value function *Q* with random weights  \n",
    ">**for** episode = 1,*M* **do**  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;Initialise sequence $s_{1} = {x_{1}}$ and preprocessed sequenced $\\phi_{1} = \\phi(s_{1})$  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;**for** *t* = 1, *T* **do**  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;With probability $\\epsilon$ select a random action $a_{t}$　  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;otherwise select $a_{t} = \\max_{a}Q^{*}(\\phi(s_{t},a;\\theta)$　  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Execute action $a_{t}$ in emulator and observe reward $r_{t}$ and image $x{t+1}$　  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Set $s_{t+1} = s_{t}, a_{t}, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$　  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Store transition ($\\phi_{t},a_{t},r_{t},\\phi_{t+1}$) in *D*　  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sample random minibatch of transitions ($\\phi_{j},a_{j},r_{j},\\phi_{j+1}$) from *D*　  \n",
    ">$$Set\\ y_{j} =\\begin{cases}\n",
    "r_{j} & \\textrm{for}\\ \\textrm{terminal}\\ \\phi_{j+1}\\\\\n",
    "r_{j} + \\gamma \\max_{a'}Q(\\phi_{j+1}, a'; \\theta) & \\textrm{for}\\ \\textrm{non-terminal}\\ \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$$\n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a gradient descent step on $(y_{i} - Q(\\phi_{j},a_{j};\\theta))^{2}$ according to equation 3  \n",
    ">&nbsp;&nbsp;&nbsp;&nbsp;end for  \n",
    ">end for  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +αのテクニック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "論文では、Q関数のニューラルネットによる近似に加え、\n",
    "下記３種のテクニックを合わせて Deep Q-Network としている。\n",
    "- Experience Replay\n",
    "- Fixed Target Q-Network\n",
    "- Reward clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experience Replay\n",
    "シーケンスは時間的に連続しているので、相関をなくすために経験のランダムサンプリングを行い学習する。  \n",
    "アルゴリズムの以下の部分で使われており、学習のターゲットとなる $y_{i}$ を生成している。\n",
    ">Sample random minibatch of transitions ($\\phi_{j},a_{j},r_{j},\\phi_{j+1}$) from *D*  \n",
    ">Set $$\\ y_{j} =\\begin{cases}\n",
    "r_{j} & \\textrm{for}\\ \\textrm{terminal}\\ \\phi_{j+1}\\\\\n",
    "r_{j} + \\gamma \\max_{a'}Q(\\phi_{j+1}, a'; \\theta) & \\textrm{for}\\ \\textrm{non-terminal}\\ \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed Target Q-Network\n",
    "上記の式(3)：  \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta_{i}}L_{i}(\\theta_{i}) = E_{s,a~\\rho(\\cdot);s'~\\varepsilon}\\left[ \\left(r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1}) - Q(s,a;\\theta_{i})\\right) \\nabla_{\\theta_{i}}Q(s,a;\\theta_{i}) \\right] \\tag{3}\n",
    "\\end{align*}\n",
    "\n",
    "において、更新の目標となる $r+\\gamma\\max_{a'}Q(s',a';\\theta_{i-1})$ をミニバッチ学習中に固定し、  \n",
    "学習後に $\\theta_{i-1}$ を $\\theta_{i}$ へ更新することで、学習の不安定性を軽減する。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward clipping\n",
    "全ての正の報酬を+1へ、負の報酬を-1へclippingすることで、学習を容易にする。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "WIP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
