{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network : ニューラルネットワークによるQ関数の近似+α"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original:  \n",
    "[Mnih,V. *et al.* Playing Atari with Deep Reinforcement Learning. *arXiv*:1312.5602v1 [cs.LG] (19 Dec 2013)](https://arxiv.org/abs/1312.5602)  \n",
    "[Mnih,V. *et al.* Human-level control through deep reinforcement learning. *Nature* 518, 529–533 (26 February 2015)](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html)    \n",
    "  \n",
    "A story on official blog:  \n",
    "[From Pixels to Actions: Human-level control through Deep Reinforcement Learning. Google Reseach Blog](https://research.googleblog.com/2015/02/from-pixels-to-actions-human-level.html)\n",
    "\n",
    "\n",
    "other sources:  \n",
    "[ゼロからDeepまで学ぶ強化学習](https://qiita.com/icoxfog417/items/242439ecd1a477ece312)  \n",
    "[いまさらだけどTensorFlowでDQN(不完全版)を実装する](https://qiita.com/yuishihara/items/0e530e9c0a17a7fa0111)  \n",
    "[いまさらだけどTensorflowでDQN（完全版）を実装する](https://qiita.com/yuishihara/items/73e8f8c4a30b8148d9fc)  \n",
    "[DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた](https://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q学習（再掲）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化    \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma \\max_{a'} Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回はテーブル型TD学習の一種としてQ学習を実装したが、  \n",
    "Q関数　$Q(s,a)$　をニューラルネットワークで近似することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V Mnih. et al.(2013)における手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm : Deep Q-learning with Experience Replay\n",
    ">Initialize replay memory *D* to capacity *N*  \n",
    ">Initialize action-value function *Q* with random weights  \n",
    ">**for** episode = 1,*M* **do**    \n",
    ">　　Initialise sequence $s_{1} = {x_{1}}$ and preprocessed sequenced $\\phi_{1} = \\phi(s_{1})$  \n",
    ">　　**for** *t* = 1, *T* **do**  \n",
    ">　　　　With probability $\\epsilon$ select a random action $a_{t}$  \n",
    ">　　　　otherwise select $a_{t} = \\max_{a}Q^{*}(\\phi(s_{t},a;\\theta)$  \n",
    ">　　　　Execute action $a_{t}$ in emulator and observe reward $r_{t}$ and image $x{t+1}$  \n",
    ">　　　　Set $s_{t+1} = s_{t}, a_{t}, x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$  \n",
    ">　　　　Store transition ($\\phi_{t},a_{t},r_{t},\\phi_{t+1}$) in *D*  \n",
    ">　　　　Sample random minibatch of transitions ($\\phi_{j},a_{j},r_{j},\\phi_{j+1}$) from *D*  \n",
    ">$$Set\\ y_{j} =\\begin{cases}\n",
    "r_{j} & \\textrm{for terminal}\\ \\phi_{j+1}\\\\\n",
    "r_{j} + \\gamma \\max_{a'}Q(\\phi_{j+1}, a'; \\theta) & \\textrm{for non-terminal}\\ \\phi_{j+1}\n",
    "\\end{cases}\n",
    "$$\n",
    ">　　　　Perform a gradient descent step on $(y_{i} - Q(\\phi_{j},a_{j};\\theta))^{2}$ according to equation 3  \n",
    ">　　end for  \n",
    ">end for  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
