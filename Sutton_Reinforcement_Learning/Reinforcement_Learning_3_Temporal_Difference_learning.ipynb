{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD学習 (Temporal Difference Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source : Richard S. Sutton and Andrew G.Barto, 「強化学習」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 強化学習の枠組み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "強化学習の枠組みは、学習と意思決定を行う「エージェント」と  \n",
    "それ以外のすべてから構成される「環境」の相互作用として表される。  \n",
    "  \n",
    "離散的な時間ステップ　$t=0,1,...$のあるステップtにおいて、  \n",
    "エージェントは環境から状態$s_{t}$を受け取り、行動$a_{t}$を選択する。  \n",
    "  \n",
    "このとき、状態$s_{t}$から行動$a_{t}$への写像はエージェントの方策（policy）と呼ばれ、$\\pi(s,a)$で表される。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、エージェントは、最終的に受け取る報酬を最大化することを目標として学習する。  \n",
    "一般的には期待収益（expected return）を最大化するように設定される。  \n",
    "  \n",
    "各時間ステップtに受け取る報酬を$r_{t}$とするとき、  \n",
    "最も単純な場合には、収益$R_{t}$は  \n",
    "\n",
    "\\begin{align*}\n",
    "R(t) = r_{t+1} + r_{t+2} + ... + r_{T}\n",
    "\\end{align*}\n",
    "\n",
    "として表される。ここでTは最終時間ステップである。（相互作用が離散的な場合）  \n",
    "  \n",
    "連続タスクにおいてはT=∞となりR(t)が発散しうるため、割引収益  \n",
    "\n",
    "\\begin{align*}\n",
    "R_{t} &= r_{t+1} + \\gamma r_{t+2} + \\gamma^{2} r_{t+3} + ... \\\\  \n",
    "　　　&= \\sum_{k=0}^{\\infty}\\gamma^{k} r_{t+k+1} \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "を最大化するようにa(t)を選択する。  \n",
    "ただし、γは割引率と呼ばれるパラメータで(0<=γ<=1)である。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般的な強化学習アルゴリズムは価値関数に基づく評価を行っている。  \n",
    "方策πのもとでの状態sの価値Vπ(s)は、MDP（マルコフ決定過程）では  \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "と表される。関数 $ V^{\\pi} $を方策πに対する状態価値観数と呼ぶ。\n",
    "  \n",
    "同様に、方策πのもとで状態sにおいて行動aを取ることの価値を $Q^{\\pi}(s,a)$で表し、  \n",
    "状態sで行動aを取り、その後に方策πに従った期待報酬として次のように定義する。  \n",
    "\n",
    "\\begin{align*}\n",
    "Q^{\\pi}(s,a) &= E_{\\pi}\\{R_{t}|s_{t}=s, a_{t}=a\\} \\\\\n",
    "&= E_{\\pi}\\{\\sum_{k=0}^{\\infty}\\gamma^{k}R_{t+k+1}|s_{t}=s, a_{t}=a\\}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のs,aが与えられたときの次に可能な各状態s'の確率を遷移確率と呼ぶ。  \n",
    "\n",
    "\\begin{align*}\n",
    "P^{a}_{ss'} = Pr\\{s_{t+1}=s'| s_{t}=s, a_{t}=a\\}\n",
    "\\end{align*}\n",
    "\n",
    "  \n",
    "同様にして、次の報酬の期待値を次のように表す。\n",
    "\n",
    "\\begin{align*}\n",
    "R^{a}_{ss'}=E\\{r_{t+1}| s_{t}=s, a_{t}=a, s_{t+1}=s'\\}\n",
    "\\end{align*}\n",
    "\n",
    "  \n",
    "    \n",
    "強化学習と動的計画法で使われている価値関数は、  \n",
    "任意の方策πと状態sに対して、sの価値と可能な後続状態群の価値との間に  \n",
    "以下の整合性条件（consistency condition）がなりたち、これを$V^{\\pi}$に対するBellman方程式という。  \n",
    "  \n",
    "\n",
    "\\begin{align*}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'} [ R^{a}_{ss'} + \\gamma E_{\\pi} \\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t+1}=s'\\} ] \\\\\n",
    "&= \\sum_{a}\\pi(s,a)\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{\\pi}(s')]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いま、すべての状態に対して、方策πの期待収益がπ'よりも良いか同じであるなら、  \n",
    "πはπ'よりも良いか、同じであると定義する。  \n",
    "つまり、すべての $ s \\in S $ に対して、$ V^{\\pi}(s) \\leqq V^{\\pi'}(s) $ であるなら、その時に限り $\\pi \\leqq \\pi'$である。\n",
    "  \n",
    "これが１つの最適方策であり、すべての最適方策を$\\pi^{*}$と記す。  \n",
    "最適方策群は最適状態価値関数 $V^{*}(s) = \\max_{\\pi}V^{\\pi}(s)$ を共有する。    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に、最適方策群は最適行動価値関数 $Q^{*}(s,a)=\\max_{\\pi}Q^{\\pi}(s,a)$ を共有する。  \n",
    "$V^{*}$を用いて$Q^{*}$を次のように書くことができる。  \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Q^{*}(s,a)=E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{*}$に対するBellman方程式を、Bellman最適方程式という。  \n",
    "  \n",
    "\n",
    "\\begin{align*}\n",
    "V^{*}(s) &= \\max_{a \\in A(s)}Q^{\\pi^{*}}(s,a) \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{ \\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+1}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E_{\\pi^{*}}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "&= \\max_{a}E\\{r_{t+1}+\\gamma V^{*}(s_{t+1})|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\max_{a}\\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma V^{*}(s')]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "$Q^{*}$に対するBellman最適方程式は次の通り。\n",
    "\n",
    "\\begin{align*}\n",
    "Q^{*}(s) &= E\\{r_{t+1}+\\gamma \\max_{a'}Q^{*}(s_{t+1},a')|s_{t}=s,a_{t}=a\\} \\\\\n",
    "&= \\sum_{s'}P^{a}_{ss'}[R^{a}_{ss'}+\\gamma \\max_{a'} Q^{*}(s',a')]\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD法は経験を用いて予測問題を解決し、方策πに従って経験をいくつか得ることで  \n",
    "$V^{\\pi}$ の推定値$V$を更新する手法の一つである。  \n",
    "  \n",
    "最も単純なTD法はTD(0)と呼ばれ、以下のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "V(s_{t}) \\leftarrow V(s_{t}) + \\alpha [ r_{t+1}+\\gamma V(s_{t+1}) - V(s_{t})]\n",
    "\\end{align*}\n",
    "\n",
    "ここで$\\alpha$はステップサイズパラメータである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V^{\\pi}$に対するBellman方程式より\n",
    "\n",
    "\\begin{align*}\n",
    "V^{\\pi}(s) &= E_{\\pi}\\{R_{t} | s_{t}=s\\} \\\\\n",
    "&= E_{\\pi}\\{r_{t+1}+ \\gamma\\sum_{k=0}^{\\infty}\\gamma^{k}r_{t+k+2}|s_{t}=s\\} \\\\\n",
    "\\therefore V^{\\pi}(s) &= E_{\\pi}\\{r_{t+1}+ \\gamma V^{\\pi}(s_{t+1})|s_{t}=s \\}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "としたとき、モンテカルロ法は前者の推定値を、動的計画法は後者の推定値を目標とする。  \n",
    "TD法は両者を融合させたものである。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブル型 TD(0) アルゴリズム\n",
    "Algorithm\n",
    ">$V(s)$を任意に初期化し、$\\pi$を評価対象の方策に初期化する  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$ a \\leftarrow s $に対して$\\pi$で与えられる行動  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$V(s) \\leftarrow V(s) + \\alpha[r+\\gamma V(s')-V(s)]$  \n",
    ">　　　　$s \\leftarrow s'$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このTD予測法を制御問題に適用する方法について考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sarsa : 方策オン型TD制御\n",
    "行動価値関数を学習するためにTD法を用いる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化  \n",
    ">　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q$から導かれる方策を用いて、$s'$での行動$a'$を選択する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s'; a \\leftarrow a';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "＊エージェントと環境との相互作用は離散的であり、エピソード的タスク群に分解されること、  \n",
    "　および行動の集合Aと状態の集合Sは有限の要素しか持たず、  \n",
    "　その数は学習開始時に既知であることを仮定する。  \n",
    "＊また、テーブル型TD(0)アルゴリズムとして実装しており、  \n",
    "　state(i) | i=0~nが一次元的に並べられることを前提としている。  \n",
    "　policyとしてはe-greedyを用いる。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Q_table_function(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 learning_rate=0.01, discount_rate=0.95, initial_value=1,random_initial_value=True,\n",
    "                 decay_learning_rate=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.initial_value = initial_value\n",
    "        self.random_initial_value = random_initial_value\n",
    "        self.decay_learning_rate = decay_learning_rate\n",
    "\n",
    "        if self.random_initial_value:\n",
    "            self.q_table = np.random.rand(self.state_space_size, self.action_space_size) * self.initial_value\n",
    "        else:\n",
    "            self.q_table = np.ones((self.state_space_size, self.action_space_size), dtype=float64) * self.initial_value\n",
    "\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        # for debug\n",
    "        #print(\"Q-table initialize. self.q_table = \", self.q_table)\n",
    "    \n",
    "    def estimate_q_value(self, state, action=None):\n",
    "        # for debug\n",
    "        #self.last_state, self.last_action = state, action\n",
    "\n",
    "        if action is None:\n",
    "            return self.q_table[state]\n",
    "        else:\n",
    "            return self.q_table[state][action]\n",
    "    \n",
    "    def update_q_table(self, reward, next_state, next_action,\n",
    "                       last_state=None, last_action=None):\n",
    "        last_state = self.last_state if last_state is None else last_state\n",
    "        last_action = self.last_action if last_action is None else last_action\n",
    "\n",
    "        delta = reward + self.discount_rate * self.estimate_q_value(next_state, next_action) \\\n",
    "                - self.estimate_q_value(last_state, last_action)\n",
    "        self.q_table[last_state][last_action] = self.q_table[last_state][last_action] + self.learning_rate * delta\n",
    "\n",
    "    def decay_learning_rate_value(self, decay_rate=None):\n",
    "        decay_rate = self.decay_learning_rate if decay_rate is None else decay_rate\n",
    "        if 0<=decay_rate<=1:\n",
    "            self.learning_rate = self.learning_rate * decay_rate\n",
    "\n",
    "    def save_q_table(self):\n",
    "        return self.q_table\n",
    "\n",
    "    def load_q_table(self, q_table=None):\n",
    "        if q_table is not None:\n",
    "            self.q_table = q_table\n",
    "\n",
    "    def reset(self, reset_q_table=True, learning_rate=None, discount_rate=None, decay_learning_rate=None):\n",
    "        if reset_q_table:\n",
    "            if self.random_initial_value:\n",
    "                self.q_table = np.random.rand(self.state_space_size, self.action_space_size) * self.initial_value\n",
    "            else:\n",
    "                self.q_table = np.ones((self.state_space_size, self.action_space_size), dtype=float64) * self.initial_value\n",
    "        self.learning_rate = learning_rate if learning_rate is not None else self.learning_rate\n",
    "        self.discount_rate = discount_rate if discount_rate is not None else self.discount_rate\n",
    "        self.decay_learning_rate = decay_learning_rate if decay_learning_rate is not None else self.decay_learning_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Policy_e_greedy(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 action_count_list=None,\n",
    "                 initial_play_count=None, epsilon=0.1, min_choose=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.total_play_count = 0\n",
    "        self.min_choose = min_choose\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        # self.action_count_list[action] = number of [action] is choosed\n",
    "        if action_count_list is None:\n",
    "            self.action_count_list = np.zeros(self.action_space_size, dtype=int)\n",
    "        else:\n",
    "            self.action_count_list = action_count_list\n",
    "        if initial_play_count is not None:\n",
    "            self.total_play_count = initial_play_count\n",
    "\n",
    "    def choose_act_greedy(self, state, value_table):\n",
    "        # for debug\n",
    "        #print(\"for choose_act_greedy, value_table = \", value_table)\n",
    "        \n",
    "        index_of_less_selected = np.where(self.action_count_list<self.min_choose)[0]\n",
    "        #for debug\n",
    "        #print(\"in policy.choose_act_greedy, self.action_count_list = \",self.action_count_list)\n",
    "        #print(\"in policy.choose_act_greedy, index_of_less_selected = \",index_of_less_selected)\n",
    "        \n",
    "        #if index_of_less_selected[0].size == 0:\n",
    "        if len(index_of_less_selected) == 0:\n",
    "            #max_index = np.where(value_table == value_table.max())\n",
    "            max_index = np.where(value_table == value_table.max())\n",
    "            # for debug\n",
    "            #print(\"in policy.choose_act_greedy, max_index = {0}, value_table = {1}\".format(max_index, value_table))\n",
    "            \n",
    "            \n",
    "            action = np.random.choice(max_index[0], 1)\n",
    "            \n",
    "            # for debug\n",
    "            #print(\"in policy.choose_act_greedy, max_index = {0}, action = {1}\".format(max_index, action))\n",
    "            \n",
    "        else:\n",
    "            action = int(np.random.choice(index_of_less_selected, 1))\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def choose_act(self, state, value_table, update_flag=True, epsilon=None):\n",
    "        epsilon = self.epsilon if epsilon is None else epsilon\n",
    "        if np.random.choice([1, 0], p=[epsilon, 1-epsilon]):            \n",
    "            action = int(np.random.choice(range(self.action_space_size)))\n",
    "            \n",
    "            # for debug\n",
    "            #print(\"in policy, random choice, action = {0}\".format(action))\n",
    "            \n",
    "        else:\n",
    "            action = self.choose_act_greedy(state, value_table)\n",
    "            # for debug\n",
    "            #print(\"in policy, act_greedy, action = {0}\".format(action))\n",
    "            \n",
    "        # for debug\n",
    "        #print(\"in policy.choose_act(), epsilon = {0}, action = {1}\".format(epsilon, action))\n",
    "\n",
    "        self.last_state, self.last_action = state, action\n",
    "        if update_flag:\n",
    "            self.total_play_count += 1\n",
    "            self.action_count_list[action] += 1\n",
    "            \n",
    "        # for debug\n",
    "        #print(\"in policy, action = {0}\".format(action))\n",
    "        \n",
    "        return int(action)\n",
    "        #return action[0]\n",
    "\n",
    "    def save_record(self):\n",
    "        return self.action_count_list, self.total_play_count\n",
    "    \n",
    "    def load_record(self, action_count_list=None, total_play_count=None):\n",
    "        self.action_count_list = action_count_list if action_count_list is not None else self.action_count_list\n",
    "        self.total_play_count = total_play_count if total_play_count is not None else self.total_play_count\n",
    "    \n",
    "    def reset_record(self, reset_count=True, reset_last=True):\n",
    "        if reset_count:\n",
    "            self.total_play_count = 0\n",
    "            self.action_count_list = np.zeros(self.action_space_size, dtype=int)\n",
    "        if reset_last:\n",
    "            self.last_state = None\n",
    "            self.last_action = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent_SARSA(object):\n",
    "    def __init__(self, state_space_size, action_space_size,\n",
    "                 state_function, q_function=None, policy_function=None,\n",
    "                 learning_rate=0.01, discount_rate=0.95, \n",
    "                 initial_value=1, random_initial_value=True, decay_learning_rate=1,\n",
    "                 action_count_list=None, initial_play_count=None, epsilon=0.1, min_choose=1):\n",
    "        self.state_space_size = state_space_size\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_rate = discount_rate\n",
    "        self.initial_value = initial_value\n",
    "        self.random_initial_value = random_initial_value\n",
    "        self.decay_learning_rate = decay_learning_rate\n",
    "        self.action_count_list = action_count_list\n",
    "        self.initial_play_count = initial_play_count\n",
    "        self.epsilon = epsilon\n",
    "        self.min_choose = min_choose\n",
    "\n",
    "        self.total_play_count = 0\n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "\n",
    "        self.state_function = state_function\n",
    "\n",
    "        if q_function is None:\n",
    "            self.q_function = Q_table_function(self.state_space_size, self.action_space_size,\n",
    "                                               self.learning_rate, self.discount_rate, self.initial_value,\n",
    "                                               self.random_initial_value, self.decay_learning_rate)\n",
    "        else:\n",
    "            self.q_function = q_function\n",
    "        self.q_function.reset()\n",
    "\n",
    "        if policy_function is None:\n",
    "            self.policy_function = Policy_e_greedy(self.state_space_size, self.action_space_size,\n",
    "                                                   self.action_count_list, self.initial_play_count,\n",
    "                                                   self.epsilon, self.min_choose)\n",
    "        else:\n",
    "            self.policy_function = policy_function\n",
    "        self.policy_function.reset_record()\n",
    "            \n",
    "        # for debug\n",
    "        #print(\"in agent.init(), self.q_function.q_table = \", self.q_function.q_table)\n",
    "\n",
    "\n",
    "    def act(self, state, update_flag=True, epsilon=None):\n",
    "        value_table = self.q_function.estimate_q_value(state, action=None)\n",
    "        \n",
    "        # for debug\n",
    "        #print(\"in agent.act(), state is = \", state)\n",
    "        #print(\"in agent.act(), value_table = \", value_table)\n",
    "        \n",
    "        action = self.policy_function.choose_act(state, value_table, update_flag=update_flag, epsilon=epsilon)\n",
    "        self.last_state, self.last_action = state, action\n",
    "        return action\n",
    "\n",
    "    def observe_state(self, action=None):\n",
    "        # state_function must return (reward, next_state, episode_end_flag)\n",
    "        return self.state_function.return_next(action)  \n",
    "\n",
    "    def learning_step(self, action, update_flag=True, epsilon=None):\n",
    "        # for debug\n",
    "        #print(\"in agent.learning_step(), action, type = \", action, type(action))\n",
    "        \n",
    "        reward, next_state, episode_end_flag = self.observe_state(action)\n",
    "        if episode_end_flag:\n",
    "            next_action = -1\n",
    "        else:\n",
    "            next_action = self.act(next_state, update_flag, epsilon)\n",
    "        if update_flag:\n",
    "            self.q_function.update_q_table(reward, next_state, next_action,\n",
    "                                           self.last_state, self.last_action)\n",
    "        return reward, next_state, next_action, episode_end_flag\n",
    "\n",
    "    def learn(self, episode_num, maximum_trial_per_episode=1000, \n",
    "              save_flag=True, record_reward=True, update_flag=True, reset_when_finished=False):\n",
    "        reward = 0\n",
    "        episode_end_flag = False\n",
    "        trial_count = 0\n",
    "        record = []\n",
    "\n",
    "        # episode loop\n",
    "        for episode in range(episode_num):\n",
    "            # choose initial action\n",
    "            self.state_function.reset_state()\n",
    "            reward, state, episode_end_flag = self.observe_state()\n",
    "            action = self.act(state, update_flag)\n",
    "            \n",
    "            while episode_end_flag==False and (trial_count < maximum_trial_per_episode):\n",
    "                reward, state, action, episode_end_flag = self.learning_step(action)\n",
    "                trial_count += 1\n",
    "                \n",
    "                # for debug\n",
    "                #print(\"in learn-loop, trial_count={0}, state={1}, action={2}\".format(trial_count, state, action))\n",
    "                #print(\"  reward={0}, episode_end_flag={1}\".format(reward, episode_end_flag))\n",
    "                \n",
    "            else:\n",
    "                if record_reward:\n",
    "                    record.append(reward)\n",
    "                reward, episode_end_flag, trial_count = 0, False, 0\n",
    "                \n",
    "        save_data = self.save() if save_flag else None\n",
    "        if reset_when_finished:\n",
    "            self.reset()\n",
    "\n",
    "        return record, save_data\n",
    "\n",
    "    def demo_play(self, episode_num=1, maximum_trial_per_episode=1000):\n",
    "        self.learn(episode_num, maximum_trial_per_episode, safe_flag=False, update_flag=False)\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "        # returns (q_table, (action_count_list, total_play_count))\n",
    "        q_save_data = self.q_function.save_q_table()\n",
    "        policy_save_data = self.policy_function.save_record()\n",
    "        return q_save_data, policy_save_data\n",
    "\n",
    "    def load(self, q_save_data=None, policy_save_data=None):\n",
    "        if q_save_data is not None:\n",
    "            self.q_function.load_q_table(q_table)\n",
    "        if policy_save_data is not None:\n",
    "            self.policy_function.load_record(self, action_count_list=policy_save_data[0], \n",
    "                                             total_play_count=policy_save_data[1])\n",
    "\n",
    "    def reset(self, reset_q_table=True, reset_count=True, reset_last=True,\n",
    "              learning_rate=None, discount_rate=None,decay_learning_rate=None):\n",
    "        self.q_function.reset(reset_q_table, learning_rate, discount_rate, decay_learning_rate)\n",
    "        self.policy_function.reset_record(reset_count, reset_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q学習 : 方策オフ型TD制御\n",
    "SARSAでは次状態$s'$を観測した後に$Q$から導かれる方策により行動$a'$を選択したが、  \n",
    "Q学習では$\\max_{a'}Q(s',a')$を与える$a'$を用いる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm\n",
    ">$Q(s,a)$を任意に初期化  \n",
    ">各エピソードに対して繰り返し：  \n",
    ">　　$s$を初期化    \n",
    ">　　エピソードの各ステップに対して繰り返し：  \n",
    ">　　　　$Q$から導かれる方策（εグリーディ方策など）を用いて、$s$で取る行動$a$を選択する  \n",
    ">　　　　行動$a$を取り、報酬$r$と次状態$s'$を観測する  \n",
    ">　　　　$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r+\\gamma \\max_{a'} Q(s',a')-Q(s,a)]$  \n",
    ">　　　　$s \\leftarrow s';$  \n",
    ">　　$s$が終端状態ならば繰り返しを終了 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from SARSA import Agent_SARSA, Q_table_function, Policy_e_greedy\n",
    "\n",
    "class Agent_Q_learning(Agent_SARSA):\n",
    "    def learning_step(self, action, update_flag=True):\n",
    "        reward, next_state, episode_end_flag = self.observe_state(action)\n",
    "        if episode_end_flag:\n",
    "            next_action = -1\n",
    "        else:\n",
    "            # choose a' which gives max Q(s',a')\n",
    "            next_action = self.act(next_state, update_flag, epsilon=0)\n",
    "        if update_flag:\n",
    "            self.q_function.update_q_table(reward, next_state, next_action,\n",
    "                                           self.last_state, self.last_action)\n",
    "                       \n",
    "        return reward, next_state, next_action, episode_end_flag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 環境：FrozenLake  \n",
    "OpenAI Gym のFrozenLake-v0を一部改変したもの。  \n",
    "map_size = (横、縦)のサイズの環境があり、startから開始してgoalに移動することを目指す。  \n",
    "地面にはflat, start, goal, holeの４種類があり、  \n",
    "goalに移動した時に+1の報酬を、holeの場所に移動した場合は-1の報酬を得てエピソードが終了する。  \n",
    "  \n",
    "行動は0:上、1:右、2:下、3:左　の４種類。  \n",
    "ただし移動する際、slip_rateに等しい確率で進行方向とは90度曲がった方向へ進んでしまう。  \n",
    "また、マップ外へ移動しようとした時はその場でとどまる。  \n",
    "  \n",
    "hole_numの値域の範囲内で穴の個数が決まり、ランダムに配置される。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Environment_FrozenLake\n",
    "#   init()\n",
    "#       input : \n",
    "#           map_size=(width,height)     #\n",
    "#           slip_rate                   # Player will slip when move from \"F\" (Icey ground) according to slip_rate.\n",
    "#                                       #   (90 degrees curved from the direction which action you choosed)\n",
    "#           hole_num=(a,b)              # Number of Hole \"n\" will choose randomly, s.t. a<=n<=b\n",
    "#           start=(x,y), goal=(x,y)     # Start / Goal position, (right/down are positive direction)\n",
    "#           max_retry=(a,b)             # Maximum number of retry to generate map.\n",
    "#                                       #   a : max num to retry in the same hole_num.\n",
    "#                                       #      if retry_count > a,  hole_num is decremented.\n",
    "#                                       #   b : max num to decrement hole_num\n",
    "#           action                      # dictionary to definit action.\n",
    "#                                       #   For example, action[i]=(a,b) will change player's position\n",
    "#                                       #   from (x,y) to (x+a,y+b) (if it can be done)\n",
    "#           ground_name                 # dictionary to show map with Uppercase letter\n",
    "#\n",
    "#   return_next()\n",
    "#       input  : \n",
    "#           action\n",
    "#       output : \n",
    "#           reward                      # +1 (when reach to Goal), -1 (when reach to Hole)\n",
    "#           next_state                  # int n.  s.t. 0<=n<=(width*height)-1 \n",
    "#                                       #   When player is on (x,y), n= (y-1)*height + (x-1)\n",
    "#           episode_end_flag            # True when player move to Goal or Hole, False in other position.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import ipywidgets\n",
    "\n",
    "class Environment_FrozenLake(object):\n",
    "    def __init__(self, map_size=(4,4), slip_rate=0.33, hole_num=(1,3), \n",
    "                 start=(1,1), goal=(-1,-1),max_retry=(10,10),\n",
    "                 reach_to_outside_map = False,\n",
    "                 action={0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)},\n",
    "                 ground_name={1:\"F\", 2:\"S\", 3:\"G\", 4:\"H\"}):\n",
    "        self.map_size = np.array(map_size)\n",
    "        self.slip_rate = slip_rate\n",
    "        self.hole_num = np.random.randint(hole_num[0], hole_num[1]+1)\n",
    "        self.start = np.array((start[0]-1, start[1]-1)) if min(start)>=1 else np.array((0,0))\n",
    "        self.goal = np.array((goal[0]-1, goal[1]-1)) if min(goal)>=1 else np.array(map_size)+np.array(goal)\n",
    "        self.max_retry = max_retry\n",
    "        self.reach_to_outside_map=False\n",
    "        self.action = action\n",
    "        self.ground_name = ground_name\n",
    "        self.ground_name[0] = \"N\"\n",
    "\n",
    "        self.player_position = np.array(self.start, dtype=int)\n",
    "        self.reward = 0\n",
    "        self.done=False\n",
    "        \n",
    "    def check_route(self, map_dup):\n",
    "        checkmap = copy.deepcopy(map_dup)\n",
    "        checkmap[(self.start[0],self.start[1])] = 1\n",
    "        checkmap[(self.goal[0],self.goal[1])] = 0\n",
    "        for i in range(self.map_size[0] * self.map_size[1]):\n",
    "            if (0 in checkmap)==False:\n",
    "                break\n",
    "            for h in range(self.map_size[1]):\n",
    "                for w in range(self.map_size[0]):\n",
    "                    if checkmap[h][w]==0: \n",
    "                        if checkmap[max(0,h-1)][w]==1 or checkmap[h][max(0,w-1)]==1 or \\\n",
    "                            checkmap[min(self.map_size[1]-1,h+1)][w]==1 or checkmap[h][min(self.map_size[0]-1,w+1)]==1:\n",
    "                            checkmap[h][w] = 1\n",
    "                            \n",
    "        can_reach = True if checkmap[(self.goal[0],self.goal[1])]==1 else False\n",
    "        return can_reach\n",
    "                            \n",
    "    def generate_map(self, max_retry=None):\n",
    "        # 番号との対応は、１：氷、２：スタート地点、３：ゴール、４：穴\n",
    "        if (max_retry is None) or (min(max_retry) <= 0) or (max(max_retry) >= 256):\n",
    "            max_retry = self.max_retry\n",
    "        self.map = np.zeros(self.map_size, dtype=int)\n",
    "        self.map[(self.start[0],self.start[1])] = 2\n",
    "        self.map[(self.goal[0],self.goal[1])] = 3\n",
    "        retry_count = [0,0]\n",
    "        success_check = False\n",
    "                \n",
    "        while (retry_count[1] < max_retry[1]) and (success_check==False):\n",
    "            while (retry_count[0] < max_retry[0]) and (success_check==False):\n",
    "                map_dup = copy.deepcopy(self.map)\n",
    "                chip_index_list = np.where(self.map==0)\n",
    "                chip_index = np.random.choice(chip_index_list[0].size, self.hole_num, replace=False)\n",
    "                \n",
    "                for i in range(len(chip_index)):\n",
    "                    map_dup[chip_index_list[0][chip_index[i]]][chip_index_list[1][chip_index[i]]] = 4 \n",
    "                \n",
    "                retry_count[0] += 1\n",
    "                if self.check_route(map_dup):\n",
    "                    self.map = np.where(map_dup==4, map_dup, self.map)\n",
    "                    self.map = np.where(self.map==0, 1, self.map)\n",
    "                    success_check = True\n",
    "                    break\n",
    "            else:\n",
    "                self.hole_num = max(1, self.hole_num-1)\n",
    "                retry_count[1] += 1\n",
    "        return success_check\n",
    "\n",
    "    def show_map(self, raw_data=False, player_position=False, print_map=True, end=\"\\n\", \n",
    "                 raw_array=False, html=False):\n",
    "        map_text = \"\"\n",
    "        output = \"\"\n",
    "        if raw_array:\n",
    "            return self.map\n",
    "        \n",
    "        for h in range(self.map_size[0]):\n",
    "            for w in range(self.map_size[1]):\n",
    "                map_text += self.ground_name[self.map[h][w]]\n",
    "                if w == self.map_size[1]-1:\n",
    "                    map_text += end\n",
    "        if raw_data:\n",
    "            output += str(self.map) + end\n",
    "        else:\n",
    "            output += map_text + end\n",
    "        if player_position:\n",
    "            output += \"player (x,y) = \" + str(self.player_position) + end\n",
    "        \n",
    "        if html:\n",
    "            player_pos = self.player_position[1] * (self.map_size[0]+1) + self.player_position[0]\n",
    "            pre = output[:player_pos] if player_pos>=1 else \"\"\n",
    "            post = output[player_pos+1:] if player_pos <= len(output)-1 else \"\"\n",
    "            html_text = pre + '<font color=\"red\">' + output[player_pos] + '</font>' + post \n",
    "            html_text = html_text.replace(\"\\n\",\"<br />\")\n",
    "            return html_text\n",
    "\n",
    "        if print_map:\n",
    "            print(output)\n",
    "        else:\n",
    "            return output\n",
    "                \n",
    "    def return_next(self, action=None, render_flag=False):\n",
    "        if action is None:\n",
    "            state = self.player_position[1] * self.map_size[0] + self.player_position[0]\n",
    "            \n",
    "            # for debug\n",
    "            #print(\"in Envirionment, return_next() called with acion=None. state = \", state)\n",
    "            \n",
    "            return 0, state, False\n",
    "\n",
    "        # move direction\n",
    "        move_p = np.array(self.action[action]) # action={0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)} in default settings\n",
    "        slip_A, slip_B = move_p[::-1], move_p[::-1]*-1\n",
    "\n",
    "        move = np.array([move_p, slip_A, slip_B])[np.random.choice(3, 1, p=[1-self.slip_rate, self.slip_rate/2, self.slip_rate/2])][0]\n",
    "\n",
    "        if self.reach_to_outside_map==False:\n",
    "            self.player_position[0] = min(max(self.player_position[0]+move[0], 0), self.map_size[0]-1)\n",
    "            self.player_position[1] = min(max(self.player_position[1]+move[1], 0), self.map_size[1]-1)\n",
    "        else:\n",
    "            self.player_position = self.player_position + move\n",
    "            \n",
    "        # goal / hole check\n",
    "        if (self.player_position[0]<0) or (self.map_size[0]-1 < self.player_position[0]) \\\n",
    "            or (self.player_position[1]<0) or (self.map_size[1]-1 < self.player_position[1]): # Outside of map\n",
    "            self.reward = -1\n",
    "            self.done = True\n",
    "        elif self.map[(self.player_position[1], self.player_position[0])] == 4: # Hole\n",
    "            self.reward = -1\n",
    "            self.done = True\n",
    "        elif self.map[(self.player_position[1], self.player_position[0])] == 3: # Goal\n",
    "            self.reward = 1\n",
    "            self.done = True\n",
    "        else:\n",
    "            self.reward = 0\n",
    "            self.done = False\n",
    "            \n",
    "        next_state = self.player_position[1] * self.map_size[0] + self.player_position[0]\n",
    "\n",
    "        return self.reward, next_state, self.done\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.player_position = np.array(self.start, dtype=int)\n",
    "        self.reward, self.done = 0, False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 環境の生成とランダム行動テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cde23d17d841e5a40a4570689aa810"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import sys, time\n",
    "\n",
    "an = Environment_FrozenLake(map_size=(4,4), slip_rate=0.33, hole_num=(1,1), \n",
    "                 start=(1,1), goal=(-1,-1),max_retry=(10,10),\n",
    "                 reach_to_outside_map = False,\n",
    "                 action={0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)},\n",
    "                 ground_name={1:\"F\", 2:\"S\", 3:\"G\", 4:\"H\"})\n",
    "an.generate_map()\n",
    "ipywidgets.HTML(an.show_map(raw_data=False, player_position=True, print_map=True, end=\"\\n\", \n",
    "                 raw_array=False, html=True))\n",
    "\n",
    "# map is displayed with 4x4 letter matrix, and the player's position is displayed with red letter.\n",
    "#  s.t. 0 <= x,y <= 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41e125d63d5448e0ad212a37653d92ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab760f5c09544471a07d01f0be1f3822"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef88b4c2c564f2ab60c2225c81dd1d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbb9fc85cae4618acac39bd4bbbedca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "import sys, time\n",
    "import traitlets\n",
    "\n",
    "def loop_random_action():\n",
    "    action = np.random.randint(0,4)\n",
    "    reward, next_state, done = an.return_next(action)\n",
    "\n",
    "    returned_text = (\"reward={0}, new_position={1}, episode_end={2}\".format(reward, next_state, done))\n",
    "    html_text = an.show_map(raw_data=False, player_position=True, print_map=True, end=\"\\n\", \n",
    "                     raw_array=False, html=True)\n",
    "    #print(html_text)\n",
    "    return action, returned_text, html_text, done\n",
    "\n",
    "    \n",
    "button_action = ipywidgets.Button(description=\"Take random action\")\n",
    "button_reset = ipywidgets.Button(description=\"Click to reset\")\n",
    "radio_buttons = ipywidgets.RadioButtons(options=['auto_act','manual_act'],description=\"update:\",disabled=False)\n",
    "out = ipywidgets.Output()\n",
    "\n",
    "htext='Click \"Take random action\" to start'\n",
    "max_trial=30\n",
    "done=False\n",
    "window = ipywidgets.HTML(value=htext, placeholder='Some HTML',description=\"\")\n",
    "\n",
    "an.reset_state()\n",
    "\n",
    "def action_button_clicked(b):\n",
    "    reset_flag=1\n",
    "    step_num = 0\n",
    "    with out:\n",
    "        if reset_flag==1:\n",
    "            an.reset_state()\n",
    "            html_text = an.show_map(html=True)\n",
    "            window.value = html_text + \"Trial will start soon\"\n",
    "            reset_flag=0\n",
    "            done=False\n",
    "            time.sleep(1.0)\n",
    "        \n",
    "        action, returned_text, html_text, done = loop_random_action()\n",
    "        step_num += 1\n",
    "        window.value = html_text + \"step: {0}, action: {1}<br />\".format(step_num, action) + returned_text\n",
    "        if radio_buttons.value=='auto_act':\n",
    "                while done==False and step_num<=max_trial:\n",
    "                    action, returned_text, html_text, done = loop_random_action()\n",
    "                    step_num += 1\n",
    "                    window.value = html_text + \"step: {0}, action: {1}<br />\".format(step_num, action) + returned_text\n",
    "                    time.sleep(0.4)\n",
    "                else:\n",
    "                    reset_flag=1\n",
    "\n",
    "def reset_button_clicked(b):\n",
    "    an.reset_state()\n",
    "    html_text = an.show_map(html=True)\n",
    "    done=True\n",
    "    reset_flag=0\n",
    "    with out:\n",
    "        window.value = html_text + \"player returned start position\"\n",
    "    \n",
    "button_action.on_click(action_button_clicked)\n",
    "ipywidgets.VBox([button_action, out])\n",
    "button_reset.on_click(reset_button_clicked)\n",
    "ipywidgets.VBox([button_reset, out])\n",
    "display(button_action, radio_buttons, button_restart, window)\n",
    "\n",
    "# map is displayed with 4x4 letter matrix, and the player's position is displayed with red letter.\n",
    "#  s.t. 0 <= x,y <= 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### テーブル型SARSA, Q-learningによる強化学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenLake map:\n",
      "SFFF\n",
      "HFFH\n",
      "FFFF\n",
      "FHFG\n",
      "\n",
      "\n",
      "keep training, epoch=10 / 100\n",
      "keep training, epoch=20 / 100\n",
      "keep training, epoch=30 / 100\n",
      "keep training, epoch=40 / 100\n",
      "keep training, epoch=50 / 100\n",
      "keep training, epoch=60 / 100\n",
      "keep training, epoch=70 / 100\n",
      "keep training, epoch=80 / 100\n",
      "keep training, epoch=90 / 100\n",
      "keep training, epoch=100 / 100\n",
      "keep training, epoch=10 / 100\n",
      "keep training, epoch=20 / 100\n",
      "keep training, epoch=30 / 100\n",
      "keep training, epoch=40 / 100\n",
      "keep training, epoch=50 / 100\n",
      "keep training, epoch=60 / 100\n",
      "keep training, epoch=70 / 100\n",
      "keep training, epoch=80 / 100\n",
      "keep training, epoch=90 / 100\n",
      "keep training, epoch=100 / 100\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys, time\n",
    "\n",
    "def training_agent(agent, episode_num=3, epoch_num=3, max_trial_num=32, print_message=True):\n",
    "    \n",
    "    # for debug\n",
    "    #print(\"training_agent() called\")\n",
    "    agent.reset()\n",
    "    records = []\n",
    "    for epoch in range(epoch_num):\n",
    "        record, save_data = agent.learn(episode_num, maximum_trial_per_episode=max_trial_num,\n",
    "                                        save_flag=True, record_reward=True, update_flag=True,\n",
    "                                        reset_when_finished=False)\n",
    "        records.append(record)\n",
    "        if print_message==True and ((epoch+1)*10)%epoch_num==0:\n",
    "            print(\"keep training, epoch={0} / {1}\".format(epoch+1, epoch_num))\n",
    "    return records, save_data    \n",
    "\n",
    "\n",
    "frozenlake = Environment_FrozenLake(map_size=(4,4), slip_rate=0.33, hole_num=(3,3), \n",
    "                 start=(1,1), goal=(-1,-1),max_retry=(10,10),\n",
    "                 reach_to_outside_map = False,\n",
    "                 action={0:(0,-1), 1:(1,0), 2:(0,1), 3:(-1,0)},\n",
    "                 ground_name={1:\"F\", 2:\"S\", 3:\"G\", 4:\"H\"})\n",
    "\n",
    "agent_sarsa = Agent_SARSA(state_space_size=16, action_space_size=4,\n",
    "                 state_function=frozenlake, q_function=None, policy_function=None,\n",
    "                 learning_rate=0.01, discount_rate=0.95, \n",
    "                 initial_value=1, random_initial_value=True, decay_learning_rate=1,\n",
    "                 action_count_list=None, initial_play_count=None, epsilon=0.1, min_choose=1)\n",
    "agent_ql = Agent_Q_learning(state_space_size=16, action_space_size=4,\n",
    "                 state_function=frozenlake, q_function=None, policy_function=None,\n",
    "                 learning_rate=0.01, discount_rate=0.95, \n",
    "                 initial_value=1, random_initial_value=True, decay_learning_rate=1,\n",
    "                 action_count_list=None, initial_play_count=None, epsilon=0.1, min_choose=1)\n",
    "\n",
    "episode_num = 1000\n",
    "epoch_num = 100\n",
    "max_trial_num = 32\n",
    "\n",
    "frozenlake.generate_map()\n",
    "print(\"FrozenLake map:\")\n",
    "frozenlake.show_map(raw_data=False, player_position=False, print_map=True, end=\"\\n\", \n",
    "                 raw_array=False, html=False)\n",
    "\n",
    "# full_records[agent] = records, savedata\n",
    "# records[epoch] = record\n",
    "full_records=[]\n",
    "full_records.append(training_agent(agent_sarsa, episode_num, epoch_num, max_trial_num))\n",
    "full_records.append(training_agent(agent_ql, episode_num, epoch_num, max_trial_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEICAYAAABMGMOEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f55d541a160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# グラフ：エポック当たり平均reward\n",
    "def draw_average_reward_per_epoch_graph(full_records, title=\"Average reward / epoch\"):\n",
    "    agent_num = len(full_records)\n",
    "    epoch_num = len(full_records[0][0])\n",
    "    episode_num = len(full_records[0][0][0])\n",
    "    full_records = np.array(full_records)\n",
    "    \n",
    "    for agent in range(agent_num):\n",
    "        x = np.array(range(epoch_num))\n",
    "        \n",
    "        # for debug\n",
    "        #print(\"x\", x)\n",
    "        #print(\"draw average, full_records[agent][0] = \",full_records[agent][0])\n",
    "        #print(\"np.average, axis=1 \",np.average(full_records[agent][0], axis=1))\n",
    "        \n",
    "        y = np.average(full_records[agent][0], axis=1)\n",
    "        plt.plot(x,y, label=[\"SARSA\",\"Q-learning\"][agent])\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.xlim(1, epoch_num)\n",
    "    plt.ylim(-1, 1)\n",
    "    \n",
    "\n",
    "draw_average_reward_per_epoch_graph(full_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent SARSA's Q-table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up(0,-1)</th>\n",
       "      <th>right(1,0)</th>\n",
       "      <th>down(0,1)</th>\n",
       "      <th>left(-1,0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.672</td>\n",
       "      <td>-0.788</td>\n",
       "      <td>-0.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.375</td>\n",
       "      <td>-0.309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.078</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.641</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.315</td>\n",
       "      <td>-0.373</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.844</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.759</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.250</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>-0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.301</td>\n",
       "      <td>0.310</td>\n",
       "      <td>1.165</td>\n",
       "      <td>0.424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.228</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.473</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.381</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.417</td>\n",
       "      <td>-0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.233</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    up(0,-1)  right(1,0)  down(0,1)  left(-1,0)\n",
       "0      0.000      -0.672     -0.788      -0.663\n",
       "1      0.000       0.000      0.000       0.000\n",
       "2      0.000       0.000      0.000       0.000\n",
       "3      0.000      -0.312     -0.375      -0.309\n",
       "4      0.078       0.441      0.050       0.167\n",
       "5     -0.641       0.000     -0.642      -0.784\n",
       "6     -0.315      -0.373     -0.308       0.000\n",
       "7      0.844       0.839      0.366       0.630\n",
       "8     -0.759      -0.286      0.000      -0.403\n",
       "9      0.000      -0.250     -0.320      -0.247\n",
       "10     0.001       0.001      0.001       0.001\n",
       "11    -0.301       0.310      1.165       0.424\n",
       "12    -0.228      -0.322     -0.249       0.000\n",
       "13     0.473       0.458      0.001       0.689\n",
       "14     0.381       1.164      0.417      -0.263\n",
       "15     0.233       0.662      0.737       0.266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa_q_dataframe = pd.DataFrame(np.round(full_records[0][1][0], 3))\n",
    "sarsa_q_dataframe.columns = ['up(0,-1)', 'right(1,0)', 'down(0,1)', 'left(-1,0)']\n",
    "sarsa_q_dataframe.index = range(16)\n",
    "print(\"Agent SARSA's Q-table\")\n",
    "sarsa_q_dataframe.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent Q-learning's Q-table\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up(0,-1)</th>\n",
       "      <th>right(1,0)</th>\n",
       "      <th>down(0,1)</th>\n",
       "      <th>left(-1,0)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.485</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.547</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.509</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.139</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.428</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.333</td>\n",
       "      <td>1.610</td>\n",
       "      <td>0.666</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.877</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    up(0,-1)  right(1,0)  down(0,1)  left(-1,0)\n",
       "0      0.000      -0.485     -0.492      -0.482\n",
       "1      0.000       0.000      0.000       0.000\n",
       "2      0.000       0.000      0.000       0.000\n",
       "3      0.000      -0.002     -0.000      -0.001\n",
       "4      0.547       0.474      0.412       0.535\n",
       "5     -0.003       0.000     -0.002      -0.002\n",
       "6     -0.001      -0.001     -0.001       0.000\n",
       "7      0.509       0.756      0.681       0.804\n",
       "8     -0.003      -0.001      0.000      -0.004\n",
       "9      0.000      -0.004     -0.004      -0.002\n",
       "10     0.000       0.000      0.000       0.000\n",
       "11     0.139       0.729      0.047       0.429\n",
       "12    -0.001      -0.002     -0.006       0.000\n",
       "13     0.428       0.082      0.272       0.257\n",
       "14     0.333       1.610      0.666       0.553\n",
       "15     0.877       0.564      0.112       0.763"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql_q_dataframe = pd.DataFrame(np.round(full_records[1][1][0], 3))\n",
    "ql_q_dataframe.columns = ['up(0,-1)', 'right(1,0)', 'down(0,1)', 'left(-1,0)']\n",
    "ql_q_dataframe.index = range(16)\n",
    "print(\"Agent Q-learning's Q-table\")\n",
    "ql_q_dataframe.head(16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
