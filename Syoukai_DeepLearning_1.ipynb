{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [詳解ディープラーニング　TensorFlow・Kerasによる時系列データ処理](https://book.mynavi.jp/ec/products/detail/id=72995)\n",
    "　巣籠悠輔 著  \n",
    "　マイナビ出版  \n",
    "　ISBN : 978-4-8399-6251-7  \n",
    "　発売 : 2017/05/30  \n",
    "  \n",
    "support site : [https://book.mynavi.jp/supportsite/detail/9784839962517.html](https://book.mynavi.jp/supportsite/detail/9784839962517.html)  \n",
    "github : [yusugomori/deeplearning-tensorflow-keras](https://github.com/yusugomori/deeplearning-tensorflow-keras) \n",
    "\n",
    "Most of codes and explanations in this notebook are based on the textbook.  \n",
    "If you want to use codes in this notebook, please check the book and original codes from the above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ３章　単純パーセプトロン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"288pt\" height=\"98pt\"\n",
       " viewBox=\"0.00 0.00 288.00 97.75\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.751958 0.751958) rotate(0) translate(4 126)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-126 379,-126 379,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\"><title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-114 367,-114 367,-8 8,-8\"/>\n",
       "</g>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\"><title>x1</title>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-84.3\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "</g>\n",
       "<!-- l1a -->\n",
       "<g id=\"node3\" class=\"node\"><title>l1a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"134\" cy=\"-88\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;l1a -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>x1&#45;&gt;l1a</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.2216,-88C78.5505,-88 87.9135,-88 96.8161,-88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.9659,-91.5001 106.966,-88 96.9658,-84.5001 96.9659,-91.5001\"/>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\"><title>x2</title>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-30.3\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n",
       "</g>\n",
       "<!-- l1b -->\n",
       "<g id=\"node4\" class=\"node\"><title>l1b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"134\" cy=\"-34\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;l1b -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>x2&#45;&gt;l1b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M70.2216,-34C78.5505,-34 87.9135,-34 96.8161,-34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"96.9659,-37.5001 106.966,-34 96.9658,-30.5001 96.9659,-37.5001\"/>\n",
       "</g>\n",
       "<!-- l2 -->\n",
       "<g id=\"node5\" class=\"node\"><title>l2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"241\" cy=\"-63\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- l1a&#45;&gt;l2 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>l1a&#45;&gt;l2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159.618,-82.1386C173.211,-78.9022 190.317,-74.8294 205.251,-71.2737\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.255,-74.6324 215.173,-68.9113 204.634,-67.8227 206.255,-74.6324\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\">w1</text>\n",
       "</g>\n",
       "<!-- l1b&#45;&gt;l2 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>l1b&#45;&gt;l2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159.089,-40.6532C172.996,-44.4941 190.708,-49.3861 206.014,-53.6133\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"205.088,-56.9887 215.659,-56.2773 206.952,-50.2414 205.088,-56.9887\"/>\n",
       "<text text-anchor=\"middle\" x=\"187.5\" y=\"-54.8\" font-family=\"Times,serif\" font-size=\"14.00\">w2</text>\n",
       "</g>\n",
       "<!-- y -->\n",
       "<g id=\"node6\" class=\"node\"><title>y</title>\n",
       "<text text-anchor=\"middle\" x=\"332\" y=\"-59.3\" font-family=\"Times,serif\" font-size=\"14.00\">y</text>\n",
       "</g>\n",
       "<!-- l2&#45;&gt;y -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>l2&#45;&gt;y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.222,-63C276.551,-63 285.914,-63 294.816,-63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.966,-66.5001 304.966,-63 294.966,-59.5001 294.966,-66.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f08ec753f98>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.graph_attr['rankdir']='LR'\n",
    "dot.graph_attr['size']=\"4,4\"\n",
    "\n",
    "with dot.subgraph(name='cluster_0') as c:\n",
    "    c.node('x1', 'x1', shape='plaintext')\n",
    "    c.node('x2', 'x2', shape='plaintext')\n",
    "    c.node('l1a', '')\n",
    "    c.node('l1b', '')\n",
    "    c.edge('x1', 'l1a')\n",
    "    c.edge('x2', 'l1b')\n",
    "    c.node('l2', '')\n",
    "    c.edge('l1a','l2', label='w1')\n",
    "    c.edge('l1b', 'l2', label='w2')\n",
    "    c.node('y', 'y',shape='plaintext')\n",
    "    c.edge('l2','y')\n",
    "    c.body.append('{rank=min; x1; x2;}')\n",
    "    c.body.append('{rank=max; y;}')\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ニューロンのモデルとして、上の図のような１層パーセプトロンを考える。  \n",
    "　（ただし、確率的な挙動は一切示さないものとする）  \n",
    "入力 $x_{1}$、$x_{2}$ を受けた時に、重み付き和が閾値 $\\theta$を超えた時に発火する、すなわち  \n",
    "  \n",
    "\\begin{align*}\n",
    "y=\\begin{cases}\n",
    "1 & (w_{1}x_{1} + w_{2}x_{2} \\geq \\theta) \\\\\\\n",
    "0 & (w_{1}x_{1} + w_{2}x_{2} < \\theta) \n",
    "\\end{cases}\\end{align*}\n",
    "に従って出力 $y$ が決定するものとする。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記モデルのパラメータ $(w_{1}, w_{2}, \\theta)$ の値を誤り訂正学習法によって得ることを考える。  \n",
    "  \n",
    "正しい出力 $t$ が既知であるとき  \n",
    "　$ y > t $ なら $y$ が小さくなるように修正する    \n",
    "　$ y < t $ なら $y$ が大きくなるように修正する  \n",
    "ことによって正しいパラメータを目指す。  \n",
    "  \n",
    "この修正分を $\\Delta w_{1}$ 、 $\\Delta w_{2}$ 、 $\\Delta \\theta$ と表し、k回目の学習によるパラメータを $(w_{1}^{k}, w_{2}^{k}, \\theta^{k})$ とすると、この方法は  \n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta w_{1} &= (t-y)x_{1} & w_{1}^{k+1} &= w_{1}^{k} + \\Delta w_{1} \\\\\n",
    "\\Delta w_{2} &= (t-y)x_{2} & w_{2}^{k+1} &= w_{2}^{k} + \\Delta w_{2} \\\\\n",
    "\\Delta \\theta &= -(t-y) & \\theta^{k+1} &= \\theta^{k} + \\Delta \\theta\n",
    "\\end{align*}\n",
    "\n",
    "のようにまとめることができる。  \n",
    "この更新式を、$y-t=0$ となるまで行うことを試みる。  \n",
    "  \n",
    "パラメータの初期値を $(w_{1}, w_{2}, \\theta) = (0, 0, 0)$ 、  \n",
    "入力 $x_{1}, x_{2}$ は0または1の値をとり、正しい出力をANDゲートとするとき、   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update finished. iter_count=24\n",
      "w1, w2, theta = 2, 1, 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>t</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>theta</th>\n",
       "      <th>y</th>\n",
       "      <th>t-y</th>\n",
       "      <th>dw1</th>\n",
       "      <th>dw2</th>\n",
       "      <th>dtheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  x1  x2  t  w1  w2  theta  y  t-y  dw1  dw2  dtheta\n",
       "0    1   0   0  0   0   0      0  1   -1    0    0       1\n",
       "1    2   0   1  0   0   0      1  0    0    0    0       0\n",
       "2    3   1   0  0   0   0      1  0    0    0    0       0\n",
       "3    4   1   1  1   0   0      1  0    1    1    1      -1\n",
       "4    5   0   0  0   1   1      0  1   -1    0    0       1\n",
       "5    6   0   1  0   1   1      1  1   -1    0   -1       1\n",
       "6    7   1   0  0   1   0      2  0    0    0    0       0\n",
       "7    8   1   1  1   1   0      2  0    1    1    1      -1\n",
       "8    9   0   0  0   2   1      1  0    0    0    0       0\n",
       "9   10   0   1  0   2   1      1  1   -1    0   -1       1\n",
       "10  11   1   0  0   2   0      2  1   -1   -1    0       1\n",
       "11  12   1   1  1   1   0      3  0    1    1    1      -1\n",
       "12  13   0   0  0   2   1      2  0    0    0    0       0\n",
       "13  14   0   1  0   2   1      2  0    0    0    0       0\n",
       "14  15   1   0  0   2   1      2  1   -1   -1    0       1\n",
       "15  16   1   1  1   1   1      3  0    1    1    1      -1\n",
       "16  17   0   0  0   2   2      2  0    0    0    0       0\n",
       "17  18   0   1  0   2   2      2  1   -1    0   -1       1\n",
       "18  19   1   0  0   2   1      3  0    0    0    0       0\n",
       "19  20   1   1  1   2   1      3  1    0    0    0       0\n",
       "20  21   0   0  0   2   1      3  0    0    0    0       0\n",
       "21  22   0   1  0   2   1      3  0    0    0    0       0\n",
       "22  23   1   0  0   2   1      3  0    0    0    0       0\n",
       "23  24   1   1  1   2   1      3  1    0    0    0       0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 正解となる出力データ\n",
    "def tercher_AND_gate(x1,x2):\n",
    "    output = {(0,0):0, (0,1):0, (1,0):0, (1,1):1}\n",
    "    return output[(x1, x2)]\n",
    "\n",
    "# 使用するモデル\n",
    "def y(x1, x2, w1, w2, theta):\n",
    "    input_sum = w1*x1 + w2*x2 \n",
    "    if input_sum >= theta:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def update(dataset, max_iter=100, w1=0, w2=0, theta=0, y=y, t=tercher_AND_gate):\n",
    "    record = []\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iter:\n",
    "        val_dif = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "            x1, x2 = data[0], data[1]\n",
    "            t_val = t(x1, x2)\n",
    "            y_val = y(x1, x2, w1, w2, theta)\n",
    "            dw1 = (t_val - y_val) * x1\n",
    "            dw2 = (t_val - y_val) * x2\n",
    "            dtheta = -1 * (t_val - y_val)\n",
    "            \n",
    "            iter_count += 1\n",
    "            val_dif = val_dif + abs(t_val - y_val)\n",
    "            record.append([iter_count, x1, x2, t_val, w1, w2, theta, y_val, t_val-y_val, dw1, dw2, dtheta])\n",
    "            \n",
    "            w1 += dw1\n",
    "            w2 += dw2\n",
    "            theta += dtheta\n",
    "            \n",
    "        if val_dif==0:\n",
    "            break         \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"update finished. iter_count={0}\".format(iter_count))\n",
    "    print(\"w1, w2, theta = {}, {}, {}\".format(w1, w2, theta))\n",
    "    return record\n",
    "            \n",
    "    \n",
    "w1, w2, theta = 0, 0, 0\n",
    "dataset = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "record = update(dataset, max_iter=100, w1=w1, w2=w2, theta=theta, t=tercher_AND_gate)\n",
    "df = pd.DataFrame(record)\n",
    "df.columns = [\"k\", \"x1\", \"x2\", \"t\", \"w1\", \"w2\", \"theta\", \"y\", \"t-y\", \"dw1\", \"dw2\", \"dtheta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update finished. iter_count=16\n",
      "w1, w2, theta = 1, 1, 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>t</th>\n",
       "      <th>w1</th>\n",
       "      <th>w2</th>\n",
       "      <th>theta</th>\n",
       "      <th>y</th>\n",
       "      <th>t-y</th>\n",
       "      <th>dw1</th>\n",
       "      <th>dw2</th>\n",
       "      <th>dtheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k  x1  x2  t  w1  w2  theta  y  t-y  dw1  dw2  dtheta\n",
       "0    1   0   0  0   0   0      0  1   -1    0    0       1\n",
       "1    2   0   1  1   0   0      1  0    1    0    1      -1\n",
       "2    3   1   0  1   0   1      0  1    0    0    0       0\n",
       "3    4   1   1  1   0   1      0  1    0    0    0       0\n",
       "4    5   0   0  0   0   1      0  1   -1    0    0       1\n",
       "5    6   0   1  1   0   1      1  1    0    0    0       0\n",
       "6    7   1   0  1   0   1      1  0    1    1    0      -1\n",
       "7    8   1   1  1   1   1      0  1    0    0    0       0\n",
       "8    9   0   0  0   1   1      0  1   -1    0    0       1\n",
       "9   10   0   1  1   1   1      1  1    0    0    0       0\n",
       "10  11   1   0  1   1   1      1  1    0    0    0       0\n",
       "11  12   1   1  1   1   1      1  1    0    0    0       0\n",
       "12  13   0   0  0   1   1      1  0    0    0    0       0\n",
       "13  14   0   1  1   1   1      1  1    0    0    0       0\n",
       "14  15   1   0  1   1   1      1  1    0    0    0       0\n",
       "15  16   1   1  1   1   1      1  1    0    0    0       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORゲートの場合\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tercher_OR_gate(x1,x2):\n",
    "    output = {(0,0):0, (0,1):1, (1,0):1, (1,1):1}\n",
    "    return output[(x1, x2)]\n",
    "\n",
    "w1, w2, theta = 0, 0, 0\n",
    "dataset = [[0,0,0],[0,1,0],[1,0,0],[1,1,1]]\n",
    "\n",
    "record = update(dataset, max_iter=100, w1=w1, w2=w2, theta=theta, t=tercher_OR_gate)\n",
    "df = pd.DataFrame(record)\n",
    "df.columns = [\"k\", \"x1\", \"x2\", \"t\", \"w1\", \"w2\", \"theta\", \"y\", \"t-y\", \"dw1\", \"dw2\", \"dtheta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update finished. iter_count=6\n",
      "w, theta = -1, 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>x</th>\n",
       "      <th>t</th>\n",
       "      <th>w</th>\n",
       "      <th>theta</th>\n",
       "      <th>y</th>\n",
       "      <th>t-y</th>\n",
       "      <th>dw</th>\n",
       "      <th>dtheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   k  x  t  w  theta  y  t-y  dw  dtheta\n",
       "0  1  0  1  0      0  1    0   0       0\n",
       "1  2  1  0  0      0  1   -1  -1       1\n",
       "2  3  0  1 -1      1  0    1   0      -1\n",
       "3  4  1  0 -1      0  0    0   0       0\n",
       "4  5  0  1 -1      0  1    0   0       0\n",
       "5  6  1  0 -1      0  0    0   0       0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTゲートの場合\n",
    "\n",
    "# 書籍と結果が違うが、書籍のほうでは\n",
    "# y=1 if w*x >= theta　の不等号が　>　に置き換わっている？\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def tercher_NOT_gate(x):\n",
    "    output = {0:1, 1:0}\n",
    "    return output[x]\n",
    "\n",
    "def y_single(x, w, theta):\n",
    "    return 1 if w*x >= theta else 0\n",
    "\n",
    "    # 書籍と同じ結果を出すなら\n",
    "    #return 1 if w*x > theta else 0\n",
    "\n",
    "def update_NOT_gate(dataset, max_iter=100, w=0, theta=0, y=y_single, t=tercher_NOT_gate):\n",
    "    record = []\n",
    "    iter_count = 0\n",
    "    while iter_count < max_iter:\n",
    "        val_dif = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "            x = data[0]\n",
    "            t_val, y_val = t(x), y(x, w, theta)\n",
    "            dw, dtheta = (t_val - y_val) * x, -1*(t_val-y_val)\n",
    "            iter_count += 1\n",
    "            val_dif = val_dif + abs(t_val - y_val)\n",
    "            record.append([iter_count, x, t_val, w, theta, y_val, t_val-y_val, dw, dtheta])\n",
    "            w += dw\n",
    "            theta += dtheta  \n",
    "        if val_dif==0:\n",
    "            break         \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    print(\"update finished. iter_count={0}\".format(iter_count))\n",
    "    print(\"w, theta = {}, {}\".format(w, theta))\n",
    "    return record\n",
    "    \n",
    "w, theta, dataset = 0, 0, [[0,1],[1,0]]\n",
    "record = update_NOT_gate(dataset, max_iter=100, w=w, theta=theta, y=y_single, t=tercher_NOT_gate)\n",
    "df = pd.DataFrame(record)\n",
    "df.columns = [\"k\", \"x\", \"t\", \"w\", \"theta\", \"y\", \"t-y\", \"dw\", \"dtheta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update finished. iter_count=16\n",
      "w, theta = [1 1], 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>x</th>\n",
       "      <th>t</th>\n",
       "      <th>w</th>\n",
       "      <th>theta</th>\n",
       "      <th>y</th>\n",
       "      <th>t-y</th>\n",
       "      <th>dw</th>\n",
       "      <th>dtheta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     k       x  t       w theta  y  t-y      dw  dtheta\n",
       "0    1  [0, 0]  0  [0, 0]     1  1   -1  [0, 0]       1\n",
       "1    2  [0, 1]  1  [0, 0]     1  0    1  [0, 1]      -1\n",
       "2    3  [1, 0]  1  [0, 1]     1  1    0  [0, 0]       0\n",
       "3    4  [1, 1]  1  [0, 1]     1  1    0  [0, 0]       0\n",
       "4    5  [0, 0]  0  [0, 1]     1  1   -1  [0, 0]       1\n",
       "5    6  [0, 1]  1  [0, 1]     1  1    0  [0, 0]       0\n",
       "6    7  [1, 0]  1  [0, 1]     1  0    1  [1, 0]      -1\n",
       "7    8  [1, 1]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "8    9  [0, 0]  0  [1, 1]     1  1   -1  [0, 0]       1\n",
       "9   10  [0, 1]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "10  11  [1, 0]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "11  12  [1, 1]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "12  13  [0, 0]  0  [1, 1]     1  0    0  [0, 0]       0\n",
       "13  14  [0, 1]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "14  15  [1, 0]  1  [1, 1]     1  1    0  [0, 0]       0\n",
       "15  16  [1, 1]  1  [1, 1]     1  1    0  [0, 0]       0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一括対応ver\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 正解となる出力データ\n",
    "def teacher_gate(x, teacher_data):\n",
    "    return teacher_data[tuple(x)]\n",
    "\n",
    "# 使用するモデル\n",
    "def gate_model(x,w,theta):\n",
    "    return 1 if np.sum(np.multiply(x,w)) >= theta else 0\n",
    "    \n",
    "def update(dataset, w, theta, teacher_data, max_iter=100, y=gate_model, t=teacher_gate):\n",
    "    record = []\n",
    "    iter_count, val_dif = 0, 1\n",
    "    while (iter_count < max_iter) and (val_dif != 0):\n",
    "        val_dif = 0\n",
    "        \n",
    "        for x in dataset:\n",
    "            x = np.array(x)\n",
    "            t_val, y_val = t(x, teacher_data), y(x, w, theta)\n",
    "            dw = (t_val - y_val) * x\n",
    "            dtheta = -1 * (t_val - y_val)\n",
    "            \n",
    "            iter_count += 1\n",
    "            val_dif = val_dif + abs(t_val - y_val)\n",
    "            record.append([iter_count, x, t_val, w, theta, y_val, t_val-y_val, dw, dtheta])\n",
    "            w = np.add(w,dw)\n",
    "            theta += dtheta\n",
    " \n",
    "    print(\"update finished. iter_count={0}\".format(iter_count))\n",
    "    print(\"w, theta = {}, {}\".format(w, theta))\n",
    "    return record\n",
    "\n",
    "dataset = {\"two_input\" : [[0,0],[0,1],[1,0],[1,1]],\n",
    "           \"one_input\" : [[0],[1]]\n",
    "          }\n",
    "teacher_data = {\"AND_gate\" : {(0,0):0, (0,1):0, (1,0):0, (1,1):1},\n",
    "                \"OR_gate\" : {(0,0):0, (0,1):1, (1,0):1, (1,1):1},\n",
    "                \"NOT_gate\" : {(0,):1, (1,):0}\n",
    "               }\n",
    "\n",
    "# 初期値\n",
    "w, theta = np.array([0,0]), np.array(0)\n",
    "\n",
    "record = update(dataset[\"two_input\"], w=w, theta=theta, \n",
    "                teacher_data = teacher_data[\"OR_gate\"],\n",
    "                max_iter=100)\n",
    "df = pd.DataFrame(record)\n",
    "df.columns = [\"k\", \"x\", \"t\", \"w\", \"theta\", \"y\", \"t-y\", \"dw\", \"dtheta\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１層パーセプトロンをn個の入力に対応させる。すなわち\n",
    "\n",
    "\\begin{align*}\n",
    "y=\\begin{cases}\n",
    "1 & (w_{1}x_{1} + w_{2}x_{2} + \\dots + w_{n}x_{n} \\geq \\theta) \\\\\\\n",
    "0 & (w_{1}x_{1} + w_{2}x_{2} + \\dots + w_{n}x_{n} < \\theta) \n",
    "\\end{cases}\\end{align*}\n",
    "\n",
    "とする。ここで、次の関数 $f(x)$ を考えると、\n",
    "\n",
    "\\begin{align*}\n",
    "f(x)=\\begin{cases}\n",
    "1 & (x \\geq 0) \\\\\\\n",
    "0 & (x < 0) \n",
    "\\end{cases}\\end{align*}\n",
    "\n",
    "ネットワークの出力 $y$ は、次のように書き直すことができる。\n",
    "\n",
    "\\begin{align*}\n",
    "y = f(w_{1}x_{1} + w_{2}x_{2} + \\dots + w_{n}x_{n} - \\theta)\n",
    "\\end{align*}\n",
    "\n",
    "この $f(x)$ をステップ関数という。\n",
    "\n",
    "入力 $x$ と重み $w$ を列ベクトルとして\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "X=\\begin{pmatrix}\n",
    "x_{1} \\\\\n",
    "x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "x_{n}\n",
    "\\end{pmatrix}\n",
    ", \\, W=\\begin{pmatrix}\n",
    "w_{1} \\\\\n",
    "w_{2} \\\\\n",
    "\\vdots \\\\\n",
    "w_{n}\n",
    "\\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "と表記し、 $-\\theta = b$ とおけば、出力は次のように表すことができる。\n",
    "\n",
    "\\begin{align*}\n",
    "y = f(W^{\\mathrm{T}}X + b)\n",
    "\\end{align*}\n",
    "\n",
    "この表記の元で、更新式は次のようになる。\n",
    "\n",
    "\\begin{align*}\n",
    "\\Delta W &= (t-y)X & W^{k+1} &= W^{k} + \\Delta W \\\\\n",
    "\\Delta b &= t-y & b^{k+1} &= b^{k} + \\Delta b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 実装\n",
    "２種類の正規分布に従うデータの分類を考える。  \n",
    "入力数は２、データ数はそれぞれ１０、  \n",
    "一方のデータは平均値が０，他方のデータは平均値が５で、  \n",
    "前者に０，後者に１を出力すべきものとする。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f466aded860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "d, N, mean = 2, 10, 5\n",
    "x1 = rng.randn(N,d) + np.array([0,0])\n",
    "x2 = rng.randn(N,d) + np.array([mean, mean])\n",
    "x = np.concatenate((x1,x2), axis=0)\n",
    "\n",
    "plt.scatter(x1[:,0], x1[:,1], c='b', label=\"mean=0\")\n",
    "plt.scatter(x2[:,0], x2[:,1], c='r', label=\"mean=5\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [ 2.14037745  1.2763927 ], b = -9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIoAKgoiIZFNBEBERAP4ffkeTgNVX+jEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f466aa3a4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.zeros(d)\n",
    "b = 0\n",
    "\n",
    "def y(x):\n",
    "    return step(np.dot(w,x)+b)\n",
    "\n",
    "def step(x):\n",
    "    return 1*(x>0)\n",
    "\n",
    "def t(i):\n",
    "    return 0 if i<N else 1\n",
    "    \n",
    "while True:\n",
    "    classified = True\n",
    "    for i in range(N*2):\n",
    "        delta_w = (t(i) - y(x[i])) * x[i]\n",
    "        delta_b = (t(i) - y(x[i]))\n",
    "        w += delta_w\n",
    "        b += delta_b\n",
    "        classified *= all(delta_w==0)*(delta_b==0)\n",
    "    if classified:\n",
    "        break\n",
    "        \n",
    "print(\"W = {}, b = {}\".format(w, b))\n",
    "line_x = np.linspace(np.min(x), np.max(x), 100)\n",
    "plt.plot(line_x, (w[0]*line_x+b) * (-1/w[1]))\n",
    "plt.scatter(x1[:,0], x1[:,1], c='b', label=\"mean=0\")\n",
    "plt.scatter(x2[:,0], x2[:,1], c='r', label=\"mean=5\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純パーセプトロンに確率的な出力を導入し、出力が行われることを「発火」と表現する  \n",
    "　（注：表記は文献に従っている。生物のニューロンの発火とは無関係である）  \n",
    "  \n",
    "ある入力 $X$ に対して発火するとき１を、しないとき０をとる確率変数 $C$ を考える。  \n",
    "このとき  \n",
    "  \n",
    "\\begin{align*}\n",
    "p(C=1|X) & = \\sigma (W^{\\mathrm{T}}X + b) \\\\\n",
    "p(C=0|X) & = 1 - p(C=1|X)\n",
    "\\end{align*}\n",
    "  \n",
    "となるので、 $y := \\sigma (W^{\\mathrm{T}}X + b)$ とすると、上の２式は  \n",
    "  \n",
    "\\begin{align*}\n",
    "p(C=t|X) &= y^{t}(1-y)^{1-t} \\\\ \n",
    "&\\text{where} \\, t \\in \\{0,1\\}\n",
    "\\end{align*}\n",
    "  \n",
    "と表すことができる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、N個の入力データ $X_{n} \\, (n=1,2,\\dots,N)$ とそれぞれ対応する正解出力 $t_{n}$ が与えられたとき、  \n",
    "重み $W$ およびバイアス $b$ を最尤推定するための尤度関数は次のように表すことができる。  \n",
    "\n",
    "\\begin{align*}\n",
    "L(W,b) &= \\prod^{N}_{n=1}p(C=t_{n}|X_{n}) \\\\\n",
    "&= \\prod^{N}_{n=1}y^{t_{n}}_{n}(1-y_{n})^{1-t_{n}}\n",
    "\\end{align*}\n",
    "\n",
    "学習を最適化問題として捉え、最尤推定（ $L(W,b)$ の最大化）によってパラメータ $W,b$ を調整することを考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の式の対数をとり、一般的な表記に合わせるため符号を入れ替えて、\n",
    "交差エントロピー誤差関数\n",
    " \n",
    " \\begin{align*}\n",
    " E(W,b) :=& -\\log L(W,b) \\\\\n",
    " =& -\\sum^{N}_{n=1} \\{ t_{n}\\log y_{n} + (1-t_{n}) \\log (1-y_{n}) \\}\n",
    " \\end{align*}\n",
    " \n",
    "を与える。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 勾配降下法 (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "W^{(k+1)} &= W^{(k)} - \\eta \\frac{\\partial E(W,b)}{\\partial W} \\\\\n",
    "b^{(k+1)} &= b^{(k)} - \\eta \\frac{\\partial E(W,b)}{\\partial b}\n",
    "\\end{align*}\n",
    "ここで $\\eta$ は学習率である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交差エントロピー誤差関数を  \n",
    "  \n",
    "\\begin{align*} E(W,b) := -\\sum^{N}_{n=1}E_{n} \\end{align*}  \n",
    "すなわち  \n",
    "\\begin{align*} E_{n} := -\\{ t_{n}\\log y_{n} + (1-t_{n} \\log (1-y_{n}) \\} \\end{align*}  \n",
    "と表すと、重み $W$ の勾配は\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(W,b)}{\\partial W} &= \\sum^{N}_{n=1} \\frac{\\partial E_{n}}{\\partial y_{n}} \\frac{\\partial y_{n}}{\\partial W} \\\\\n",
    "&= -\\sum^{N}_{n=1} \\left( \\frac{t_{n}}{y_{n}} - \\frac{1-t_{n}}{1-y_{n}} \\right) \\frac{\\partial y_{n}}{\\partial W} \\\\\n",
    "&= -\\sum^{N}_{n=1} \\left( \\frac{t_{n}}{y_{n}} - \\frac{1-t_{n}}{1-y_{n}} \\right) y_{n}(1-y_{n})X_{n} \\\\\n",
    "&= -\\sum^{N}_{n=1} \\left( t_{n}(1-y_{n}) - y_{n}(1-t_{n}) \\right) X_{n} \\\\\n",
    "&= -\\sum^{N}_{n=1} \\left( t_{n} - y_{n} \\right) X_{n}\n",
    "\\end{align*}\n",
    "\n",
    "となる。バイアス $b$ についても同様に計算して\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E(W,b)}{\\partial b} =　-\\sum^{N}_{n=1} \\left( t_{n} - y_{n} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "となるから、パラメータの更新式は\n",
    "\n",
    "\\begin{align*}\n",
    "W^{(k+1)} &= W^{(k)} - \\eta \\sum^{N}_{n=1} \\left( t_{n} - y_{n} \\right) X_{n} \\\\\n",
    "b^{(k+1)} &= b^{(k)} - \\eta \\sum^{N}_{n=1} \\left( t_{n} - y_{n} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "と書くことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の更新式はN個のデータについて総和をとる必要があり、巨大なデータセットに対しては現実的でない。  \n",
    "そこで勾配降下法のオンライン版である確率的勾配降下法 (Stochastic gradient descent) を用いる。  \n",
    "これはパラメータの更新に際しランダムにデータを選択して更新式を適用するもので、  \n",
    "\n",
    "\\begin{align*}\n",
    "W^{(k+1)} &= W^{(k)} - \\eta \\left( t_{n} - y_{n} \\right) X_{n} \\\\\n",
    "b^{(k+1)} &= b^{(k)} - \\eta \\left( t_{n} - y_{n} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "と表される。\n",
    "\n",
    "データ全体を用いるバッチ学習とランダムに１つを選んでの更新を繰り返すオンライン学習との中間として  \n",
    "ミニバッチ学習があり、よく用いられる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### TensorFlowによる実装\n",
    "（tensorflowのバージョンアップに伴い、githubに掲載されているコードを参照した。  \n",
    "　参照：[deeplearning-tensorflow-keras/3/tensorflow/01_logistic_regression_or_tensorflow.py](https://github.com/yusugomori/deeplearning-tensorflow-keras/blob/master/3/tensorflow/01_logistic_regression_or_tensorflow.py)）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "memo:  \n",
    "　TensorFlowをpip installするとpython3.5用のものが入るので、エラーが生じる場合は  \n",
    "　[公式の手順](https://www.tensorflow.org/install/install_linux#InstallingNativePip)から[バイナリのURL](https://www.tensorflow.org/install/install_linux#the_url_of_the_tensorflow_python_package)を指定して導入またはUpgradeしておく\n",
    "  \n",
    "参考：  \n",
    "　[TensorFlow 1.4 (gpu Linux) Py36 not built with Py36 #14218](https://github.com/tensorflow/tensorflow/issues/14218)  \n",
    "　[Creating a specific 3.6 binary for Linux #14182](https://github.com/tensorflow/tensorflow/issues/14182)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 変数、パラメータ、誤差関数の定義\n",
    "w = tf.Variable(tf.zeros([2,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "t = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "y = tf.nn.sigmoid(tf.matmul(x,w)+b)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(t * tf.log(y) + (1-t) * tf.log(1-y))\n",
    "\n",
    "# 学習と結果確認方法の定義\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.to_float(tf.greater(y, 0.5)), t)\n",
    "\n",
    "# データの準備\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "# セッションの作成と初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 学習の実行\n",
    "for epoch in range(200):\n",
    "    sess.run(train_step, feed_dict = {\n",
    "        x: X,\n",
    "        t: Y\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified:\n",
      " [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "output probability:\n",
      " [[ 0.22355038]\n",
      " [ 0.91425949]\n",
      " [ 0.91425949]\n",
      " [ 0.99747425]]\n",
      "parameter:\n",
      " W=[[ 3.61188436]\n",
      " [ 3.61188436]], \n",
      " b=[-1.24509501]\n"
     ]
    }
   ],
   "source": [
    "# 学習結果の確認\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "    x: X,\n",
    "    t: Y\n",
    "})\n",
    "print(\"classified:\\n\", classified)\n",
    "\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "    x: X,\n",
    "    t: Y\n",
    "})\n",
    "print(\"output probability:\\n\", prob)\n",
    "\n",
    "print(\"parameter:\\n W={}, \\n b={}\".format(sess.run(w), sess.run(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kerasによる実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 0s - loss: 1.1153     \n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s - loss: 0.9699     \n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s - loss: 0.8575     \n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s - loss: 0.7739     \n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s - loss: 0.7087     \n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s - loss: 0.6595     \n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s - loss: 0.6215     \n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s - loss: 0.5914     \n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s - loss: 0.5676     \n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s - loss: 0.5477     \n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s - loss: 0.5312     \n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s - loss: 0.5172     \n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s - loss: 0.5045     \n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s - loss: 0.4938     \n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s - loss: 0.4838     \n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s - loss: 0.4747     \n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s - loss: 0.4662     \n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s - loss: 0.4583     \n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s - loss: 0.4512     \n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s - loss: 0.4446     \n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s - loss: 0.4379     \n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s - loss: 0.4317     \n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s - loss: 0.4257     \n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s - loss: 0.4198     \n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s - loss: 0.4142     \n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s - loss: 0.4088     \n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s - loss: 0.4035     \n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s - loss: 0.3984     \n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s - loss: 0.3935     \n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s - loss: 0.3886     \n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s - loss: 0.3838     \n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s - loss: 0.3792     \n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s - loss: 0.3747     \n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s - loss: 0.3703     \n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s - loss: 0.3660     \n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s - loss: 0.3617     \n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s - loss: 0.3576     \n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s - loss: 0.3535     \n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s - loss: 0.3496     \n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s - loss: 0.3456     \n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s - loss: 0.3418     \n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s - loss: 0.3381     \n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s - loss: 0.3344     \n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s - loss: 0.3308     \n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s - loss: 0.3273     \n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s - loss: 0.3237     \n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s - loss: 0.3204     \n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s - loss: 0.3170     \n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s - loss: 0.3137     \n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s - loss: 0.3105     \n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s - loss: 0.3073     \n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s - loss: 0.3042     \n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s - loss: 0.3011     \n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s - loss: 0.2981     \n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s - loss: 0.2952     \n",
      "Epoch 56/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.752 - 0s - loss: 0.2923     \n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s - loss: 0.2894     \n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s - loss: 0.2866     \n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s - loss: 0.2839     \n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s - loss: 0.2812     \n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s - loss: 0.2785     \n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s - loss: 0.2759     \n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s - loss: 0.2734     \n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s - loss: 0.2709     \n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s - loss: 0.2683     \n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s - loss: 0.2659     \n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s - loss: 0.2635     \n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s - loss: 0.2612     \n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s - loss: 0.2588     \n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s - loss: 0.2566     \n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s - loss: 0.2543     \n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s - loss: 0.2521     \n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s - loss: 0.2499     \n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s - loss: 0.2478     \n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s - loss: 0.2457     \n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s - loss: 0.2436     \n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s - loss: 0.2416     \n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s - loss: 0.2395     \n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s - loss: 0.2376     \n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s - loss: 0.2356     \n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s - loss: 0.2337     \n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s - loss: 0.2318     \n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s - loss: 0.2299     \n",
      "Epoch 84/200\n",
      "4/4 [==============================] - 0s - loss: 0.2281     \n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s - loss: 0.2263     \n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s - loss: 0.2245     \n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s - loss: 0.2228     \n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s - loss: 0.2210     \n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s - loss: 0.2193     \n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s - loss: 0.2177     \n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s - loss: 0.2160     \n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s - loss: 0.2144     \n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s - loss: 0.2127     \n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s - loss: 0.2112     \n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s - loss: 0.2096     \n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s - loss: 0.2080     \n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s - loss: 0.2065     \n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s - loss: 0.2050     \n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s - loss: 0.2035     \n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s - loss: 0.2021     \n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s - loss: 0.2006     \n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s - loss: 0.1992     \n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s - loss: 0.1978     \n",
      "Epoch 104/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.124 - 0s - loss: 0.1964     \n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s - loss: 0.1950     \n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s - loss: 0.1937     \n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s - loss: 0.1923     \n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s - loss: 0.1910     \n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s - loss: 0.1897     \n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s - loss: 0.1884     \n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s - loss: 0.1872     \n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s - loss: 0.1859     \n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s - loss: 0.1847     \n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s - loss: 0.1835     \n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s - loss: 0.1823     \n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s - loss: 0.1811     \n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s - loss: 0.1799     \n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s - loss: 0.1787     \n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s - loss: 0.1776     \n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s - loss: 0.1764     \n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s - loss: 0.1753     \n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s - loss: 0.1742     \n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s - loss: 0.1731     \n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s - loss: 0.1720     \n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s - loss: 0.1710     \n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s - loss: 0.1699     \n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s - loss: 0.1689     \n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s - loss: 0.1678     \n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s - loss: 0.1668     \n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s - loss: 0.1658     \n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s - loss: 0.1648     \n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s - loss: 0.1638     \n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s - loss: 0.1628     \n",
      "Epoch 134/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.384 - 0s - loss: 0.1619     \n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s - loss: 0.1609     \n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s - loss: 0.1600     \n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s - loss: 0.1590     \n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s - loss: 0.1581     \n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s - loss: 0.1572     \n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s - loss: 0.1563     \n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s - loss: 0.1554     \n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s - loss: 0.1545     \n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s - loss: 0.1536     \n",
      "Epoch 144/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.108 - 0s - loss: 0.1528     \n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s - loss: 0.1519     \n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s - loss: 0.1511     \n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s - loss: 0.1502     \n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s - loss: 0.1494     \n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s - loss: 0.1486     \n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s - loss: 0.1478     \n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s - loss: 0.1470     \n",
      "Epoch 152/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.344 - 0s - loss: 0.1462     \n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s - loss: 0.1454     \n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s - loss: 0.1446     \n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s - loss: 0.1438     \n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s - loss: 0.1430     \n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s - loss: 0.1423     \n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s - loss: 0.1415     \n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s - loss: 0.1408     \n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s - loss: 0.1401     \n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s - loss: 0.1393     \n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s - loss: 0.1386     \n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s - loss: 0.1379     \n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s - loss: 0.1372     \n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s - loss: 0.1365     \n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s - loss: 0.1358     \n",
      "Epoch 167/200\n",
      "4/4 [==============================] - 0s - loss: 0.1351     \n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s - loss: 0.1344     \n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s - loss: 0.1337     \n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s - loss: 0.1331     \n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s - loss: 0.1324     \n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s - loss: 0.1318     \n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s - loss: 0.1311     \n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s - loss: 0.1305     \n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s - loss: 0.1298     \n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s - loss: 0.1292     \n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s - loss: 0.1286     \n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s - loss: 0.1279     \n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s - loss: 0.1273     \n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s - loss: 0.1267     \n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s - loss: 0.1261     \n",
      "Epoch 182/200\n",
      "4/4 [==============================] - ETA: 0s - loss: 0.095 - 0s - loss: 0.1255     \n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s - loss: 0.1249     \n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s - loss: 0.1243     \n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s - loss: 0.1238     \n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s - loss: 0.1232     \n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s - loss: 0.1226     \n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s - loss: 0.1220     \n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s - loss: 0.1215     \n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s - loss: 0.1209     \n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s - loss: 0.1204     \n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s - loss: 0.1198     \n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s - loss: 0.1193     \n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s - loss: 0.1187     \n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s - loss: 0.1182     \n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s - loss: 0.1177     \n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s - loss: 0.1171     \n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s - loss: 0.1166     \n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s - loss: 0.1161     \n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s - loss: 0.1156     \n",
      "1/4 [======>.......................] - ETA: 0s\n",
      "\n",
      "classified:\n",
      " [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n",
      "\n",
      "output probability:\n",
      " [[ 0.2340852 ]\n",
      " [ 0.9148761 ]\n",
      " [ 0.90633869]\n",
      " [ 0.99706995]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(input_dim=2, units=1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=SGD(lr=0.1))\n",
    "\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "model.fit(X, Y, epochs=200, batch_size=1)\n",
    "\n",
    "classes = model.predict_classes(X, batch_size=1)\n",
    "prob = model.predict_proba(X, batch_size=1)\n",
    "\n",
    "print(\"\\n\\nclassified:\\n\", Y==classes)\n",
    "print(\"\\noutput probability:\\n\", prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorchによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：  \n",
    "[PyTorch Tutorials](http://pytorch.org/tutorials/)  \n",
    "[epochventures/PyTorch/XOR.py](https://github.com/epochventures/PyTorch/blob/master/XOR.py)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1/200, loss:3.833631217479706\n",
      "epoch:2/200, loss:3.621105968952179\n",
      "epoch:3/200, loss:3.2993175387382507\n",
      "epoch:4/200, loss:2.950357973575592\n",
      "epoch:5/200, loss:2.6273014545440674\n",
      "epoch:6/200, loss:2.356114536523819\n",
      "epoch:7/200, loss:2.1426139771938324\n",
      "epoch:8/200, loss:1.9810305833816528\n",
      "epoch:9/200, loss:1.8610761761665344\n",
      "epoch:10/200, loss:1.7722336947917938\n",
      "epoch:11/200, loss:1.7056519836187363\n",
      "epoch:12/200, loss:1.6545888632535934\n",
      "epoch:13/200, loss:1.6141964197158813\n",
      "epoch:14/200, loss:1.5811055600643158\n",
      "epoch:15/200, loss:1.5530239269137383\n",
      "epoch:16/200, loss:1.528408795595169\n",
      "epoch:17/200, loss:1.5062270611524582\n",
      "epoch:18/200, loss:1.4857889786362648\n",
      "epoch:19/200, loss:1.4666314348578453\n",
      "epoch:20/200, loss:1.4484435766935349\n",
      "epoch:21/200, loss:1.4310133755207062\n",
      "epoch:22/200, loss:1.4141965880990028\n",
      "epoch:23/200, loss:1.3978931531310081\n",
      "epoch:24/200, loss:1.382032498717308\n",
      "epoch:25/200, loss:1.3665642514824867\n",
      "epoch:26/200, loss:1.3514516726136208\n",
      "epoch:27/200, loss:1.3366676270961761\n",
      "epoch:28/200, loss:1.3221907876431942\n",
      "epoch:29/200, loss:1.3080048374831676\n",
      "epoch:30/200, loss:1.2940966561436653\n",
      "epoch:31/200, loss:1.2804548367857933\n",
      "epoch:32/200, loss:1.2670703195035458\n",
      "epoch:33/200, loss:1.2539346441626549\n",
      "epoch:34/200, loss:1.2410401701927185\n",
      "epoch:35/200, loss:1.2283807694911957\n",
      "epoch:36/200, loss:1.2159495986998081\n",
      "epoch:37/200, loss:1.203741479665041\n",
      "epoch:38/200, loss:1.191750392317772\n",
      "epoch:39/200, loss:1.1799711249768734\n",
      "epoch:40/200, loss:1.1683988869190216\n",
      "epoch:41/200, loss:1.1570286490023136\n",
      "epoch:42/200, loss:1.1458559855818748\n",
      "epoch:43/200, loss:1.1348760202527046\n",
      "epoch:44/200, loss:1.1240845657885075\n",
      "epoch:45/200, loss:1.1134773530066013\n",
      "epoch:46/200, loss:1.103050071746111\n",
      "epoch:47/200, loss:1.0927984602749348\n",
      "epoch:48/200, loss:1.0827190019190311\n",
      "epoch:49/200, loss:1.0728077106177807\n",
      "epoch:50/200, loss:1.0630606152117252\n",
      "epoch:51/200, loss:1.0534740649163723\n",
      "epoch:52/200, loss:1.0440445952117443\n",
      "epoch:53/200, loss:1.034768633544445\n",
      "epoch:54/200, loss:1.0256427805870771\n",
      "epoch:55/200, loss:1.016663782298565\n",
      "epoch:56/200, loss:1.0078284740447998\n",
      "epoch:57/200, loss:0.9991332124918699\n",
      "epoch:58/200, loss:0.9905756637454033\n",
      "epoch:59/200, loss:0.9821518957614899\n",
      "epoch:60/200, loss:0.9738597087562084\n",
      "epoch:61/200, loss:0.9656960181891918\n",
      "epoch:62/200, loss:0.9576579760760069\n",
      "epoch:63/200, loss:0.9497428871691227\n",
      "epoch:64/200, loss:0.9419480729848146\n",
      "epoch:65/200, loss:0.934271115809679\n",
      "epoch:66/200, loss:0.9267092365771532\n",
      "epoch:67/200, loss:0.9192604050040245\n",
      "epoch:68/200, loss:0.9119218941777945\n",
      "epoch:69/200, loss:0.9046913497149944\n",
      "epoch:70/200, loss:0.8975664917379618\n",
      "epoch:71/200, loss:0.8905454520136118\n",
      "epoch:72/200, loss:0.8836259841918945\n",
      "epoch:73/200, loss:0.8768055867403746\n",
      "epoch:74/200, loss:0.8700827397406101\n",
      "epoch:75/200, loss:0.863454882055521\n",
      "epoch:76/200, loss:0.8569207787513733\n",
      "epoch:77/200, loss:0.850478008389473\n",
      "epoch:78/200, loss:0.8441248722374439\n",
      "epoch:79/200, loss:0.8378596100956202\n",
      "epoch:80/200, loss:0.8316804021596909\n",
      "epoch:81/200, loss:0.8255856391042471\n",
      "epoch:82/200, loss:0.8195736836642027\n",
      "epoch:83/200, loss:0.813642792403698\n",
      "epoch:84/200, loss:0.8077914658933878\n",
      "epoch:85/200, loss:0.8020181404426694\n",
      "epoch:86/200, loss:0.7963214367628098\n",
      "epoch:87/200, loss:0.7906995797529817\n",
      "epoch:88/200, loss:0.7851515673100948\n",
      "epoch:89/200, loss:0.779675523750484\n",
      "epoch:90/200, loss:0.7742703817784786\n",
      "epoch:91/200, loss:0.7689347947016358\n",
      "epoch:92/200, loss:0.7636673050001264\n",
      "epoch:93/200, loss:0.7584666851907969\n",
      "epoch:94/200, loss:0.7533320635557175\n",
      "epoch:95/200, loss:0.7482617348432541\n",
      "epoch:96/200, loss:0.7432550368830562\n",
      "epoch:97/200, loss:0.7383103528991342\n",
      "epoch:98/200, loss:0.7334267972037196\n",
      "epoch:99/200, loss:0.7286031441763043\n",
      "epoch:100/200, loss:0.7238384773954749\n",
      "epoch:101/200, loss:0.7191316457465291\n",
      "epoch:102/200, loss:0.714481795206666\n",
      "epoch:103/200, loss:0.7098876070231199\n",
      "epoch:104/200, loss:0.7053484544157982\n",
      "epoch:105/200, loss:0.7008630204945803\n",
      "epoch:106/200, loss:0.6964306309819221\n",
      "epoch:107/200, loss:0.6920504616573453\n",
      "epoch:108/200, loss:0.6877213176339865\n",
      "epoch:109/200, loss:0.6834426438435912\n",
      "epoch:110/200, loss:0.6792131848633289\n",
      "epoch:111/200, loss:0.6750325812026858\n",
      "epoch:112/200, loss:0.6709000235423446\n",
      "epoch:113/200, loss:0.6668141400441527\n",
      "epoch:114/200, loss:0.6627745227888227\n",
      "epoch:115/200, loss:0.6587807256728411\n",
      "epoch:116/200, loss:0.6548316413536668\n",
      "epoch:117/200, loss:0.6509263729676604\n",
      "epoch:118/200, loss:0.6470645256340504\n",
      "epoch:119/200, loss:0.6432451633736491\n",
      "epoch:120/200, loss:0.6394679052755237\n",
      "epoch:121/200, loss:0.6357316924259067\n",
      "epoch:122/200, loss:0.6320362989790738\n",
      "epoch:123/200, loss:0.6283808872103691\n",
      "epoch:124/200, loss:0.6247648089192808\n",
      "epoch:125/200, loss:0.6211872934363782\n",
      "epoch:126/200, loss:0.6176480958238244\n",
      "epoch:127/200, loss:0.6141462582163513\n",
      "epoch:128/200, loss:0.6106815505772829\n",
      "epoch:129/200, loss:0.6072529857046902\n",
      "epoch:130/200, loss:0.6038603559136391\n",
      "epoch:131/200, loss:0.600503190420568\n",
      "epoch:132/200, loss:0.5971806226298213\n",
      "epoch:133/200, loss:0.5938924951478839\n",
      "epoch:134/200, loss:0.5906377108767629\n",
      "epoch:135/200, loss:0.5874165170826018\n",
      "epoch:136/200, loss:0.5842278385534883\n",
      "epoch:137/200, loss:0.5810715253464878\n",
      "epoch:138/200, loss:0.5779468608088791\n",
      "epoch:139/200, loss:0.5748537112958729\n",
      "epoch:140/200, loss:0.5717912474647164\n",
      "epoch:141/200, loss:0.5687592988833785\n",
      "epoch:142/200, loss:0.5657572513446212\n",
      "epoch:143/200, loss:0.5627846964634955\n",
      "epoch:144/200, loss:0.5598412682302296\n",
      "epoch:145/200, loss:0.5569266229867935\n",
      "epoch:146/200, loss:0.5540402308106422\n",
      "epoch:147/200, loss:0.5511818453669548\n",
      "epoch:148/200, loss:0.5483507961034775\n",
      "epoch:149/200, loss:0.5455470141023397\n",
      "epoch:150/200, loss:0.5427698441781104\n",
      "epoch:151/200, loss:0.540019144769758\n",
      "epoch:152/200, loss:0.5372945121489465\n",
      "epoch:153/200, loss:0.534595453646034\n",
      "epoch:154/200, loss:0.531921642832458\n",
      "epoch:155/200, loss:0.5292729204520583\n",
      "epoch:156/200, loss:0.5266487072221935\n",
      "epoch:157/200, loss:0.5240488001145422\n",
      "epoch:158/200, loss:0.521473117172718\n",
      "epoch:159/200, loss:0.518920801114291\n",
      "epoch:160/200, loss:0.5163918971084058\n",
      "epoch:161/200, loss:0.5138859716244042\n",
      "epoch:162/200, loss:0.5114028607495129\n",
      "epoch:163/200, loss:0.5089421393349767\n",
      "epoch:164/200, loss:0.5065035466104746\n",
      "epoch:165/200, loss:0.5040868218056858\n",
      "epoch:166/200, loss:0.5016914275474846\n",
      "epoch:167/200, loss:0.4993174832779914\n",
      "epoch:168/200, loss:0.49696477269753814\n",
      "epoch:169/200, loss:0.49463265435770154\n",
      "epoch:170/200, loss:0.49232077808119357\n",
      "epoch:171/200, loss:0.4900294344406575\n",
      "epoch:172/200, loss:0.48775772121734917\n",
      "epoch:173/200, loss:0.48550588451325893\n",
      "epoch:174/200, loss:0.48327351408079267\n",
      "epoch:175/200, loss:0.4810603568330407\n",
      "epoch:176/200, loss:0.4788661887869239\n",
      "epoch:177/200, loss:0.47669073357246816\n",
      "epoch:178/200, loss:0.4745337013155222\n",
      "epoch:179/200, loss:0.4723951437044889\n",
      "epoch:180/200, loss:0.4702744788955897\n",
      "epoch:181/200, loss:0.4681715362239629\n",
      "epoch:182/200, loss:0.4660865687765181\n",
      "epoch:183/200, loss:0.46401868877001107\n",
      "epoch:184/200, loss:0.4619680908508599\n",
      "epoch:185/200, loss:0.459934409474954\n",
      "epoch:186/200, loss:0.45791759225539863\n",
      "epoch:187/200, loss:0.45591737143695354\n",
      "epoch:188/200, loss:0.453933514887467\n",
      "epoch:189/200, loss:0.45196580002084374\n",
      "epoch:190/200, loss:0.45001406921073794\n",
      "epoch:191/200, loss:0.44807821908034384\n",
      "epoch:192/200, loss:0.4461578391492367\n",
      "epoch:193/200, loss:0.4442530337255448\n",
      "epoch:194/200, loss:0.4423634826671332\n",
      "epoch:195/200, loss:0.4404891631565988\n",
      "epoch:196/200, loss:0.43862934480421245\n",
      "epoch:197/200, loss:0.4367846017703414\n",
      "epoch:198/200, loss:0.43495446420274675\n",
      "epoch:199/200, loss:0.43313854467123747\n",
      "epoch:200/200, loss:0.43133699195459485\n",
      "Finished Training\n",
      "Variable containing:\n",
      " 0.2211\n",
      " 0.9156\n",
      " 0.9149\n",
      " 0.9976\n",
      "[torch.FloatTensor of size 4x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.utils.data\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "Y = [[0], [1], [1], [1]]\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.BCELoss() # binary_crossentropy\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(200):\n",
    "\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for i, data in enumerate(zip(X,Y)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(torch.FloatTensor(inputs)), Variable(torch.FloatTensor(labels))\n",
    "        \n",
    "        # GPUを使用する場合の例\n",
    "        # net.cuda()\n",
    "        # inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "    print('epoch:{}/200, loss:{}'.format(epoch + 1, running_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "print(net(Variable(torch.FloatTensor(X))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 多クラスロジスティック回帰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ソフトマックス関数\n",
    "多クラス分類については、シグモイド関数の代わりにソフトマックス関数\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{softmax}(x)_{i} &= \\frac{\\mathrm{e}^{x_{i}}}{\\sum^{n}_{j=1}\\mathrm{e}^{x_{i}}} & (i = 1, 2, \\dots, n)\n",
    "\\end{align*}\n",
    "\n",
    "が用いられる。  \n",
    "ソフトマックス関数は、$n=2$ のとき\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{softmax}(x)_{1} = \\frac{\\mathrm{e}^{x_{1}}}{\\mathrm{e}^{x_{1}} + \\mathrm{e}^{x_{2}}} = \\frac{1}{1 + \\mathrm{e}^{x_{2}-x_{1}}}\n",
    "\\end{align*}\n",
    "\n",
    "と変形できる。  \n",
    "  \n",
    "また、$y_{i} = \\text{softmax}(x)_{i}$ とおくとき、  \n",
    "\n",
    "\\begin{align*}\n",
    "0 < y_{i} < 1 \\\\\n",
    "\\sum_{i=1}^{n}y_{i} = 1\n",
    "\\end{align*}\n",
    "\n",
    "が成り立つ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、\n",
    "\n",
    "\\begin{align*}\n",
    "Z := \\sum_{j=1}^{n}\\mathrm{e}^{x_{i}}\n",
    "\\end{align*}\n",
    "\n",
    "とおくとき、ソフトマックス関数の微分は\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{when} \\, &(i=j) & \\frac{\\partial y_{i}}{\\partial x_{i}} &= \\frac{\\mathrm{e}^{x_{i}}Z - \\mathrm{e}^{x_{i}}\\mathrm{e}^{x_{i}}}{Z^{2}} = y_{i}(1-y_{i}) \\\\\n",
    "&(i \\neq j) & \\frac{\\partial y_{i}}{\\partial x_{i}} &= \\frac{- \\mathrm{e}^{x_{i}}\\mathrm{e}^{x_{j}}}{Z^{2}} = - y_{i}y_{j}\n",
    "\\end{align*}\n",
    "\n",
    "となるので、これをまとめて\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial y_{i}}{\\partial x_{i}} =\\begin{cases}\n",
    "y_{i}(1-y_{i}) & (i=j) \\\\\n",
    "- y_{i}y_{j} & (i \\neq j)\n",
    "\\end{cases}\\end{align*}\n",
    "\n",
    "と表すことができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 多クラス分類のモデル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"288pt\" height=\"148pt\"\n",
       " viewBox=\"0.00 0.00 288.00 147.53\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.784741 0.784741) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-184 363,-184 363,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\"><title>x1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "</g>\n",
       "<!-- l11 -->\n",
       "<g id=\"node2\" class=\"node\"><title>l11</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"118\" cy=\"-162\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;l11 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>x1&#45;&gt;l11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.2216,-162C62.5505,-162 71.9135,-162 80.8161,-162\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.9659,-165.5 90.9659,-162 80.9658,-158.5 80.9659,-165.5\"/>\n",
       "</g>\n",
       "<!-- l1m -->\n",
       "<g id=\"node4\" class=\"node\"><title>l1m</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"118\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- l11&#45;&gt;l1m -->\n",
       "<g id=\"edge12\" class=\"edge\"><title>l11&#45;&gt;l1m</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M118,-143.859C118,-131.906 118,-119.953 118,-108\"/>\n",
       "</g>\n",
       "<!-- l21 -->\n",
       "<g id=\"node7\" class=\"node\"><title>l21</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"241\" cy=\"-113\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- l11&#45;&gt;l21 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>l11&#45;&gt;l21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143.818,-156.229C159.18,-152.259 179.125,-146.374 196,-139 204.618,-135.234 213.643,-130.129 221.261,-125.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-154.8\" font-family=\"Times,serif\" font-size=\"14.00\">w11</text>\n",
       "</g>\n",
       "<!-- l2K -->\n",
       "<g id=\"node9\" class=\"node\"><title>l2K</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"241\" cy=\"-41\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- l11&#45;&gt;l2K -->\n",
       "<g id=\"edge9\" class=\"edge\"><title>l11&#45;&gt;l2K</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M133.837,-147.163C157.06,-123.94 201.744,-79.256 225.042,-55.9583\"/>\n",
       "</g>\n",
       "<!-- xm -->\n",
       "<g id=\"node3\" class=\"node\"><title>xm</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">xm</text>\n",
       "</g>\n",
       "<!-- xm&#45;&gt;l1m -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>xm&#45;&gt;l1m</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.2216,-90C62.5505,-90 71.9135,-90 80.8161,-90\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.9659,-93.5001 90.9659,-90 80.9658,-86.5001 80.9659,-93.5001\"/>\n",
       "</g>\n",
       "<!-- l1M -->\n",
       "<g id=\"node6\" class=\"node\"><title>l1M</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"118\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "</g>\n",
       "<!-- l1m&#45;&gt;l1M -->\n",
       "<g id=\"edge13\" class=\"edge\"><title>l1m&#45;&gt;l1M</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M118,-71.8594C118,-59.9062 118,-47.9531 118,-36\"/>\n",
       "</g>\n",
       "<!-- l1m&#45;&gt;l21 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>l1m&#45;&gt;l21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M144.071,-94.7656C164.838,-98.713 194.203,-104.295 214.961,-108.24\"/>\n",
       "</g>\n",
       "<!-- l1m&#45;&gt;l2K -->\n",
       "<g id=\"edge10\" class=\"edge\"><title>l1m&#45;&gt;l2K</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.476,-80.8983C163.209,-72.0971 195.952,-58.8376 217.645,-50.0529\"/>\n",
       "</g>\n",
       "<!-- xM -->\n",
       "<g id=\"node5\" class=\"node\"><title>xM</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">xM</text>\n",
       "</g>\n",
       "<!-- xM&#45;&gt;l1M -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>xM&#45;&gt;l1M</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.2216,-18C62.5505,-18 71.9135,-18 80.8161,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"80.9659,-21.5001 90.9659,-18 80.9658,-14.5001 80.9659,-21.5001\"/>\n",
       "</g>\n",
       "<!-- l1M&#45;&gt;l21 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>l1M&#45;&gt;l21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.08,-31.4097C159.01,-49.4131 199.53,-81.226 222.615,-99.3509\"/>\n",
       "</g>\n",
       "<!-- l1M&#45;&gt;l2K -->\n",
       "<g id=\"edge11\" class=\"edge\"><title>l1M&#45;&gt;l2K</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145.322,-17.6179C160.342,-17.9176 179.422,-19.178 196,-23 203.56,-24.7429 211.5,-27.6327 218.523,-30.5922\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-26.8\" font-family=\"Times,serif\" font-size=\"14.00\">wKM</text>\n",
       "</g>\n",
       "<!-- y1 -->\n",
       "<g id=\"node8\" class=\"node\"><title>y1</title>\n",
       "<text text-anchor=\"middle\" x=\"332\" y=\"-109.3\" font-family=\"Times,serif\" font-size=\"14.00\">y1</text>\n",
       "</g>\n",
       "<!-- l21&#45;&gt;y1 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>l21&#45;&gt;y1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.222,-113C276.551,-113 285.914,-113 294.816,-113\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.966,-116.5 304.966,-113 294.966,-109.5 294.966,-116.5\"/>\n",
       "</g>\n",
       "<!-- l21&#45;&gt;l2K -->\n",
       "<g id=\"edge14\" class=\"edge\"><title>l21&#45;&gt;l2K</title>\n",
       "<path fill=\"none\" stroke=\"black\" stroke-dasharray=\"1,5\" d=\"M241,-94.8594C241,-82.9062 241,-70.9531 241,-59\"/>\n",
       "</g>\n",
       "<!-- yK -->\n",
       "<g id=\"node10\" class=\"node\"><title>yK</title>\n",
       "<text text-anchor=\"middle\" x=\"332\" y=\"-37.3\" font-family=\"Times,serif\" font-size=\"14.00\">yK</text>\n",
       "</g>\n",
       "<!-- l2K&#45;&gt;yK -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>l2K&#45;&gt;yK</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M268.222,-41C276.551,-41 285.914,-41 294.816,-41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"294.966,-44.5001 304.966,-41 294.966,-37.5001 294.966,-44.5001\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f08ec545128>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "dot.graph_attr['rankdir']='LR'\n",
    "dot.graph_attr['size']=\"4,4\"\n",
    "\n",
    "with dot.subgraph(name='b_0_0') as c00:\n",
    "    c00.node('x1', 'x1', shape='plaintext')\n",
    "    c00.node('l11', '')\n",
    "    c00.edge('x1', 'l11')\n",
    "    c00.attr(label='abcde')\n",
    "with dot.subgraph(name='b_0_1') as c01:\n",
    "    c01.node('xm', 'xm', shape='plaintext')\n",
    "    c01.node('l1m', '')\n",
    "    c01.edge('xm', 'l1m')\n",
    "with dot.subgraph(name='b_0_2') as c02:\n",
    "    c02.node('xM', 'xM', shape='plaintext')\n",
    "    c02.node('l1M', '')\n",
    "    c02.edge('xM', 'l1M')\n",
    "with dot.subgraph(name='b_1_0') as c10:\n",
    "    c10.node('l21', '')\n",
    "    c10.node('y1', 'y1', shape='plaintext')\n",
    "    c10.edge('l21', 'y1')\n",
    "with dot.subgraph(name='b_1_0') as c11:\n",
    "    c11.node('l2K', '')\n",
    "    c11.node('yK', 'yK', shape='plaintext')\n",
    "    c11.edge('l2K', 'yK')\n",
    "    \n",
    "dot.edge('l11', 'l21', arrowhead='none', label='w11')\n",
    "dot.edge('l1m', 'l21', arrowhead='none')\n",
    "dot.edge('l1M', 'l21', arrowhead='none')\n",
    "dot.edge('l11', 'l2K', arrowhead='none')\n",
    "dot.edge('l1m', 'l2K', arrowhead='none')\n",
    "dot.edge('l1M', 'l2K', arrowhead='none', label='wKM')\n",
    "    \n",
    "dot.edge('l11', 'l1m', style='dotted', arrowhead='none')\n",
    "dot.edge('l1m', 'l1M', style='dotted', arrowhead='none')\n",
    "dot.edge('l21', 'l2K', style='dotted', arrowhead='none')\n",
    "dot.body.append('{rank=min; x1; xm; xM;}')\n",
    "dot.body.append('{rank=same; l11; l1m; l1M;}')\n",
    "dot.body.append('{rank=same; l21; l2K;}')\n",
    "dot.body.append('{rank=max; y1; yK;}')\n",
    "    \n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多クラス分類のため、上図のように入出力を拡張した１層パーセプトロンを考える。  \n",
    "入力がM次元ベクトル $X=(x_{1}, x_{2}, \\dots, x_{M})^{\\mathrm{T}}$ 、出力がK次元ベクトル $Y=(y_{1}, y_{2}, \\dots, y_{K})^{\\mathrm{T}}$ であるとき、  \n",
    "あるノードkからのニューロンの出力 $y_{k}$ は\n",
    "\n",
    "\\begin{align*}\n",
    "y_{k} &= f(W_{k,1}x_{1} + W_{k,2}x_{2} + \\dots + W_{k,M}x_{M} + b_{k}) \\\\\n",
    "&= f(W^{T}_{k}X + b_{k})\n",
    "\\end{align*}\n",
    "\n",
    "となる。すると、\n",
    "\n",
    "\\begin{align*}\n",
    "W &= (W_{1} + W_{2} + \\dots + W_{k} + \\dots + W_{K})^{\\mathrm{T}} \\\\\n",
    "& \\\\\n",
    "&=\\left(\n",
    "    \\begin{array}{ccccc}\n",
    "      w_{11} & \\dots & w_{1n} & \\dots & w_{1M} \\\\\n",
    "      \\vdots & & \\vdots & & \\vdots \\\\\n",
    "      w_{k1} & \\dots & w_{kn} & \\dots & w_{kM} \\\\\n",
    "      \\vdots & & \\vdots & & \\vdots \\\\\n",
    "      w_{K1} & \\dots & w_{Kn} & \\dots & w_{KM} \\\\\n",
    "    \\end{array}\n",
    "  \\right) \\\\\n",
    "& \\\\\n",
    "b &= \\left( \\begin{array}{ccc} b_{1} \\\\ \\vdots \\\\ b_{k} \\\\ \\vdots \\\\ b_{K} \\end{array} \\right)\n",
    "\\end{align*}\n",
    "\n",
    "に対して、モデルの出力全体は\n",
    "\n",
    "\\begin{align*}\n",
    "Y = f(WX + b)\n",
    "\\end{align*}\n",
    "\n",
    "と表せる。  \n",
    "ここで、 $W$ を重み行列、 $b$ をバイアスベクトルと呼ぶ。  \n",
    "  \n",
    "活性化関数 $f$ をソフトマックス関数とすることで多クラス分類課題に使えるようになるため、  \n",
    "このモデルを多クラスロジスティック回帰という。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いま、入力 $X$ に対して、分類されるいずれかのクラス値を取る確率変数を $C$ とする。  \n",
    "クラス数がKであるとき、$C$ は1からKまでのいずれかの値を取る。  \n",
    "  \n",
    "あるニューロンの出力 $y_{k}$ は、クラスkに $X$ が分類される確率に他ならないので、\n",
    "\n",
    "\\begin{align*}\n",
    "p(C = k|X) = y_{k} &= \\frac{\\exp(W^{\\mathrm{T}}_{k}X + b_{k})}{\\sum^{K}_{j=1} \\exp (W^{\\mathrm{T}}_{j}X + b_{j})}\n",
    "\\end{align*}\n",
    "\n",
    "となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、尤度関数について考える。  \n",
    "N個の入力データ $Xn \\, (n=1,2,\\dots,N) $ と、それに対応する正解データ $t_{n}$ があるとする。  \n",
    "$X_{n}$ がクラス $k$ に属するとき、 $t_{n}$ の $j$ 番目の成分 $t_{nk}$ は、  \n",
    "\n",
    "\\begin{align*}\n",
    "t_{nj} = \\begin{cases} 1 &(j=k) \\\\ 0 &(j \\neq k) \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "となる。（すなわち、 $t_{n}$ はone-hot vectorである）  \n",
    "このとき、 $y_{n} := \\text{softmax}(WX_{n} + b)$ に対して\n",
    "\n",
    "\\begin{align*}\n",
    "L(W,b) &= \\prod_{n=1}^{N} \\prod_{k=1}^{K} p(C = k|X_{n})^{t_{nk}} \\\\\n",
    "&= \\prod_{n=1}^{N} \\prod_{k=1}^{K}y^{t_{nk}}_{nk}\n",
    "\\end{align*}\n",
    "\n",
    "が得られる。  \n",
    "単純パーセプトロンのときと同様に、対数をとって符号を反転させると、交差エントロピー誤差関数  \n",
    "\n",
    "\\begin{align*}\n",
    "E(W,b) :=& -\\log L(W,b) \\\\\n",
    "=& - \\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\log y_{nk}\n",
    "\\end{align*}\n",
    "\n",
    "が求まる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配降下法を適用するため、各パラメータに対する勾配を求める。  \n",
    "  \n",
    "< 重み $W$ に対する勾配 > \n",
    "\n",
    "\\begin{align*}\n",
    "W = (W_{1} + W_{2} + \\dots + W_{k} + \\dots + W_{K})^{\\mathrm{T}}\n",
    "\\end{align*}\n",
    "\n",
    "より\n",
    "\n",
    "\\begin{align*}\n",
    "E := \\,& E(W,b) \\\\\n",
    "= \\,& E(W_{1} + W_{2} + \\dots + W_{k} + \\dots + W_{K}, b)\n",
    "\\end{align*}\n",
    "\n",
    "とおく。$I$ をK次単位行列、 $a_{n} := WX_{n} + b$ とすると、\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W_{j}} &= -\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\frac{\\partial}{\\partial y_{nk}}(t_{nk} \\log y_{nk}) \\frac{\\partial y_{nk}}{\\partial a_{nj}} \\frac{\\partial a_{nj}}{\\partial W_{j}} \\\\\n",
    "&= -\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\frac{t_{nk}}{y_{nk}} \\frac{\\partial y_{nk}}{\\partial a_{nj}} X_{n} \\\\\n",
    "&= -\\sum_{n=1}^{N} \\sum_{k=1}^{K} \\frac{t_{nk}}{y_{nk}} y_{nk}(I_{kj} - y_{nk})X_{n} \\\\\n",
    "&= -\\sum_{n=1}^{N} \\left( \\sum_{k=1}^{K}t_{nk}I_{kj} - \\sum_{k=1}^{K}t_{nk}y_{nj} \\right) X_{n} \\\\\n",
    "&= -\\sum_{n=1}^{N} (t_{nj} - y_{nj})X_{n}\n",
    "\\end{align*}\n",
    "\n",
    "< バイアス $b$ に対する勾配 >   \n",
    "同様にして、  \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial b_{j}} = - \\sum_{n=1}^{N}(t_{nk} - y_{nj})\n",
    "\\end{align*}\n",
    "\n",
    "となる。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上をまとめて、\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial W} &= -\\sum_{n=1}^{N} (t_{n} - y_{n})X_{n}^{\\mathrm{T}} \\\\\n",
    "\\frac{\\partial E}{\\partial b} &= - \\sum_{n=1}^{N}(t_{n} - y_{n})\n",
    "\\end{align*}\n",
    "\n",
    "が求まった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### TensorFlowによる実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力２，出力３の３クラス分類ロジスティック回帰を実装する。  \n",
    "各クラスのデータは平均 $\\mu_{n} \\neq 0$ の正規分布に従い、クラスごとのデータ者100とする。  \n",
    "  \n",
    "また、ミニバッチSGDによる学習を行う。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape =  (300, 2)\n",
      "Y.shape =  (300, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYrM/wem0dQYClcV+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112b0cd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "M=2 # dimention of input data\n",
    "K=3 # the number of classes\n",
    "n=100 # the number of data per each class\n",
    "N=n*K # total number of data\n",
    "\n",
    "# sample dataset\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "X1 = np.random.randn(n,M) + np.array([0,10])\n",
    "X2 = np.random.randn(n,M) + np.array([5,5])\n",
    "X3 = np.random.randn(n,M) + np.array([10,0])\n",
    "Y1 = np.array([[1,0,0] for i in range(n)])\n",
    "Y2 = np.array([[0,1,0] for i in range(n)])\n",
    "Y3 = np.array([[0,0,1] for i in range(n)])\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis=0)\n",
    "Y = np.concatenate((Y1, Y2, Y3), axis=0)\n",
    "\n",
    "print(\"X.shape = \", X.shape)\n",
    "print(\"Y.shape = \", Y.shape)\n",
    "\n",
    "plt.scatter(X1[:,0], X1[:,1], c='r', label=\"mean=[0,10]\")\n",
    "plt.scatter(X2[:,0], X2[:,1], c='b', label=\"mean=[5,5]\")\n",
    "plt.scatter(X3[:,0], X3[:,1], c='g', label=\"mean=[10,0]\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  9.87794995e-01   1.22050513e-02   1.88995664e-08]\n",
      " [  2.69929412e-09   2.90587149e-03   9.97094154e-01]\n",
      " [  1.17402747e-01   8.71111274e-01   1.14860097e-02]\n",
      " [  9.81616616e-01   1.83833446e-02   5.69576635e-08]\n",
      " [  2.50821046e-07   6.89897016e-02   9.31010008e-01]\n",
      " [  9.98301208e-01   1.69882015e-03   8.59508992e-11]\n",
      " [  2.37434428e-08   7.24603282e-03   9.92753923e-01]\n",
      " [  9.90505278e-01   9.49475449e-03   1.84695264e-08]\n",
      " [  2.02121242e-04   8.85585546e-01   1.14212289e-01]\n",
      " [  1.95146166e-09   9.97832138e-03   9.90021646e-01]]\n",
      "\n",
      "parameter:\n",
      " W=[[-1.09316552  0.30220953  0.79095626]\n",
      " [ 0.79625851  0.2952835  -1.09154177]], \n",
      " b=[-0.06021591  0.10705093 -0.04683502]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# model setting\n",
    "W = tf.Variable(tf.zeros([M,K]))\n",
    "b = tf.Variable(tf.zeros([K]))\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, M])\n",
    "t = tf.placeholder(tf.float32, shape=[None, K])\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(t * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(t,1))\n",
    "\n",
    "\n",
    "# learning\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 50\n",
    "n_batches = N // batch_size\n",
    "\n",
    "for epoch in range(20):\n",
    "    X_, Y_ = shuffle(X,Y)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        sess.run(train_step, feed_dict={\n",
    "            x: X_[start:end],\n",
    "            t: Y_[start:end]\n",
    "        })\n",
    "        \n",
    "# check result\n",
    "X_, Y_ = shuffle(X,Y)\n",
    "\n",
    "classified = correct_prediction.eval(session=sess, feed_dict={\n",
    "    x: X_[0:10],\n",
    "    t: Y_[0:10]\n",
    "})\n",
    "prob = y.eval(session=sess, feed_dict={\n",
    "    x: X_[0:10]\n",
    "})\n",
    "\n",
    "print('classified:')\n",
    "print(classified)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)\n",
    "print()\n",
    "print(\"parameter:\\n W={}, \\n b={}\".format(sess.run(W), sess.run(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08a648c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Wv=np.array([[-1.09316552, 0.30220953, 0.79095626], \n",
    "             [0.79625851, 0.2952835, -1.09154177]])\n",
    "bv=np.array([-0.06021591, 0.10705093, -0.04683502])\n",
    "\n",
    "XL = np.linspace(np.min(X[:,0])*1.4, np.max(X[:,0])*1.4, 1000)\n",
    "C12 = (1/(Wv[1][1]-Wv[1][0])) * ((Wv[0][0] - Wv[0][1]) * XL + (bv[1] - bv[0]))\n",
    "C23 = (1/(Wv[1][2]-Wv[1][1])) * ((Wv[0][1] - Wv[0][2]) * XL + (bv[2] - bv[1]))\n",
    "#C31 = (1/(Wv[1][0]-Wv[1][2])) * ((Wv[0][2] - Wv[0][0]) * XL + (bv[0] - bv[2]))\n",
    "\n",
    "plt.plot(XL, C12, label=\"X1:X2 boundary\")\n",
    "plt.plot(XL, C23, label=\"X2:X3 boundary\")\n",
    "#plt.plot(XL, C31, label=\"X3:X1 boundary\")\n",
    "plt.scatter(X1[:,0], X1[:,1], c='r', label=\"mean=[0,10]\")\n",
    "plt.scatter(X2[:,0], X2[:,1], c='b', label=\"mean=[5,5]\")\n",
    "plt.scatter(X3[:,0], X3[:,1], c='g', label=\"mean=[10,0]\")\n",
    "plt.xlim(np.min(X[:,0])*1.4, np.max(X[:,0])*1.2)\n",
    "plt.ylim(np.min(X[:,1])*1.4, np.max(X[:,1])*1.2)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### kerasによる実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "300/300 [==============================] - 0s - loss: 4.4912     \n",
      "Epoch 2/20\n",
      "300/300 [==============================] - 0s - loss: 0.3862     \n",
      "Epoch 3/20\n",
      "300/300 [==============================] - 0s - loss: 0.2098     \n",
      "Epoch 4/20\n",
      "300/300 [==============================] - 0s - loss: 0.1491     \n",
      "Epoch 5/20\n",
      "300/300 [==============================] - 0s - loss: 0.1159     \n",
      "Epoch 6/20\n",
      "300/300 [==============================] - 0s - loss: 0.0965     \n",
      "Epoch 7/20\n",
      "300/300 [==============================] - 0s - loss: 0.0838     \n",
      "Epoch 8/20\n",
      "300/300 [==============================] - 0s - loss: 0.0750     \n",
      "Epoch 9/20\n",
      "300/300 [==============================] - 0s - loss: 0.0674     \n",
      "Epoch 10/20\n",
      "300/300 [==============================] - 0s - loss: 0.0613     \n",
      "Epoch 11/20\n",
      "300/300 [==============================] - 0s - loss: 0.0566     \n",
      "Epoch 12/20\n",
      "300/300 [==============================] - 0s - loss: 0.0527     \n",
      "Epoch 13/20\n",
      "300/300 [==============================] - 0s - loss: 0.0491     \n",
      "Epoch 14/20\n",
      "300/300 [==============================] - 0s - loss: 0.0465     \n",
      "Epoch 15/20\n",
      "300/300 [==============================] - 0s - loss: 0.0439     \n",
      "Epoch 16/20\n",
      "300/300 [==============================] - 0s - loss: 0.0416     \n",
      "Epoch 17/20\n",
      "300/300 [==============================] - 0s - loss: 0.0396     \n",
      "Epoch 18/20\n",
      "300/300 [==============================] - 0s - loss: 0.0380     \n",
      "Epoch 19/20\n",
      "300/300 [==============================] - ETA: 0s - loss: 0.030 - 0s - loss: 0.0364     \n",
      "Epoch 20/20\n",
      "300/300 [==============================] - 0s - loss: 0.0349     \n",
      "10/10 [==============================] - 0s\n",
      "10/10 [==============================] - 0s\n",
      "classified:\n",
      "[ True  True  True  True  True  True  True  True  True  True]\n",
      "\n",
      "output probability:\n",
      "[[  6.64980803e-03   9.77659941e-01   1.56902559e-02]\n",
      " [  3.36967455e-03   9.80399847e-01   1.62304975e-02]\n",
      " [  9.93589282e-01   6.41077664e-03   3.11998538e-09]\n",
      " [  1.52216163e-02   9.77653563e-01   7.12473271e-03]\n",
      " [  2.08329186e-02   9.71514046e-01   7.65311625e-03]\n",
      " [  4.03997262e-07   3.48641165e-02   9.65135455e-01]\n",
      " [  9.98069823e-01   1.93025183e-03   9.77082193e-10]\n",
      " [  1.44022783e-09   1.04066059e-02   9.89593387e-01]\n",
      " [  9.95147288e-01   4.85274056e-03   2.38945219e-08]\n",
      " [  9.92643058e-01   7.35690538e-03   1.56063393e-08]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_dim=M, units=K))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.1))\n",
    "\n",
    "batch_size = 50\n",
    "model.fit(X, Y, epochs=20, batch_size = batch_size)\n",
    "\n",
    "X_, Y_ = shuffle(X,Y)\n",
    "classes = model.predict_classes(X_[0:10], batch_size=batch_size)\n",
    "prob = model.predict_proba(X_[0:10], batch_size=batch_size)\n",
    "\n",
    "print('classified:')\n",
    "print(np.argmax(model.predict(X_[0:10]), axis=1)== classes)\n",
    "print()\n",
    "print('output probability:')\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.utils.data\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(M,K)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    running_loss = 0.0 \n",
    "\n",
    "    for i, data in enumerate(zip(X,Y)):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(torch.FloatTensor(inputs)), Variable(torch.FloatTensor(labels))\n",
    "        \n",
    "        # GPUを使用する場合の例\n",
    "        # net.cuda()\n",
    "        # inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "    print('epoch:{}/200, loss:{}'.format(epoch + 1, running_loss))\n",
    "\n",
    "print('Finished Training')\n",
    "print(net(Variable(torch.FloatTensor(X))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
